<!doctype html>
<!--[if IE 9]><html class="ie9" lang={defaultLocale}><![endif]-->
<html lang="en-US" class="Preview">
<head>
<meta name="citation_pii" content="S0306457321001527" />
<meta name="citation_id" content="102664" />
<meta name="citation_issn" content="0306-4573" />
<meta name="citation_volume" content="58" />
<meta name="citation_issue" content="5" />
<meta name="citation_publisher" content="Pergamon" />
<meta name="citation_firstpage" content="102664" />
<meta name="citation_journal_title" content="Information Processing &amp; Management" />
<meta name="citation_type" content="JOUR" />
<meta name="citation_doi" content="10.1016/j.ipm.2021.102664" />
<meta name="dc.identifier" content="10.1016/j.ipm.2021.102664" />
<meta name="citation_article_type" content="Full-length article" />
<meta property="og:description" content="This paper focuses on an important problem of detecting offensive analogy meme on online social media where the visual content and the texts/captions …" />
<meta property="og:image" content="https://ars.els-cdn.com/content/image/1-s2.0-S0306457321X00039-cov150h.gif" />
<meta name="citation_title" content="AOMD: An analogy-aware approach to offensive meme detection on social media" />
<meta property="og:title" content="AOMD: An analogy-aware approach to offensive meme detection on social media" />
<meta name="citation_publication_date" content="2021/09/01" />
<meta name="citation_online_date" content="2021/06/25" />
<meta name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOCACHE,NOODP,NOYDIR" />
<title>AOMD: An analogy-aware approach to offensive meme detection on social media - ScienceDirect</title>
<link rel="canonical" href="https://www.sciencedirect.com/science/article/abs/pii/S0306457321001527" />
<meta property="og:type" content="article" />
<meta name="viewport" content="initial-scale=1" />
<meta name="SDTech" content="Proudly brought to you by the SD Technology team in London, Dayton, and Amsterdam" />
<script type="f099078ee6a757eb17f3100f-text/javascript">(function newRelicBrowserProSPA() {
  ;
  window.NREUM || (NREUM = {});
  NREUM.init = {
    privacy: {
      cookies_enabled: true
    },
    ajax: {
      deny_list: ["bam-cell.nr-data.net"]
    }
  };
  ;
  NREUM.loader_config = {
    accountID: "2128461",
    trustKey: "2038175",
    agentID: "1118783207",
    licenseKey: "7ac4127487",
    applicationID: "814814845"
  };
  ;
  NREUM.info = {
    beacon: "bam.nr-data.net",
    errorBeacon: "bam.nr-data.net",
    licenseKey: "7ac4127487",
    applicationID: "814814845",
    sa: 1
  };
  ; /*! For license information please see nr-loader-spa-1.238.0.min.js.LICENSE.txt */
  (() => {
    "use strict";

    var e,
      t,
      r = {
        5763: (e, t, r) => {
          r.d(t, {
            P_: () => f,
            Mt: () => p,
            C5: () => s,
            DL: () => v,
            OP: () => T,
            lF: () => D,
            Yu: () => y,
            Dg: () => h,
            CX: () => c,
            GE: () => b,
            sU: () => _
          });
          var n = r(8632),
            i = r(9567);
          const o = {
              beacon: n.ce.beacon,
              errorBeacon: n.ce.errorBeacon,
              licenseKey: void 0,
              applicationID: void 0,
              sa: void 0,
              queueTime: void 0,
              applicationTime: void 0,
              ttGuid: void 0,
              user: void 0,
              account: void 0,
              product: void 0,
              extra: void 0,
              jsAttributes: {},
              userAttributes: void 0,
              atts: void 0,
              transactionName: void 0,
              tNamePlain: void 0
            },
            a = {};
          function s(e) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            if (!a[e]) throw new Error("Info for ".concat(e, " was never set"));
            return a[e];
          }
          function c(e, t) {
            if (!e) throw new Error("All info objects require an agent identifier!");
            a[e] = (0, i.D)(t, o), (0, n.Qy)(e, a[e], "info");
          }
          var u = r(7056);
          const d = () => {
              const e = {
                blockSelector: "[data-nr-block]",
                maskInputOptions: {
                  password: !0
                }
              };
              return {
                allow_bfcache: !0,
                privacy: {
                  cookies_enabled: !0
                },
                ajax: {
                  deny_list: void 0,
                  block_internal: !0,
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                distributed_tracing: {
                  enabled: void 0,
                  exclude_newrelic_header: void 0,
                  cors_use_newrelic_header: void 0,
                  cors_use_tracecontext_headers: void 0,
                  allowed_origins: void 0
                },
                session: {
                  domain: void 0,
                  expiresMs: u.oD,
                  inactiveMs: u.Hb
                },
                ssl: void 0,
                obfuscate: void 0,
                jserrors: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                metrics: {
                  enabled: !0
                },
                page_action: {
                  enabled: !0,
                  harvestTimeSeconds: 30
                },
                page_view_event: {
                  enabled: !0
                },
                page_view_timing: {
                  enabled: !0,
                  harvestTimeSeconds: 30,
                  long_task: !1
                },
                session_trace: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                },
                harvest: {
                  tooManyRequestsDelay: 60
                },
                session_replay: {
                  enabled: !1,
                  harvestTimeSeconds: 60,
                  sampleRate: .1,
                  errorSampleRate: .1,
                  maskTextSelector: "*",
                  maskAllInputs: !0,
                  get blockClass() {
                    return "nr-block";
                  },
                  get ignoreClass() {
                    return "nr-ignore";
                  },
                  get maskTextClass() {
                    return "nr-mask";
                  },
                  get blockSelector() {
                    return e.blockSelector;
                  },
                  set blockSelector(t) {
                    e.blockSelector += ",".concat(t);
                  },
                  get maskInputOptions() {
                    return e.maskInputOptions;
                  },
                  set maskInputOptions(t) {
                    e.maskInputOptions = {
                      ...t,
                      password: !0
                    };
                  }
                },
                spa: {
                  enabled: !0,
                  harvestTimeSeconds: 10
                }
              };
            },
            l = {};
          function f(e) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            if (!l[e]) throw new Error("Configuration for ".concat(e, " was never set"));
            return l[e];
          }
          function h(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            l[e] = (0, i.D)(t, d()), (0, n.Qy)(e, l[e], "config");
          }
          function p(e, t) {
            if (!e) throw new Error("All configuration objects require an agent identifier!");
            var r = f(e);
            if (r) {
              for (var n = t.split("."), i = 0; i < n.length - 1; i++) if ("object" != typeof (r = r[n[i]])) return;
              r = r[n[n.length - 1]];
            }
            return r;
          }
          const g = {
              accountID: void 0,
              trustKey: void 0,
              agentID: void 0,
              licenseKey: void 0,
              applicationID: void 0,
              xpid: void 0
            },
            m = {};
          function v(e) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            if (!m[e]) throw new Error("LoaderConfig for ".concat(e, " was never set"));
            return m[e];
          }
          function b(e, t) {
            if (!e) throw new Error("All loader-config objects require an agent identifier!");
            m[e] = (0, i.D)(t, g), (0, n.Qy)(e, m[e], "loader_config");
          }
          const y = (0, n.mF)().o;
          var w = r(385),
            A = r(6818);
          const x = {
              buildEnv: A.Re,
              bytesSent: {},
              queryBytesSent: {},
              customTransaction: void 0,
              disabled: !1,
              distMethod: A.gF,
              isolatedBacklog: !1,
              loaderType: void 0,
              maxBytes: 3e4,
              offset: Math.floor(w._A?.performance?.timeOrigin || w._A?.performance?.timing?.navigationStart || Date.now()),
              onerror: void 0,
              origin: "" + w._A.location,
              ptid: void 0,
              releaseIds: {},
              session: void 0,
              xhrWrappable: "function" == typeof w._A.XMLHttpRequest?.prototype?.addEventListener,
              version: A.q4,
              denyList: void 0
            },
            E = {};
          function T(e) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            if (!E[e]) throw new Error("Runtime for ".concat(e, " was never set"));
            return E[e];
          }
          function _(e, t) {
            if (!e) throw new Error("All runtime objects require an agent identifier!");
            E[e] = (0, i.D)(t, x), (0, n.Qy)(e, E[e], "runtime");
          }
          function D(e) {
            return function (e) {
              try {
                const t = s(e);
                return !!t.licenseKey && !!t.errorBeacon && !!t.applicationID;
              } catch (e) {
                return !1;
              }
            }(e);
          }
        },
        9567: (e, t, r) => {
          r.d(t, {
            D: () => i
          });
          var n = r(50);
          function i(e, t) {
            try {
              if (!e || "object" != typeof e) return (0, n.Z)("Setting a Configurable requires an object as input");
              if (!t || "object" != typeof t) return (0, n.Z)("Setting a Configurable requires a model to set its initial properties");
              const r = Object.create(Object.getPrototypeOf(t), Object.getOwnPropertyDescriptors(t)),
                o = 0 === Object.keys(r).length ? e : r;
              for (let a in o) if (void 0 !== e[a]) try {
                "object" == typeof e[a] && "object" == typeof t[a] ? r[a] = i(e[a], t[a]) : r[a] = e[a];
              } catch (e) {
                (0, n.Z)("An error occurred while setting a property of a Configurable", e);
              }
              return r;
            } catch (e) {
              (0, n.Z)("An error occured while setting a Configurable", e);
            }
          }
        },
        6818: (e, t, r) => {
          r.d(t, {
            Re: () => i,
            gF: () => o,
            q4: () => n
          });
          const n = "1.238.0",
            i = "PROD",
            o = "CDN";
        },
        385: (e, t, r) => {
          r.d(t, {
            FN: () => a,
            IF: () => u,
            Nk: () => l,
            Tt: () => s,
            _A: () => o,
            il: () => n,
            pL: () => c,
            v6: () => i,
            w1: () => d
          });
          const n = "undefined" != typeof window && !!window.document,
            i = "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self.navigator instanceof WorkerNavigator || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis.navigator instanceof WorkerNavigator),
            o = n ? window : "undefined" != typeof WorkerGlobalScope && ("undefined" != typeof self && self instanceof WorkerGlobalScope && self || "undefined" != typeof globalThis && globalThis instanceof WorkerGlobalScope && globalThis),
            a = "" + o?.location,
            s = /iPad|iPhone|iPod/.test(navigator.userAgent),
            c = s && "undefined" == typeof SharedWorker,
            u = (() => {
              const e = navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);
              return Array.isArray(e) && e.length >= 2 ? +e[1] : 0;
            })(),
            d = Boolean(n && window.document.documentMode),
            l = !!navigator.sendBeacon;
        },
        1117: (e, t, r) => {
          r.d(t, {
            w: () => o
          });
          var n = r(50);
          const i = {
            agentIdentifier: "",
            ee: void 0
          };
          class o {
            constructor(e) {
              try {
                if ("object" != typeof e) return (0, n.Z)("shared context requires an object as input");
                this.sharedContext = {}, Object.assign(this.sharedContext, i), Object.entries(e).forEach(e => {
                  let [t, r] = e;
                  Object.keys(i).includes(t) && (this.sharedContext[t] = r);
                });
              } catch (e) {
                (0, n.Z)("An error occured while setting SharedContext", e);
              }
            }
          }
        },
        8e3: (e, t, r) => {
          r.d(t, {
            L: () => d,
            R: () => c
          });
          var n = r(8325),
            i = r(1284),
            o = r(4322),
            a = r(3325);
          const s = {};
          function c(e, t) {
            const r = {
              staged: !1,
              priority: a.p[t] || 0
            };
            u(e), s[e].get(t) || s[e].set(t, r);
          }
          function u(e) {
            e && (s[e] || (s[e] = new Map()));
          }
          function d() {
            let e = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : "",
              t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : "feature";
            if (u(e), !e || !s[e].get(t)) return a(t);
            s[e].get(t).staged = !0;
            const r = [...s[e]];
            function a(t) {
              const r = e ? n.ee.get(e) : n.ee,
                a = o.X.handlers;
              if (r.backlog && a) {
                var s = r.backlog[t],
                  c = a[t];
                if (c) {
                  for (var u = 0; s && u < s.length; ++u) l(s[u], c);
                  (0, i.D)(c, function (e, t) {
                    (0, i.D)(t, function (t, r) {
                      r[0].on(e, r[1]);
                    });
                  });
                }
                delete a[t], r.backlog[t] = null, r.emit("drain-" + t, []);
              }
            }
            r.every(e => {
              let [t, r] = e;
              return r.staged;
            }) && (r.sort((e, t) => e[1].priority - t[1].priority), r.forEach(e => {
              let [t] = e;
              a(t);
            }));
          }
          function l(e, t) {
            var r = e[1];
            (0, i.D)(t[r], function (t, r) {
              var n = e[0];
              if (r[0] === n) {
                var i = r[1],
                  o = e[3],
                  a = e[2];
                i.apply(o, a);
              }
            });
          }
        },
        8325: (e, t, r) => {
          r.d(t, {
            A: () => c,
            ee: () => u
          });
          var n = r(8632),
            i = r(2210),
            o = r(5763);
          class a {
            constructor(e) {
              this.contextId = e;
            }
          }
          var s = r(3117);
          const c = "nr@context:".concat(s.a),
            u = function e(t, r) {
              var n = {},
                s = {},
                d = {},
                f = !1;
              try {
                f = 16 === r.length && (0, o.OP)(r).isolatedBacklog;
              } catch (e) {}
              var h = {
                on: g,
                addEventListener: g,
                removeEventListener: function (e, t) {
                  var r = n[e];
                  if (!r) return;
                  for (var i = 0; i < r.length; i++) r[i] === t && r.splice(i, 1);
                },
                emit: function (e, r, n, i, o) {
                  !1 !== o && (o = !0);
                  if (u.aborted && !i) return;
                  t && o && t.emit(e, r, n);
                  for (var a = p(n), c = m(e), d = c.length, l = 0; l < d; l++) c[l].apply(a, r);
                  var f = b()[s[e]];
                  f && f.push([h, e, r, a]);
                  return a;
                },
                get: v,
                listeners: m,
                context: p,
                buffer: function (e, t) {
                  const r = b();
                  if (t = t || "feature", h.aborted) return;
                  Object.entries(e || {}).forEach(e => {
                    let [n, i] = e;
                    s[i] = t, t in r || (r[t] = []);
                  });
                },
                abort: l,
                aborted: !1,
                isBuffering: function (e) {
                  return !!b()[s[e]];
                },
                debugId: r,
                backlog: f ? {} : t && "object" == typeof t.backlog ? t.backlog : {}
              };
              return h;
              function p(e) {
                return e && e instanceof a ? e : e ? (0, i.X)(e, c, () => new a(c)) : new a(c);
              }
              function g(e, t) {
                n[e] = m(e).concat(t);
              }
              function m(e) {
                return n[e] || [];
              }
              function v(t) {
                return d[t] = d[t] || e(h, t);
              }
              function b() {
                return h.backlog;
              }
            }(void 0, "globalEE"),
            d = (0, n.fP)();
          function l() {
            u.aborted = !0, u.backlog = {};
          }
          d.ee || (d.ee = u);
        },
        5546: (e, t, r) => {
          r.d(t, {
            E: () => n,
            p: () => i
          });
          var n = r(8325).ee.get("handle");
          function i(e, t, r, i, o) {
            o ? (o.buffer([e], i), o.emit(e, t, r)) : (n.buffer([e], i), n.emit(e, t, r));
          }
        },
        4322: (e, t, r) => {
          r.d(t, {
            X: () => o
          });
          var n = r(5546);
          o.on = a;
          var i = o.handlers = {};
          function o(e, t, r, o) {
            a(o || n.E, i, e, t, r);
          }
          function a(e, t, r, i, o) {
            o || (o = "feature"), e || (e = n.E);
            var a = t[o] = t[o] || {};
            (a[r] = a[r] || []).push([e, i]);
          }
        },
        3239: (e, t, r) => {
          r.d(t, {
            bP: () => s,
            iz: () => c,
            m$: () => a
          });
          var n = r(385);
          let i = !1,
            o = !1;
          try {
            const e = {
              get passive() {
                return i = !0, !1;
              },
              get signal() {
                return o = !0, !1;
              }
            };
            n._A.addEventListener("test", null, e), n._A.removeEventListener("test", null, e);
          } catch (e) {}
          function a(e, t) {
            return i || o ? {
              capture: !!e,
              passive: i,
              signal: t
            } : !!e;
          }
          function s(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            window.addEventListener(e, t, a(r, n));
          }
          function c(e, t) {
            let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2],
              n = arguments.length > 3 ? arguments[3] : void 0;
            document.addEventListener(e, t, a(r, n));
          }
        },
        3117: (e, t, r) => {
          r.d(t, {
            a: () => n
          });
          const n = (0, r(4402).Rl)();
        },
        4402: (e, t, r) => {
          r.d(t, {
            Ht: () => u,
            M: () => c,
            Rl: () => a,
            ky: () => s
          });
          var n = r(385);
          const i = "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";
          function o(e, t) {
            return e ? 15 & e[t] : 16 * Math.random() | 0;
          }
          function a() {
            const e = n._A?.crypto || n._A?.msCrypto;
            let t,
              r = 0;
            return e && e.getRandomValues && (t = e.getRandomValues(new Uint8Array(31))), i.split("").map(e => "x" === e ? o(t, ++r).toString(16) : "y" === e ? (3 & o() | 8).toString(16) : e).join("");
          }
          function s(e) {
            const t = n._A?.crypto || n._A?.msCrypto;
            let r,
              i = 0;
            t && t.getRandomValues && (r = t.getRandomValues(new Uint8Array(31)));
            const a = [];
            for (var s = 0; s < e; s++) a.push(o(r, ++i).toString(16));
            return a.join("");
          }
          function c() {
            return s(16);
          }
          function u() {
            return s(32);
          }
        },
        7056: (e, t, r) => {
          r.d(t, {
            Bq: () => n,
            Hb: () => o,
            oD: () => i
          });
          const n = "NRBA",
            i = 144e5,
            o = 18e5;
        },
        7894: (e, t, r) => {
          function n() {
            return Math.round(performance.now());
          }
          r.d(t, {
            z: () => n
          });
        },
        7243: (e, t, r) => {
          r.d(t, {
            e: () => o
          });
          var n = r(385),
            i = {};
          function o(e) {
            if (e in i) return i[e];
            if (0 === (e || "").indexOf("data:")) return {
              protocol: "data"
            };
            let t;
            var r = n._A?.location,
              o = {};
            if (n.il) t = document.createElement("a"), t.href = e;else try {
              t = new URL(e, r.href);
            } catch (e) {
              return o;
            }
            o.port = t.port;
            var a = t.href.split("://");
            !o.port && a[1] && (o.port = a[1].split("/")[0].split("@").pop().split(":")[1]), o.port && "0" !== o.port || (o.port = "https" === a[0] ? "443" : "80"), o.hostname = t.hostname || r.hostname, o.pathname = t.pathname, o.protocol = a[0], "/" !== o.pathname.charAt(0) && (o.pathname = "/" + o.pathname);
            var s = !t.protocol || ":" === t.protocol || t.protocol === r.protocol,
              c = t.hostname === r.hostname && t.port === r.port;
            return o.sameOrigin = s && (!t.hostname || c), "/" === o.pathname && (i[e] = o), o;
          }
        },
        50: (e, t, r) => {
          function n(e, t) {
            "function" == typeof console.warn && (console.warn("New Relic: ".concat(e)), t && console.warn(t));
          }
          r.d(t, {
            Z: () => n
          });
        },
        2587: (e, t, r) => {
          r.d(t, {
            N: () => c,
            T: () => u
          });
          var n = r(8325),
            i = r(5546),
            o = r(8e3),
            a = r(3325);
          const s = {
            stn: [a.D.sessionTrace],
            err: [a.D.jserrors, a.D.metrics],
            ins: [a.D.pageAction],
            spa: [a.D.spa],
            sr: [a.D.sessionReplay, a.D.sessionTrace]
          };
          function c(e, t) {
            const r = n.ee.get(t);
            e && "object" == typeof e && (Object.entries(e).forEach(e => {
              let [t, n] = e;
              void 0 === u[t] && (s[t] ? s[t].forEach(e => {
                n ? (0, i.p)("feat-" + t, [], void 0, e, r) : (0, i.p)("block-" + t, [], void 0, e, r), (0, i.p)("rumresp-" + t, [Boolean(n)], void 0, e, r);
              }) : n && (0, i.p)("feat-" + t, [], void 0, void 0, r), u[t] = Boolean(n));
            }), Object.keys(s).forEach(e => {
              void 0 === u[e] && (s[e]?.forEach(t => (0, i.p)("rumresp-" + e, [!1], void 0, t, r)), u[e] = !1);
            }), (0, o.L)(t, a.D.pageViewEvent));
          }
          const u = {};
        },
        2210: (e, t, r) => {
          r.d(t, {
            X: () => i
          });
          var n = Object.prototype.hasOwnProperty;
          function i(e, t, r) {
            if (n.call(e, t)) return e[t];
            var i = r();
            if (Object.defineProperty && Object.keys) try {
              return Object.defineProperty(e, t, {
                value: i,
                writable: !0,
                enumerable: !1
              }), i;
            } catch (e) {}
            return e[t] = i, i;
          }
        },
        1284: (e, t, r) => {
          r.d(t, {
            D: () => n
          });
          const n = (e, t) => Object.entries(e || {}).map(e => {
            let [r, n] = e;
            return t(r, n);
          });
        },
        4351: (e, t, r) => {
          r.d(t, {
            P: () => o
          });
          var n = r(8325);
          const i = () => {
            const e = new WeakSet();
            return (t, r) => {
              if ("object" == typeof r && null !== r) {
                if (e.has(r)) return;
                e.add(r);
              }
              return r;
            };
          };
          function o(e) {
            try {
              return JSON.stringify(e, i());
            } catch (e) {
              try {
                n.ee.emit("internal-error", [e]);
              } catch (e) {}
            }
          }
        },
        3960: (e, t, r) => {
          r.d(t, {
            K: () => a,
            b: () => o
          });
          var n = r(3239);
          function i() {
            return "undefined" == typeof document || "complete" === document.readyState;
          }
          function o(e, t) {
            if (i()) return e();
            (0, n.bP)("load", e, t);
          }
          function a(e) {
            if (i()) return e();
            (0, n.iz)("DOMContentLoaded", e);
          }
        },
        8632: (e, t, r) => {
          r.d(t, {
            EZ: () => u,
            Qy: () => c,
            ce: () => o,
            fP: () => a,
            gG: () => d,
            mF: () => s
          });
          var n = r(7894),
            i = r(385);
          const o = {
            beacon: "bam.nr-data.net",
            errorBeacon: "bam.nr-data.net"
          };
          function a() {
            return i._A.NREUM || (i._A.NREUM = {}), void 0 === i._A.newrelic && (i._A.newrelic = i._A.NREUM), i._A.NREUM;
          }
          function s() {
            let e = a();
            return e.o || (e.o = {
              ST: i._A.setTimeout,
              SI: i._A.setImmediate,
              CT: i._A.clearTimeout,
              XHR: i._A.XMLHttpRequest,
              REQ: i._A.Request,
              EV: i._A.Event,
              PR: i._A.Promise,
              MO: i._A.MutationObserver,
              FETCH: i._A.fetch
            }), e;
          }
          function c(e, t, r) {
            let i = a();
            const o = i.initializedAgents || {},
              s = o[e] || {};
            return Object.keys(s).length || (s.initializedAt = {
              ms: (0, n.z)(),
              date: new Date()
            }), i.initializedAgents = {
              ...o,
              [e]: {
                ...s,
                [r]: t
              }
            }, i;
          }
          function u(e, t) {
            a()[e] = t;
          }
          function d() {
            return function () {
              let e = a();
              const t = e.info || {};
              e.info = {
                beacon: o.beacon,
                errorBeacon: o.errorBeacon,
                ...t
              };
            }(), function () {
              let e = a();
              const t = e.init || {};
              e.init = {
                ...t
              };
            }(), s(), function () {
              let e = a();
              const t = e.loader_config || {};
              e.loader_config = {
                ...t
              };
            }(), a();
          }
        },
        7956: (e, t, r) => {
          r.d(t, {
            N: () => i
          });
          var n = r(3239);
          function i(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] && arguments[1],
              r = arguments.length > 2 ? arguments[2] : void 0,
              i = arguments.length > 3 ? arguments[3] : void 0;
            return void (0, n.iz)("visibilitychange", function () {
              if (t) return void ("hidden" == document.visibilityState && e());
              e(document.visibilityState);
            }, r, i);
          }
        },
        1214: (e, t, r) => {
          r.d(t, {
            em: () => b,
            u5: () => j,
            QU: () => O,
            _L: () => I,
            Gm: () => H,
            Lg: () => L,
            BV: () => G,
            Kf: () => K
          });
          var n = r(8325),
            i = r(3117);
          const o = "nr@original:".concat(i.a);
          var a = Object.prototype.hasOwnProperty,
            s = !1;
          function c(e, t) {
            return e || (e = n.ee), r.inPlace = function (e, t, n, i, o) {
              n || (n = "");
              const a = "-" === n.charAt(0);
              for (let s = 0; s < t.length; s++) {
                const c = t[s],
                  u = e[c];
                d(u) || (e[c] = r(u, a ? c + n : n, i, c, o));
              }
            }, r.flag = o, r;
            function r(t, r, n, s, c) {
              return d(t) ? t : (r || (r = ""), nrWrapper[o] = t, function (e, t, r) {
                if (Object.defineProperty && Object.keys) try {
                  return Object.keys(e).forEach(function (r) {
                    Object.defineProperty(t, r, {
                      get: function () {
                        return e[r];
                      },
                      set: function (t) {
                        return e[r] = t, t;
                      }
                    });
                  }), t;
                } catch (e) {
                  u([e], r);
                }
                for (var n in e) a.call(e, n) && (t[n] = e[n]);
              }(t, nrWrapper, e), nrWrapper);
              function nrWrapper() {
                var o, a, d, l;
                try {
                  a = this, o = [...arguments], d = "function" == typeof n ? n(o, a) : n || {};
                } catch (t) {
                  u([t, "", [o, a, s], d], e);
                }
                i(r + "start", [o, a, s], d, c);
                try {
                  return l = t.apply(a, o);
                } catch (e) {
                  throw i(r + "err", [o, a, e], d, c), e;
                } finally {
                  i(r + "end", [o, a, l], d, c);
                }
              }
            }
            function i(r, n, i, o) {
              if (!s || t) {
                var a = s;
                s = !0;
                try {
                  e.emit(r, n, i, t, o);
                } catch (t) {
                  u([t, r, n, i], e);
                }
                s = a;
              }
            }
          }
          function u(e, t) {
            t || (t = n.ee);
            try {
              t.emit("internal-error", e);
            } catch (e) {}
          }
          function d(e) {
            return !(e && e instanceof Function && e.apply && !e[o]);
          }
          var l = r(2210),
            f = r(385);
          const h = {},
            p = f._A.XMLHttpRequest,
            g = "addEventListener",
            m = "removeEventListener",
            v = "nr@wrapped:".concat(n.A);
          function b(e) {
            var t = function (e) {
              return (e || n.ee).get("events");
            }(e);
            if (h[t.debugId]++) return t;
            h[t.debugId] = 1;
            var r = c(t, !0);
            function i(e) {
              r.inPlace(e, [g, m], "-", o);
            }
            function o(e, t) {
              return e[1];
            }
            return "getPrototypeOf" in Object && (f.il && y(document, i), y(f._A, i), y(p.prototype, i)), t.on(g + "-start", function (e, t) {
              var n = e[1];
              if (null !== n && ("function" == typeof n || "object" == typeof n)) {
                var i = (0, l.X)(n, v, function () {
                  var e = {
                    object: function () {
                      if ("function" != typeof n.handleEvent) return;
                      return n.handleEvent.apply(n, arguments);
                    },
                    function: n
                  }[typeof n];
                  return e ? r(e, "fn-", null, e.name || "anonymous") : n;
                });
                this.wrapped = e[1] = i;
              }
            }), t.on(m + "-start", function (e) {
              e[1] = this.wrapped || e[1];
            }), t;
          }
          function y(e, t) {
            let r = e;
            for (; "object" == typeof r && !Object.prototype.hasOwnProperty.call(r, g);) r = Object.getPrototypeOf(r);
            for (var n = arguments.length, i = new Array(n > 2 ? n - 2 : 0), o = 2; o < n; o++) i[o - 2] = arguments[o];
            r && t(r, ...i);
          }
          var w = "fetch-",
            A = w + "body-",
            x = ["arrayBuffer", "blob", "json", "text", "formData"],
            E = f._A.Request,
            T = f._A.Response,
            _ = "prototype";
          const D = {};
          function j(e) {
            const t = function (e) {
              return (e || n.ee).get("fetch");
            }(e);
            if (!(E && T && f._A.fetch)) return t;
            if (D[t.debugId]++) return t;
            function r(e, r, i) {
              var o = e[r];
              "function" == typeof o && (e[r] = function () {
                var e,
                  r = [...arguments],
                  a = {};
                t.emit(i + "before-start", [r], a), a[n.A] && a[n.A].dt && (e = a[n.A].dt);
                var s = o.apply(this, r);
                return t.emit(i + "start", [r, e], s), s.then(function (e) {
                  return t.emit(i + "end", [null, e], s), e;
                }, function (e) {
                  throw t.emit(i + "end", [e], s), e;
                });
              });
            }
            return D[t.debugId] = 1, x.forEach(e => {
              r(E[_], e, A), r(T[_], e, A);
            }), r(f._A, "fetch", w), t.on(w + "end", function (e, r) {
              var n = this;
              if (r) {
                var i = r.headers.get("content-length");
                null !== i && (n.rxSize = i), t.emit(w + "done", [null, r], n);
              } else t.emit(w + "done", [e], n);
            }), t;
          }
          const C = {},
            N = ["pushState", "replaceState"];
          function O(e) {
            const t = function (e) {
              return (e || n.ee).get("history");
            }(e);
            return !f.il || C[t.debugId]++ || (C[t.debugId] = 1, c(t).inPlace(window.history, N, "-")), t;
          }
          var S = r(3239);
          const P = {},
            R = ["appendChild", "insertBefore", "replaceChild"];
          function I(e) {
            const t = function (e) {
              return (e || n.ee).get("jsonp");
            }(e);
            if (!f.il || P[t.debugId]) return t;
            P[t.debugId] = !0;
            var r = c(t),
              i = /[?&](?:callback|cb)=([^&#]+)/,
              o = /(.*)\.([^.]+)/,
              a = /^(\w+)(\.|$)(.*)$/;
            function s(e, t) {
              if (!e) return t;
              const r = e.match(a),
                n = r[1];
              return s(r[3], t[n]);
            }
            return r.inPlace(Node.prototype, R, "dom-"), t.on("dom-start", function (e) {
              !function (e) {
                if (!e || "string" != typeof e.nodeName || "script" !== e.nodeName.toLowerCase()) return;
                if ("function" != typeof e.addEventListener) return;
                var n = (a = e.src, c = a.match(i), c ? c[1] : null);
                var a, c;
                if (!n) return;
                var u = function (e) {
                  var t = e.match(o);
                  if (t && t.length >= 3) return {
                    key: t[2],
                    parent: s(t[1], window)
                  };
                  return {
                    key: e,
                    parent: window
                  };
                }(n);
                if ("function" != typeof u.parent[u.key]) return;
                var d = {};
                function l() {
                  t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                function f() {
                  t.emit("jsonp-error", [], d), t.emit("jsonp-end", [], d), e.removeEventListener("load", l, (0, S.m$)(!1)), e.removeEventListener("error", f, (0, S.m$)(!1));
                }
                r.inPlace(u.parent, [u.key], "cb-", d), e.addEventListener("load", l, (0, S.m$)(!1)), e.addEventListener("error", f, (0, S.m$)(!1)), t.emit("new-jsonp", [e.src], d);
              }(e[0]);
            }), t;
          }
          const k = {};
          function H(e) {
            const t = function (e) {
              return (e || n.ee).get("mutation");
            }(e);
            if (!f.il || k[t.debugId]) return t;
            k[t.debugId] = !0;
            var r = c(t),
              i = f._A.MutationObserver;
            return i && (window.MutationObserver = function (e) {
              return this instanceof i ? new i(r(e, "fn-")) : i.apply(this, arguments);
            }, MutationObserver.prototype = i.prototype), t;
          }
          const z = {};
          function L(e) {
            const t = function (e) {
              return (e || n.ee).get("promise");
            }(e);
            if (z[t.debugId]) return t;
            z[t.debugId] = !0;
            var r = t.context,
              i = c(t),
              a = f._A.Promise;
            return a && function () {
              function e(r) {
                var n = t.context(),
                  o = i(r, "executor-", n, null, !1);
                const s = Reflect.construct(a, [o], e);
                return t.context(s).getCtx = function () {
                  return n;
                }, s;
              }
              f._A.Promise = e, Object.defineProperty(e, "name", {
                value: "Promise"
              }), e.toString = function () {
                return a.toString();
              }, Object.setPrototypeOf(e, a), ["all", "race"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  let i = !1;
                  [...(e || [])].forEach(e => {
                    this.resolve(e).then(a("all" === r), a(!1));
                  });
                  const o = n.apply(this, arguments);
                  return o;
                  function a(e) {
                    return function () {
                      t.emit("propagate", [null, !i], o, !1, !1), i = i || !e;
                    };
                  }
                };
              }), ["resolve", "reject"].forEach(function (r) {
                const n = a[r];
                e[r] = function (e) {
                  const r = n.apply(this, arguments);
                  return e !== r && t.emit("propagate", [e, !0], r, !1, !1), r;
                };
              }), e.prototype = a.prototype;
              const n = a.prototype.then;
              a.prototype.then = function () {
                var e = this,
                  o = r(e);
                o.promise = e;
                for (var a = arguments.length, s = new Array(a), c = 0; c < a; c++) s[c] = arguments[c];
                s[0] = i(s[0], "cb-", o, null, !1), s[1] = i(s[1], "cb-", o, null, !1);
                const u = n.apply(this, s);
                return o.nextPromise = u, t.emit("propagate", [e, !0], u, !1, !1), u;
              }, a.prototype.then[o] = n, t.on("executor-start", function (e) {
                e[0] = i(e[0], "resolve-", this, null, !1), e[1] = i(e[1], "resolve-", this, null, !1);
              }), t.on("executor-err", function (e, t, r) {
                e[1](r);
              }), t.on("cb-end", function (e, r, n) {
                t.emit("propagate", [n, !0], this.nextPromise, !1, !1);
              }), t.on("propagate", function (e, r, n) {
                this.getCtx && !r || (this.getCtx = function () {
                  if (e instanceof Promise) var r = t.context(e);
                  return r && r.getCtx ? r.getCtx() : this;
                });
              });
            }(), t;
          }
          const M = {},
            B = "setTimeout",
            F = "setInterval",
            U = "clearTimeout",
            q = "-start",
            Z = "-",
            V = [B, "setImmediate", F, U, "clearImmediate"];
          function G(e) {
            const t = function (e) {
              return (e || n.ee).get("timer");
            }(e);
            if (M[t.debugId]++) return t;
            M[t.debugId] = 1;
            var r = c(t);
            return r.inPlace(f._A, V.slice(0, 2), B + Z), r.inPlace(f._A, V.slice(2, 3), F + Z), r.inPlace(f._A, V.slice(3), U + Z), t.on(F + q, function (e, t, n) {
              e[0] = r(e[0], "fn-", null, n);
            }), t.on(B + q, function (e, t, n) {
              this.method = n, this.timerDuration = isNaN(e[1]) ? 0 : +e[1], e[0] = r(e[0], "fn-", this, n);
            }), t;
          }
          var W = r(50);
          const X = {},
            Q = ["open", "send"];
          function K(e) {
            var t = e || n.ee;
            const r = function (e) {
              return (e || n.ee).get("xhr");
            }(t);
            if (X[r.debugId]++) return r;
            X[r.debugId] = 1, b(t);
            var i = c(r),
              o = f._A.XMLHttpRequest,
              a = f._A.MutationObserver,
              s = f._A.Promise,
              u = f._A.setInterval,
              d = "readystatechange",
              l = ["onload", "onerror", "onabort", "onloadstart", "onloadend", "onprogress", "ontimeout"],
              h = [],
              p = f._A.XMLHttpRequest = function (e) {
                const t = new o(e),
                  n = r.context(t);
                try {
                  r.emit("new-xhr", [t], n), t.addEventListener(d, (a = n, function () {
                    var e = this;
                    e.readyState > 3 && !a.resolved && (a.resolved = !0, r.emit("xhr-resolved", [], e)), i.inPlace(e, l, "fn-", A);
                  }), (0, S.m$)(!1));
                } catch (e) {
                  (0, W.Z)("An error occurred while intercepting XHR", e);
                  try {
                    r.emit("internal-error", [e]);
                  } catch (e) {}
                }
                var a;
                return t;
              };
            function g(e, t) {
              i.inPlace(t, ["onreadystatechange"], "fn-", A);
            }
            if (function (e, t) {
              for (var r in e) t[r] = e[r];
            }(o, p), p.prototype = o.prototype, i.inPlace(p.prototype, Q, "-xhr-", A), r.on("send-xhr-start", function (e, t) {
              g(e, t), function (e) {
                h.push(e), a && (m ? m.then(w) : u ? u(w) : (v = -v, y.data = v));
              }(t);
            }), r.on("open-xhr-start", g), a) {
              var m = s && s.resolve();
              if (!u && !s) {
                var v = 1,
                  y = document.createTextNode(v);
                new a(w).observe(y, {
                  characterData: !0
                });
              }
            } else t.on("fn-end", function (e) {
              e[0] && e[0].type === d || w();
            });
            function w() {
              for (var e = 0; e < h.length; e++) g(0, h[e]);
              h.length && (h = []);
            }
            function A(e, t) {
              return t;
            }
            return r;
          }
        },
        7825: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.ajax;
        },
        6660: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.jserrors;
        },
        3081: (e, t, r) => {
          r.d(t, {
            gF: () => o,
            mY: () => i,
            t9: () => n,
            vz: () => s,
            xS: () => a
          });
          const n = r(3325).D.metrics,
            i = "sm",
            o = "cm",
            a = "storeSupportabilityMetrics",
            s = "storeEventMetrics";
        },
        4649: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageAction;
        },
        7633: (e, t, r) => {
          r.d(t, {
            Dz: () => i,
            OJ: () => a,
            qw: () => o,
            t9: () => n
          });
          const n = r(3325).D.pageViewEvent,
            i = "firstbyte",
            o = "domcontent",
            a = "windowload";
        },
        9251: (e, t, r) => {
          r.d(t, {
            t: () => n
          });
          const n = r(3325).D.pageViewTiming;
        },
        3614: (e, t, r) => {
          r.d(t, {
            BST_RESOURCE: () => i,
            END: () => s,
            FEATURE_NAME: () => n,
            FN_END: () => u,
            FN_START: () => c,
            PUSH_STATE: () => d,
            RESOURCE: () => o,
            START: () => a
          });
          const n = r(3325).D.sessionTrace,
            i = "bstResource",
            o = "resource",
            a = "-start",
            s = "-end",
            c = "fn" + a,
            u = "fn" + s,
            d = "pushState";
        },
        7836: (e, t, r) => {
          r.d(t, {
            BODY: () => x,
            CB_END: () => E,
            CB_START: () => u,
            END: () => A,
            FEATURE_NAME: () => i,
            FETCH: () => _,
            FETCH_BODY: () => v,
            FETCH_DONE: () => m,
            FETCH_START: () => g,
            FN_END: () => c,
            FN_START: () => s,
            INTERACTION: () => f,
            INTERACTION_API: () => d,
            INTERACTION_EVENTS: () => o,
            JSONP_END: () => b,
            JSONP_NODE: () => p,
            JS_TIME: () => T,
            MAX_TIMER_BUDGET: () => a,
            REMAINING: () => l,
            SPA_NODE: () => h,
            START: () => w,
            originalSetTimeout: () => y
          });
          var n = r(5763);
          const i = r(3325).D.spa,
            o = ["click", "submit", "keypress", "keydown", "keyup", "change"],
            a = 999,
            s = "fn-start",
            c = "fn-end",
            u = "cb-start",
            d = "api-ixn-",
            l = "remaining",
            f = "interaction",
            h = "spaNode",
            p = "jsonpNode",
            g = "fetch-start",
            m = "fetch-done",
            v = "fetch-body-",
            b = "jsonp-end",
            y = n.Yu.ST,
            w = "-start",
            A = "-end",
            x = "-body",
            E = "cb" + A,
            T = "jsTime",
            _ = "fetch";
        },
        5938: (e, t, r) => {
          r.d(t, {
            W: () => o
          });
          var n = r(5763),
            i = r(8325);
          class o {
            constructor(e, t, r) {
              this.agentIdentifier = e, this.aggregator = t, this.ee = i.ee.get(e, (0, n.OP)(this.agentIdentifier).isolatedBacklog), this.featureName = r, this.blocked = !1;
            }
          }
        },
        9144: (e, t, r) => {
          r.d(t, {
            j: () => m
          });
          var n = r(3325),
            i = r(5763),
            o = r(5546),
            a = r(8325),
            s = r(7894),
            c = r(8e3),
            u = r(3960),
            d = r(385),
            l = r(50),
            f = r(3081),
            h = r(8632);
          function p() {
            const e = (0, h.gG)();
            ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease", "addPageAction", "setCurrentRouteName", "setPageViewName", "setCustomAttribute", "interaction", "noticeError", "setUserId", "setApplicationVersion"].forEach(t => {
              e[t] = function () {
                for (var r = arguments.length, n = new Array(r), i = 0; i < r; i++) n[i] = arguments[i];
                return function (t) {
                  for (var r = arguments.length, n = new Array(r > 1 ? r - 1 : 0), i = 1; i < r; i++) n[i - 1] = arguments[i];
                  let o = [];
                  return Object.values(e.initializedAgents).forEach(e => {
                    e.exposed && e.api[t] && o.push(e.api[t](...n));
                  }), o.length > 1 ? o : o[0];
                }(t, ...n);
              };
            });
          }
          var g = r(2587);
          function m(e) {
            let t = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : {},
              m = arguments.length > 2 ? arguments[2] : void 0,
              v = arguments.length > 3 ? arguments[3] : void 0,
              {
                init: b,
                info: y,
                loader_config: w,
                runtime: A = {
                  loaderType: m
                },
                exposed: x = !0
              } = t;
            const E = (0, h.gG)();
            y || (b = E.init, y = E.info, w = E.loader_config), (0, i.Dg)(e, b || {}), (0, i.GE)(e, w || {}), y.jsAttributes ??= {}, d.v6 && (y.jsAttributes.isWorker = !0), (0, i.CX)(e, y);
            const T = (0, i.P_)(e);
            A.denyList = [...(T.ajax?.deny_list || []), ...(T.ajax?.block_internal ? [y.beacon, y.errorBeacon] : [])], (0, i.sU)(e, A), p();
            const _ = function (e, t) {
              t || (0, c.R)(e, "api");
              const h = {};
              var p = a.ee.get(e),
                g = p.get("tracer"),
                m = "api-",
                v = m + "ixn-";
              function b(t, r, n, o) {
                const a = (0, i.C5)(e);
                return null === r ? delete a.jsAttributes[t] : (0, i.CX)(e, {
                  ...a,
                  jsAttributes: {
                    ...a.jsAttributes,
                    [t]: r
                  }
                }), A(m, n, !0, o || null === r ? "session" : void 0)(t, r);
              }
              function y() {}
              ["setErrorHandler", "finished", "addToTrace", "inlineHit", "addRelease"].forEach(e => h[e] = A(m, e, !0, "api")), h.addPageAction = A(m, "addPageAction", !0, n.D.pageAction), h.setCurrentRouteName = A(m, "routeName", !0, n.D.spa), h.setPageViewName = function (t, r) {
                if ("string" == typeof t) return "/" !== t.charAt(0) && (t = "/" + t), (0, i.OP)(e).customTransaction = (r || "http://custom.transaction") + t, A(m, "setPageViewName", !0)();
              }, h.setCustomAttribute = function (e, t) {
                let r = arguments.length > 2 && void 0 !== arguments[2] && arguments[2];
                if ("string" == typeof e) {
                  if (["string", "number"].includes(typeof t) || null === t) return b(e, t, "setCustomAttribute", r);
                  (0, l.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t, "> was provided."));
                } else (0, l.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setUserId = function (e) {
                if ("string" == typeof e || null === e) return b("enduser.id", e, "setUserId", !0);
                (0, l.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e, "> was provided."));
              }, h.setApplicationVersion = function (e) {
                if ("string" == typeof e || null === e) return b("application.version", e, "setApplicationVersion", !1);
                (0, l.Z)("Failed to execute setApplicationVersion. Expected <String | null>, but got <".concat(typeof e, ">."));
              }, h.interaction = function () {
                return new y().get();
              };
              var w = y.prototype = {
                createTracer: function (e, t) {
                  var r = {},
                    i = this,
                    a = "function" == typeof t;
                  return (0, o.p)(v + "tracer", [(0, s.z)(), e, r], i, n.D.spa, p), function () {
                    if (g.emit((a ? "" : "no-") + "fn-start", [(0, s.z)(), i, a], r), a) try {
                      return t.apply(this, arguments);
                    } catch (e) {
                      throw g.emit("fn-err", [arguments, this, e], r), e;
                    } finally {
                      g.emit("fn-end", [(0, s.z)()], r);
                    }
                  };
                }
              };
              function A(e, t, r, i) {
                return function () {
                  return (0, o.p)(f.xS, ["API/" + t + "/called"], void 0, n.D.metrics, p), i && (0, o.p)(e + t, [(0, s.z)(), ...arguments], r ? null : this, i, p), r ? void 0 : this;
                };
              }
              function x() {
                r.e(111).then(r.bind(r, 7438)).then(t => {
                  let {
                    setAPI: r
                  } = t;
                  r(e), (0, c.L)(e, "api");
                }).catch(() => (0, l.Z)("Downloading runtime APIs failed..."));
              }
              return ["actionText", "setName", "setAttribute", "save", "ignore", "onEnd", "getContext", "end", "get"].forEach(e => {
                w[e] = A(v, e, void 0, n.D.spa);
              }), h.noticeError = function (e, t) {
                "string" == typeof e && (e = new Error(e)), (0, o.p)(f.xS, ["API/noticeError/called"], void 0, n.D.metrics, p), (0, o.p)("err", [e, (0, s.z)(), !1, t], void 0, n.D.jserrors, p);
              }, d.il ? (0, u.b)(() => x(), !0) : x(), h;
            }(e, v);
            return (0, h.Qy)(e, _, "api"), (0, h.Qy)(e, x, "exposed"), (0, h.EZ)("activatedFeatures", g.T), _;
          }
        },
        3325: (e, t, r) => {
          r.d(t, {
            D: () => n,
            p: () => i
          });
          const n = {
              ajax: "ajax",
              jserrors: "jserrors",
              metrics: "metrics",
              pageAction: "page_action",
              pageViewEvent: "page_view_event",
              pageViewTiming: "page_view_timing",
              sessionReplay: "session_replay",
              sessionTrace: "session_trace",
              spa: "spa"
            },
            i = {
              [n.pageViewEvent]: 1,
              [n.pageViewTiming]: 2,
              [n.metrics]: 3,
              [n.jserrors]: 4,
              [n.ajax]: 5,
              [n.sessionTrace]: 6,
              [n.pageAction]: 7,
              [n.spa]: 8,
              [n.sessionReplay]: 9
            };
        }
      },
      n = {};
    function i(e) {
      var t = n[e];
      if (void 0 !== t) return t.exports;
      var o = n[e] = {
        exports: {}
      };
      return r[e](o, o.exports, i), o.exports;
    }
    i.m = r, i.d = (e, t) => {
      for (var r in t) i.o(t, r) && !i.o(e, r) && Object.defineProperty(e, r, {
        enumerable: !0,
        get: t[r]
      });
    }, i.f = {}, i.e = e => Promise.all(Object.keys(i.f).reduce((t, r) => (i.f[r](e, t), t), [])), i.u = e => "nr-spa.1097a448-1.238.0.min.js", i.o = (e, t) => Object.prototype.hasOwnProperty.call(e, t), e = {}, t = "NRBA-1.238.0.PROD:", i.l = (r, n, o, a) => {
      if (e[r]) e[r].push(n);else {
        var s, c;
        if (void 0 !== o) for (var u = document.getElementsByTagName("script"), d = 0; d < u.length; d++) {
          var l = u[d];
          if (l.getAttribute("src") == r || l.getAttribute("data-webpack") == t + o) {
            s = l;
            break;
          }
        }
        s || (c = !0, (s = document.createElement("script")).charset = "utf-8", s.timeout = 120, i.nc && s.setAttribute("nonce", i.nc), s.setAttribute("data-webpack", t + o), s.src = r), e[r] = [n];
        var f = (t, n) => {
            s.onerror = s.onload = null, clearTimeout(h);
            var i = e[r];
            if (delete e[r], s.parentNode && s.parentNode.removeChild(s), i && i.forEach(e => e(n)), t) return t(n);
          },
          h = setTimeout(f.bind(null, void 0, {
            type: "timeout",
            target: s
          }), 12e4);
        s.onerror = f.bind(null, s.onerror), s.onload = f.bind(null, s.onload), c && document.head.appendChild(s);
      }
    }, i.r = e => {
      "undefined" != typeof Symbol && Symbol.toStringTag && Object.defineProperty(e, Symbol.toStringTag, {
        value: "Module"
      }), Object.defineProperty(e, "__esModule", {
        value: !0
      });
    }, i.p = "https://js-agent.newrelic.com/", (() => {
      var e = {
        801: 0,
        92: 0
      };
      i.f.j = (t, r) => {
        var n = i.o(e, t) ? e[t] : void 0;
        if (0 !== n) if (n) r.push(n[2]);else {
          var o = new Promise((r, i) => n = e[t] = [r, i]);
          r.push(n[2] = o);
          var a = i.p + i.u(t),
            s = new Error();
          i.l(a, r => {
            if (i.o(e, t) && (0 !== (n = e[t]) && (e[t] = void 0), n)) {
              var o = r && ("load" === r.type ? "missing" : r.type),
                a = r && r.target && r.target.src;
              s.message = "Loading chunk " + t + " failed.\n(" + o + ": " + a + ")", s.name = "ChunkLoadError", s.type = o, s.request = a, n[1](s);
            }
          }, "chunk-" + t, t);
        }
      };
      var t = (t, r) => {
          var n,
            o,
            [a, s, c] = r,
            u = 0;
          if (a.some(t => 0 !== e[t])) {
            for (n in s) i.o(s, n) && (i.m[n] = s[n]);
            if (c) c(i);
          }
          for (t && t(r); u < a.length; u++) o = a[u], i.o(e, o) && e[o] && e[o][0](), e[o] = 0;
        },
        r = self["webpackChunk:NRBA-1.238.0.PROD"] = self["webpackChunk:NRBA-1.238.0.PROD"] || [];
      r.forEach(t.bind(null, 0)), r.push = t.bind(null, r.push.bind(r));
    })(), (() => {
      var e = i(50);
      class t {
        addPageAction(t, r) {
          (0, e.Z)("Call to agent api addPageAction failed. The session trace feature is not currently initialized.");
        }
        setPageViewName(t, r) {
          (0, e.Z)("Call to agent api setPageViewName failed. The page view feature is not currently initialized.");
        }
        setCustomAttribute(t, r, n) {
          (0, e.Z)("Call to agent api setCustomAttribute failed. The js errors feature is not currently initialized.");
        }
        noticeError(t, r) {
          (0, e.Z)("Call to agent api noticeError failed. The js errors feature is not currently initialized.");
        }
        setUserId(t) {
          (0, e.Z)("Call to agent api setUserId failed. The js errors feature is not currently initialized.");
        }
        setApplicationVersion(t) {
          (0, e.Z)("Call to agent api setApplicationVersion failed. The agent is not currently initialized.");
        }
        setErrorHandler(t) {
          (0, e.Z)("Call to agent api setErrorHandler failed. The js errors feature is not currently initialized.");
        }
        finished(t) {
          (0, e.Z)("Call to agent api finished failed. The page action feature is not currently initialized.");
        }
        addRelease(t, r) {
          (0, e.Z)("Call to agent api addRelease failed. The agent is not currently initialized.");
        }
      }
      var r = i(3325),
        n = i(5763);
      const o = Object.values(r.D);
      function a(e) {
        const t = {};
        return o.forEach(r => {
          t[r] = function (e, t) {
            return !1 !== (0, n.Mt)(t, "".concat(e, ".enabled"));
          }(r, e);
        }), t;
      }
      var s = i(9144);
      var c = i(5546),
        u = i(385),
        d = i(8e3),
        l = i(5938),
        f = i(3960);
      class h extends l.W {
        constructor(e, t, r) {
          let n = !(arguments.length > 3 && void 0 !== arguments[3]) || arguments[3];
          super(e, t, r), this.auto = n, this.abortHandler, this.featAggregate, this.onAggregateImported, n && (0, d.R)(e, r);
        }
        importAggregator() {
          let t = arguments.length > 0 && void 0 !== arguments[0] ? arguments[0] : {};
          if (this.featAggregate || !this.auto) return;
          const r = u.il && !0 === (0, n.Mt)(this.agentIdentifier, "privacy.cookies_enabled");
          let o;
          this.onAggregateImported = new Promise(e => {
            o = e;
          });
          const a = async () => {
            let n;
            try {
              if (r) {
                const {
                  setupAgentSession: e
                } = await i.e(111).then(i.bind(i, 3228));
                n = e(this.agentIdentifier);
              }
            } catch (t) {
              (0, e.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.", t);
            }
            try {
              if (!this.shouldImportAgg(this.featureName, n)) return (0, d.L)(this.agentIdentifier, this.featureName), void o(!1);
              const {
                  lazyFeatureLoader: e
                } = await i.e(111).then(i.bind(i, 8582)),
                {
                  Aggregate: r
                } = await e(this.featureName, "aggregate");
              this.featAggregate = new r(this.agentIdentifier, this.aggregator, t), o(!0);
            } catch (t) {
              (0, e.Z)("Downloading and initializing ".concat(this.featureName, " failed..."), t), this.abortHandler?.(), o(!1);
            }
          };
          u.il ? (0, f.b)(() => a(), !0) : a();
        }
        shouldImportAgg(e, t) {
          return e !== r.D.sessionReplay || !!n.Yu.MO && !1 !== (0, n.Mt)(this.agentIdentifier, "session_trace.enabled") && (!!t?.isNew || !!t?.state.sessionReplay);
        }
      }
      var p = i(7633),
        g = i(7894);
      class m extends h {
        static featureName = p.t9;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          if (super(e, t, p.t9, i), ("undefined" == typeof PerformanceNavigationTiming || u.Tt) && "undefined" != typeof PerformanceTiming) {
            const t = (0, n.OP)(e);
            t[p.Dz] = Math.max(Date.now() - t.offset, 0), (0, f.K)(() => t[p.qw] = Math.max((0, g.z)() - t[p.Dz], 0)), (0, f.b)(() => {
              const e = (0, g.z)();
              t[p.OJ] = Math.max(e - t[p.Dz], 0), (0, c.p)("timing", ["load", e], void 0, r.D.pageViewTiming, this.ee);
            });
          }
          this.importAggregator();
        }
      }
      var v = i(1117),
        b = i(1284);
      class y extends v.w {
        constructor(e) {
          super(e), this.aggregatedData = {};
        }
        store(e, t, r, n, i) {
          var o = this.getBucket(e, t, r, i);
          return o.metrics = function (e, t) {
            t || (t = {
              count: 0
            });
            return t.count += 1, (0, b.D)(e, function (e, r) {
              t[e] = w(r, t[e]);
            }), t;
          }(n, o.metrics), o;
        }
        merge(e, t, r, n, i) {
          var o = this.getBucket(e, t, n, i);
          if (o.metrics) {
            var a = o.metrics;
            a.count += r.count, (0, b.D)(r, function (e, t) {
              if ("count" !== e) {
                var n = a[e],
                  i = r[e];
                i && !i.c ? a[e] = w(i.t, n) : a[e] = function (e, t) {
                  if (!t) return e;
                  t.c || (t = A(t.t));
                  return t.min = Math.min(e.min, t.min), t.max = Math.max(e.max, t.max), t.t += e.t, t.sos += e.sos, t.c += e.c, t;
                }(i, a[e]);
              }
            });
          } else o.metrics = r;
        }
        storeMetric(e, t, r, n) {
          var i = this.getBucket(e, t, r);
          return i.stats = w(n, i.stats), i;
        }
        getBucket(e, t, r, n) {
          this.aggregatedData[e] || (this.aggregatedData[e] = {});
          var i = this.aggregatedData[e][t];
          return i || (i = this.aggregatedData[e][t] = {
            params: r || {}
          }, n && (i.custom = n)), i;
        }
        get(e, t) {
          return t ? this.aggregatedData[e] && this.aggregatedData[e][t] : this.aggregatedData[e];
        }
        take(e) {
          for (var t = {}, r = "", n = !1, i = 0; i < e.length; i++) t[r = e[i]] = x(this.aggregatedData[r]), t[r].length && (n = !0), delete this.aggregatedData[r];
          return n ? t : null;
        }
      }
      function w(e, t) {
        return null == e ? function (e) {
          e ? e.c++ : e = {
            c: 1
          };
          return e;
        }(t) : t ? (t.c || (t = A(t.t)), t.c += 1, t.t += e, t.sos += e * e, e > t.max && (t.max = e), e < t.min && (t.min = e), t) : {
          t: e
        };
      }
      function A(e) {
        return {
          t: e,
          min: e,
          max: e,
          sos: e * e,
          c: 1
        };
      }
      function x(e) {
        return "object" != typeof e ? [] : (0, b.D)(e, E);
      }
      function E(e, t) {
        return t;
      }
      var T = i(8632),
        _ = i(4402),
        D = i(4351);
      var j = i(7956),
        C = i(3239),
        N = i(9251);
      class O extends h {
        static featureName = N.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, N.t, r), u.il && ((0, n.OP)(e).initHidden = Boolean("hidden" === document.visibilityState), (0, j.N)(() => (0, c.p)("docHidden", [(0, g.z)()], void 0, N.t, this.ee), !0), (0, C.bP)("pagehide", () => (0, c.p)("winPagehide", [(0, g.z)()], void 0, N.t, this.ee)), this.importAggregator());
        }
      }
      var S = i(3081);
      class P extends h {
        static featureName = S.t9;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, S.t9, r), this.importAggregator();
        }
      }
      var R = i(6660);
      class I {
        constructor(e, t, r, n) {
          this.name = "UncaughtError", this.message = e, this.sourceURL = t, this.line = r, this.column = n;
        }
      }
      class k extends h {
        static featureName = R.t;
        #e = new Set();
        constructor(e, t) {
          let n = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, R.t, n);
          try {
            this.removeOnAbort = new AbortController();
          } catch (e) {}
          this.ee.on("fn-err", (e, t, n) => {
            this.abortHandler && !this.#e.has(n) && (this.#e.add(n), (0, c.p)("err", [this.#t(n), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }), this.ee.on("internal-error", e => {
            this.abortHandler && (0, c.p)("ierr", [this.#t(e), (0, g.z)(), !0], void 0, r.D.jserrors, this.ee);
          }), u._A.addEventListener("unhandledrejection", e => {
            this.abortHandler && (0, c.p)("err", [this.#r(e), (0, g.z)(), !1, {
              unhandledPromiseRejection: 1
            }], void 0, r.D.jserrors, this.ee);
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), u._A.addEventListener("error", e => {
            this.abortHandler && (this.#e.has(e.error) ? this.#e.delete(e.error) : (0, c.p)("err", [this.#n(e), (0, g.z)()], void 0, r.D.jserrors, this.ee));
          }, (0, C.m$)(!1, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
        }
        #i() {
          this.removeOnAbort?.abort(), this.#e.clear(), this.abortHandler = void 0;
        }
        #t(e) {
          return e instanceof Error ? e : void 0 !== e?.message ? new I(e.message, e.filename || e.sourceURL, e.lineno || e.line, e.colno || e.col) : new I("string" == typeof e ? e : (0, D.P)(e));
        }
        #r(e) {
          let t = "Unhandled Promise Rejection: ";
          if (e?.reason instanceof Error) try {
            return e.reason.message = t + e.reason.message, e.reason;
          } catch (t) {
            return e.reason;
          }
          if (void 0 === e.reason) return new I(t);
          const r = this.#t(e.reason);
          return r.message = t + r.message, r;
        }
        #n(e) {
          return e.error instanceof Error ? e.error : new I(e.message, e.filename, e.lineno, e.colno);
        }
      }
      var H = i(2210);
      let z = 1;
      const L = "nr@id";
      function M(e) {
        const t = typeof e;
        return !e || "object" !== t && "function" !== t ? -1 : e === u._A ? 0 : (0, H.X)(e, L, function () {
          return z++;
        });
      }
      function B(e) {
        if ("string" == typeof e && e.length) return e.length;
        if ("object" == typeof e) {
          if ("undefined" != typeof ArrayBuffer && e instanceof ArrayBuffer && e.byteLength) return e.byteLength;
          if ("undefined" != typeof Blob && e instanceof Blob && e.size) return e.size;
          if (!("undefined" != typeof FormData && e instanceof FormData)) try {
            return (0, D.P)(e).length;
          } catch (e) {
            return;
          }
        }
      }
      var F = i(1214),
        U = i(7243);
      class q {
        constructor(e) {
          this.agentIdentifier = e;
        }
        generateTracePayload(e) {
          if (!this.shouldGenerateTrace(e)) return null;
          var t = (0, n.DL)(this.agentIdentifier);
          if (!t) return null;
          var r = (t.accountID || "").toString() || null,
            i = (t.agentID || "").toString() || null,
            o = (t.trustKey || "").toString() || null;
          if (!r || !i) return null;
          var a = (0, _.M)(),
            s = (0, _.Ht)(),
            c = Date.now(),
            u = {
              spanId: a,
              traceId: s,
              timestamp: c
            };
          return (e.sameOrigin || this.isAllowedOrigin(e) && this.useTraceContextHeadersForCors()) && (u.traceContextParentHeader = this.generateTraceContextParentHeader(a, s), u.traceContextStateHeader = this.generateTraceContextStateHeader(a, c, r, i, o)), (e.sameOrigin && !this.excludeNewrelicHeader() || !e.sameOrigin && this.isAllowedOrigin(e) && this.useNewrelicHeaderForCors()) && (u.newrelicHeader = this.generateTraceHeader(a, s, c, r, i, o)), u;
        }
        generateTraceContextParentHeader(e, t) {
          return "00-" + t + "-" + e + "-01";
        }
        generateTraceContextStateHeader(e, t, r, n, i) {
          return i + "@nr=0-1-" + r + "-" + n + "-" + e + "----" + t;
        }
        generateTraceHeader(e, t, r, n, i, o) {
          if (!("function" == typeof u._A?.btoa)) return null;
          var a = {
            v: [0, 1],
            d: {
              ty: "Browser",
              ac: n,
              ap: i,
              id: e,
              tr: t,
              ti: r
            }
          };
          return o && n !== o && (a.d.tk = o), btoa((0, D.P)(a));
        }
        shouldGenerateTrace(e) {
          return this.isDtEnabled() && this.isAllowedOrigin(e);
        }
        isAllowedOrigin(e) {
          var t = !1,
            r = {};
          if ((0, n.Mt)(this.agentIdentifier, "distributed_tracing") && (r = (0, n.P_)(this.agentIdentifier).distributed_tracing), e.sameOrigin) t = !0;else if (r.allowed_origins instanceof Array) for (var i = 0; i < r.allowed_origins.length; i++) {
            var o = (0, U.e)(r.allowed_origins[i]);
            if (e.hostname === o.hostname && e.protocol === o.protocol && e.port === o.port) {
              t = !0;
              break;
            }
          }
          return t;
        }
        isDtEnabled() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.enabled;
        }
        excludeNewrelicHeader() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.exclude_newrelic_header;
        }
        useNewrelicHeaderForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !1 !== e.cors_use_newrelic_header;
        }
        useTraceContextHeadersForCors() {
          var e = (0, n.Mt)(this.agentIdentifier, "distributed_tracing");
          return !!e && !!e.cors_use_tracecontext_headers;
        }
      }
      var Z = i(7825),
        V = ["load", "error", "abort", "timeout"],
        G = V.length,
        W = n.Yu.REQ,
        X = n.Yu.XHR;
      class Q extends h {
        static featureName = Z.t;
        constructor(e, t) {
          let i = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, Z.t, i), (0, n.OP)(e).xhrWrappable && (this.dt = new q(e), this.handler = (e, t, r, n) => (0, c.p)(e, t, r, n, this.ee), (0, F.u5)(this.ee), (0, F.Kf)(this.ee), function (e, t, i, o) {
            function a(e) {
              var t = this;
              t.totalCbs = 0, t.called = 0, t.cbTime = 0, t.end = E, t.ended = !1, t.xhrGuids = {}, t.lastSize = null, t.loadCaptureCalled = !1, t.params = this.params || {}, t.metrics = this.metrics || {}, e.addEventListener("load", function (r) {
                _(t, e);
              }, (0, C.m$)(!1)), u.IF || e.addEventListener("progress", function (e) {
                t.lastSize = e.loaded;
              }, (0, C.m$)(!1));
            }
            function s(e) {
              this.params = {
                method: e[0]
              }, T(this, e[1]), this.metrics = {};
            }
            function c(t, r) {
              var i = (0, n.DL)(e);
              i.xpid && this.sameOrigin && r.setRequestHeader("X-NewRelic-ID", i.xpid);
              var a = o.generateTracePayload(this.parsedOrigin);
              if (a) {
                var s = !1;
                a.newrelicHeader && (r.setRequestHeader("newrelic", a.newrelicHeader), s = !0), a.traceContextParentHeader && (r.setRequestHeader("traceparent", a.traceContextParentHeader), a.traceContextStateHeader && r.setRequestHeader("tracestate", a.traceContextStateHeader), s = !0), s && (this.dt = a);
              }
            }
            function d(e, r) {
              var n = this.metrics,
                i = e[0],
                o = this;
              if (n && i) {
                var a = B(i);
                a && (n.txSize = a);
              }
              this.startTime = (0, g.z)(), this.listener = function (e) {
                try {
                  "abort" !== e.type || o.loadCaptureCalled || (o.params.aborted = !0), ("load" !== e.type || o.called === o.totalCbs && (o.onloadCalled || "function" != typeof r.onload) && "function" == typeof o.end) && o.end(r);
                } catch (e) {
                  try {
                    t.emit("internal-error", [e]);
                  } catch (e) {}
                }
              };
              for (var s = 0; s < G; s++) r.addEventListener(V[s], this.listener, (0, C.m$)(!1));
            }
            function l(e, t, r) {
              this.cbTime += e, t ? this.onloadCalled = !0 : this.called += 1, this.called !== this.totalCbs || !this.onloadCalled && "function" == typeof r.onload || "function" != typeof this.end || this.end(r);
            }
            function f(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && !this.xhrGuids[r] && (this.xhrGuids[r] = !0, this.totalCbs += 1);
            }
            function h(e, t) {
              var r = "" + M(e) + !!t;
              this.xhrGuids && this.xhrGuids[r] && (delete this.xhrGuids[r], this.totalCbs -= 1);
            }
            function p() {
              this.endTime = (0, g.z)();
            }
            function m(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-added", [e[1], e[2]], r);
            }
            function v(e, r) {
              r instanceof X && "load" === e[0] && t.emit("xhr-load-removed", [e[1], e[2]], r);
            }
            function b(e, t, r) {
              t instanceof X && ("onload" === r && (this.onload = !0), ("load" === (e[0] && e[0].type) || this.onload) && (this.xhrCbStart = (0, g.z)()));
            }
            function y(e, r) {
              this.xhrCbStart && t.emit("xhr-cb-time", [(0, g.z)() - this.xhrCbStart, this.onload, r], r);
            }
            function w(e) {
              var t,
                r = e[1] || {};
              if ("string" == typeof e[0] ? 0 === (t = e[0]).length && u.il && (t = "" + u._A.location.href) : e[0] && e[0].url ? t = e[0].url : u._A?.URL && e[0] && e[0] instanceof URL ? t = e[0].href : "function" == typeof e[0].toString && (t = e[0].toString()), "string" == typeof t && 0 !== t.length) {
                t && (this.parsedOrigin = (0, U.e)(t), this.sameOrigin = this.parsedOrigin.sameOrigin);
                var n = o.generateTracePayload(this.parsedOrigin);
                if (n && (n.newrelicHeader || n.traceContextParentHeader)) if (e[0] && e[0].headers) s(e[0].headers, n) && (this.dt = n);else {
                  var i = {};
                  for (var a in r) i[a] = r[a];
                  i.headers = new Headers(r.headers || {}), s(i.headers, n) && (this.dt = n), e.length > 1 ? e[1] = i : e.push(i);
                }
              }
              function s(e, t) {
                var r = !1;
                return t.newrelicHeader && (e.set("newrelic", t.newrelicHeader), r = !0), t.traceContextParentHeader && (e.set("traceparent", t.traceContextParentHeader), t.traceContextStateHeader && e.set("tracestate", t.traceContextStateHeader), r = !0), r;
              }
            }
            function A(e, t) {
              this.params = {}, this.metrics = {}, this.startTime = (0, g.z)(), this.dt = t, e.length >= 1 && (this.target = e[0]), e.length >= 2 && (this.opts = e[1]);
              var r,
                n = this.opts || {},
                i = this.target;
              "string" == typeof i ? r = i : "object" == typeof i && i instanceof W ? r = i.url : u._A?.URL && "object" == typeof i && i instanceof URL && (r = i.href), T(this, r);
              var o = ("" + (i && i instanceof W && i.method || n.method || "GET")).toUpperCase();
              this.params.method = o, this.txSize = B(n.body) || 0;
            }
            function x(e, t) {
              var n;
              this.endTime = (0, g.z)(), this.params || (this.params = {}), this.params.status = t ? t.status : 0, "string" == typeof this.rxSize && this.rxSize.length > 0 && (n = +this.rxSize);
              var o = {
                txSize: this.txSize,
                rxSize: n,
                duration: (0, g.z)() - this.startTime
              };
              i("xhr", [this.params, o, this.startTime, this.endTime, "fetch"], this, r.D.ajax);
            }
            function E(e) {
              var t = this.params,
                n = this.metrics;
              if (!this.ended) {
                this.ended = !0;
                for (var o = 0; o < G; o++) e.removeEventListener(V[o], this.listener, !1);
                t.aborted || (n.duration = (0, g.z)() - this.startTime, this.loadCaptureCalled || 4 !== e.readyState ? null == t.status && (t.status = 0) : _(this, e), n.cbTime = this.cbTime, i("xhr", [t, n, this.startTime, this.endTime, "xhr"], this, r.D.ajax));
              }
            }
            function T(e, t) {
              var r = (0, U.e)(t),
                n = e.params;
              n.hostname = r.hostname, n.port = r.port, n.protocol = r.protocol, n.host = r.hostname + ":" + r.port, n.pathname = r.pathname, e.parsedOrigin = r, e.sameOrigin = r.sameOrigin;
            }
            function _(e, t) {
              e.params.status = t.status;
              var r = function (e, t) {
                var r = e.responseType;
                return "json" === r && null !== t ? t : "arraybuffer" === r || "blob" === r || "json" === r ? B(e.response) : "text" === r || "" === r || void 0 === r ? B(e.responseText) : void 0;
              }(t, e.lastSize);
              if (r && (e.metrics.rxSize = r), e.sameOrigin) {
                var n = t.getResponseHeader("X-NewRelic-App-Data");
                n && (e.params.cat = n.split(", ").pop());
              }
              e.loadCaptureCalled = !0;
            }
            t.on("new-xhr", a), t.on("open-xhr-start", s), t.on("open-xhr-end", c), t.on("send-xhr-start", d), t.on("xhr-cb-time", l), t.on("xhr-load-added", f), t.on("xhr-load-removed", h), t.on("xhr-resolved", p), t.on("addEventListener-end", m), t.on("removeEventListener-end", v), t.on("fn-end", y), t.on("fetch-before-start", w), t.on("fetch-start", A), t.on("fn-start", b), t.on("fetch-done", x);
          }(e, this.ee, this.handler, this.dt), this.importAggregator());
        }
      }
      var K = i(3614);
      const {
        BST_RESOURCE: Y,
        RESOURCE: J,
        START: ee,
        END: te,
        FEATURE_NAME: re,
        FN_END: ne,
        FN_START: ie,
        PUSH_STATE: oe
      } = K;
      var ae = i(7836);
      const {
        FEATURE_NAME: se,
        START: ce,
        END: ue,
        BODY: de,
        CB_END: le,
        JS_TIME: fe,
        FETCH: he,
        FN_START: pe,
        CB_START: ge,
        FN_END: me
      } = ae;
      var ve = i(4649);
      class be extends h {
        static featureName = ve.t;
        constructor(e, t) {
          let r = !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2];
          super(e, t, ve.t, r), this.importAggregator();
        }
      }
      new class extends t {
        constructor(t) {
          let r = arguments.length > 1 && void 0 !== arguments[1] ? arguments[1] : (0, _.ky)(16);
          super(), u._A ? (this.agentIdentifier = r, this.sharedAggregator = new y({
            agentIdentifier: this.agentIdentifier
          }), this.features = {}, this.desiredFeatures = new Set(t.features || []), this.desiredFeatures.add(m), Object.assign(this, (0, s.j)(this.agentIdentifier, t, t.loaderType || "agent")), this.start()) : (0, e.Z)("Failed to initial the agent. Could not determine the runtime environment.");
        }
        get config() {
          return {
            info: (0, n.C5)(this.agentIdentifier),
            init: (0, n.P_)(this.agentIdentifier),
            loader_config: (0, n.DL)(this.agentIdentifier),
            runtime: (0, n.OP)(this.agentIdentifier)
          };
        }
        start() {
          const t = "features";
          try {
            const n = a(this.agentIdentifier),
              i = [...this.desiredFeatures];
            i.sort((e, t) => r.p[e.featureName] - r.p[t.featureName]), i.forEach(t => {
              if (n[t.featureName] || t.featureName === r.D.pageViewEvent) {
                const i = function (e) {
                  switch (e) {
                    case r.D.ajax:
                      return [r.D.jserrors];
                    case r.D.sessionTrace:
                      return [r.D.ajax, r.D.pageViewEvent];
                    case r.D.sessionReplay:
                      return [r.D.sessionTrace];
                    case r.D.pageViewTiming:
                      return [r.D.pageViewEvent];
                    default:
                      return [];
                  }
                }(t.featureName);
                i.every(e => n[e]) || (0, e.Z)("".concat(t.featureName, " is enabled but one or more dependent features has been disabled (").concat((0, D.P)(i), "). This may cause unintended consequences or missing data...")), this.features[t.featureName] = new t(this.agentIdentifier, this.sharedAggregator);
              }
            }), (0, T.Qy)(this.agentIdentifier, this.features, t);
          } catch (r) {
            (0, e.Z)("Failed to initialize all enabled instrument classes (agent aborted) -", r);
            for (const e in this.features) this.features[e].abortHandler?.();
            const n = (0, T.fP)();
            return delete n.initializedAgents[this.agentIdentifier]?.api, delete n.initializedAgents[this.agentIdentifier]?.[t], delete this.sharedAggregator, n.ee?.abort(), delete n.ee?.get(this.agentIdentifier), !1;
          }
        }
        addToTrace(t) {
          (0, e.Z)("Call to agent api addToTrace failed. The page action feature is not currently initialized.");
        }
        setCurrentRouteName(t) {
          (0, e.Z)("Call to agent api setCurrentRouteName failed. The spa feature is not currently initialized.");
        }
        interaction() {
          (0, e.Z)("Call to agent api interaction failed. The spa feature is not currently initialized.");
        }
      }({
        features: [Q, m, O, class extends h {
          static featureName = re;
          constructor(e, t) {
            if (super(e, t, re, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            const n = this.ee;
            let i;
            (0, F.QU)(n), this.eventsEE = (0, F.em)(n), this.eventsEE.on(ie, function (e, t) {
              this.bstStart = (0, g.z)();
            }), this.eventsEE.on(ne, function (e, t) {
              (0, c.p)("bst", [e[0], t, this.bstStart, (0, g.z)()], void 0, r.D.sessionTrace, n);
            }), n.on(oe + ee, function (e) {
              this.time = (0, g.z)(), this.startPath = location.pathname + location.hash;
            }), n.on(oe + te, function (e) {
              (0, c.p)("bstHist", [location.pathname + location.hash, this.startPath, this.time], void 0, r.D.sessionTrace, n);
            });
            try {
              i = new PerformanceObserver(e => {
                const t = e.getEntries();
                (0, c.p)(Y, [t], void 0, r.D.sessionTrace, n);
              }), i.observe({
                type: J,
                buffered: !0
              });
            } catch (e) {}
            this.importAggregator({
              resourceObserver: i
            });
          }
        }, P, be, k, class extends h {
          static featureName = se;
          constructor(e, t) {
            if (super(e, t, se, !(arguments.length > 2 && void 0 !== arguments[2]) || arguments[2]), !u.il) return;
            if (!(0, n.OP)(e).xhrWrappable) return;
            try {
              this.removeOnAbort = new AbortController();
            } catch (e) {}
            let r,
              i = 0;
            const o = this.ee.get("tracer"),
              a = (0, F._L)(this.ee),
              s = (0, F.Lg)(this.ee),
              c = (0, F.BV)(this.ee),
              d = (0, F.Kf)(this.ee),
              l = this.ee.get("events"),
              f = (0, F.u5)(this.ee),
              h = (0, F.QU)(this.ee),
              p = (0, F.Gm)(this.ee);
            function m(e, t) {
              h.emit("newURL", ["" + window.location, t]);
            }
            function v() {
              i++, r = window.location.hash, this[pe] = (0, g.z)();
            }
            function b() {
              i--, window.location.hash !== r && m(0, !0);
              var e = (0, g.z)();
              this[fe] = ~~this[fe] + e - this[pe], this[me] = e;
            }
            function y(e, t) {
              e.on(t, function () {
                this[t] = (0, g.z)();
              });
            }
            this.ee.on(pe, v), s.on(ge, v), a.on(ge, v), this.ee.on(me, b), s.on(le, b), a.on(le, b), this.ee.buffer([pe, me, "xhr-resolved"], this.featureName), l.buffer([pe], this.featureName), c.buffer(["setTimeout" + ue, "clearTimeout" + ce, pe], this.featureName), d.buffer([pe, "new-xhr", "send-xhr" + ce], this.featureName), f.buffer([he + ce, he + "-done", he + de + ce, he + de + ue], this.featureName), h.buffer(["newURL"], this.featureName), p.buffer([pe], this.featureName), s.buffer(["propagate", ge, le, "executor-err", "resolve" + ce], this.featureName), o.buffer([pe, "no-" + pe], this.featureName), a.buffer(["new-jsonp", "cb-start", "jsonp-error", "jsonp-end"], this.featureName), y(f, he + ce), y(f, he + "-done"), y(a, "new-jsonp"), y(a, "jsonp-end"), y(a, "cb-start"), h.on("pushState-end", m), h.on("replaceState-end", m), window.addEventListener("hashchange", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("load", m, (0, C.m$)(!0, this.removeOnAbort?.signal)), window.addEventListener("popstate", function () {
              m(0, i > 1);
            }, (0, C.m$)(!0, this.removeOnAbort?.signal)), this.abortHandler = this.#i, this.importAggregator();
          }
          #i() {
            this.removeOnAbort?.abort(), this.abortHandler = void 0;
          }
        }],
        loaderType: "spa"
      });
    })();
  })();
})()</script>
<link rel="shortcut icon" href="https://sciencedirect.elseviercdn.cn/shared-assets/103/images/favSD.ico" type="image/x-icon" />
<link rel="icon" href="https://sciencedirect.elseviercdn.cn/shared-assets/103/images/favSD.ico" type="image/x-icon">
<link rel="stylesheet" href="https://sciencedirect.elseviercdn.cn/prod/f624014e8238d0b70668b2e1dcc65fef7457d17f/arp.css">
<link href="//cdn.pendo.io" rel="dns-prefetch" />
<link href="https://cdn.pendo.io" rel="preconnect" crossorigin="anonymous" />
<link rel="dns-prefetch" href="https://smetrics.elsevier.com">
<script type="f099078ee6a757eb17f3100f-text/javascript">
        var targetServerState = JSON.stringify({"4D6368F454EC41940A4C98A6@AdobeOrg":{"sdid":{"supplementalDataIDCurrent":"451427F2EF16AF1E-475C4A30602A1A23","supplementalDataIDCurrentConsumed":{"payload:target-global-mbox":true},"supplementalDataIDLastConsumed":{}}}});
        window.appData = window.appData || [];
        window.pageTargeting = {"region":"us-east-1","platform":"sdtech","entitled":false,"crawler":"","journal":"Information Processing & Management","auth":"AE"};
        window.arp = {
          config: {"adobeSuite":"elsevier-sd-prod","arsUrl":"https://ars.els-cdn.com","enableAskCopilot":false,"recommendationsFeedback":{"enabled":true,"url":"https://feedback.recs.d.elsevier.com/raw/events","timeout":60000},"googleMapsApiKey":"AIzaSyCBYU6I6lrbEU6wQXUEIte3NwGtm3jwHQc","mediaBaseUrl":"https://ars.els-cdn.com/content/image/","strictMode":false,"seamlessAccess":{"enableSeamlessAccess":true,"scriptUrl":"https://unpkg.com/@theidentityselector/thiss-ds@1.0.13/dist/thiss-ds.js","persistenceUrl":"https://service.seamlessaccess.org/ps/","persistenceContext":"seamlessaccess.org","scienceDirectUrl":"https://www.sciencedirect.com","shibAuthUrl":"https://auth.elsevier.com/ShibAuth/institutionLogin"},"universalPdf":{"enableUniversalPdf":true,"universalPdfUrl":"https://static.mendeley.com/view-pdf-component/0.8.9/dist/view-pdf-element.js","integratorId":{"other":8301,"accessbar":10879}},"reaxys":{"apiUrl":"https://reaxys-sdlc.reaxys.com","origin":"sciencedirect","queryBuilderHostPath":"https://www.reaxys.com/reaxys/secured/hopinto.do","url":"https://www.reaxys.com"},"oneTrustCookie":{"enabled":true},"ssrn":{"url":"https://papers.ssrn.com","path":"/sol3/papers.cfm"},"assetRoute":"https://sciencedirect.elseviercdn.cn/prod/f624014e8238d0b70668b2e1dcc65fef7457d17f"},
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
        window.addEventListener('beforeprint', () => pendo.onGuideDismissed());
      </script>
</head>
<body>
<noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics.elsevier.com/b/ss/elsevier-sd-prod/1/G.4--NS/1713677274659?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AANON_GUEST&c1=ae%3A228598&c12=ae%3A12975512 />
    </noscript>
<a class="anchor sr-only sr-only-focusable u-display-inline anchor-default" href="#screen-reader-main-content" data-reactroot><span class="anchor-text">Skip to main content</span></a><a class="anchor sr-only sr-only-focusable u-display-inline anchor-default" href="#screen-reader-main-title" data-reactroot><span class="anchor-text">Skip to article</span></a>
<div data-iso-key="_0"><div class="App" id="app" data-aa-name="root"><div class="page"><div class="sd-flex-container"><div class="sd-flex-content"><header id="gh-cnt"><div id="gh-main-cnt" class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-l-hor-from-xl"><a id="gh-branding" class="u-flex-center-ver" href="/" aria-label="ScienceDirect home page" data-aa-region="header" data-aa-name="ScienceDirect"><img class="gh-logo" src="https://sciencedirect.elseviercdn.cn/shared-assets/24/images/elsevier-non-solus-new-grey.svg" alt="Elsevier logo" height="48" width="54" /><svg xmlns="http://www.w3.org/2000/svg" version="1.1" height="15" viewBox="0 0 190 23" role="img" class="gh-wordmark u-margin-s-left" aria-labelledby="gh-wm-science-direct" focusable="false" aria-hidden="true" alt="ScienceDirect Wordmark"><title id="gh-wm-science-direct">ScienceDirect</title><g><path fill="#EB6500" d="M3.81 6.9c0-1.48 0.86-3.04 3.7-3.04 1.42 0 3.1 0.43 4.65 1.32l0.13-2.64c-1.42-0.63-2.97-0.96-4.78-0.96 -4.62 0-6.6 2.44-6.6 5.45 0 5.61 8.78 6.14 8.78 9.93 0 1.48-1.15 3.04-3.86 3.04 -1.72 0-3.4-0.56-4.72-1.39l-0.36 2.64c1.55 0.76 3.57 1.06 5.15 1.06 4.26 0 6.7-2.48 6.7-5.51C12.59 11.49 3.81 10.76 3.81 6.9M20.27 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.14 0-4.55-1.71-4.55-5.91C17.93 10.2 20.01 9.18 20.27 9.01"></path><rect x="29.42" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M30.67 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.68-0.96 1.68-1.88C32.35 1.55 31.56 0.7 30.67 0.7M48.06 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H48.06M39.91 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C38.56 10.27 39.71 9.37 39.91 9.18zM58.82 6.57c-2.24 0-3.63 1.12-4.85 2.61l-0.4-2.21h-2.34l0.13 1.19c0.1 0.76 0.13 1.78 0.13 2.97v10.79h2.54V11.88c0.69-0.96 2.15-2.48 2.48-2.64 0.23-0.13 1.29-0.4 2.08-0.4 2.28 0 2.48 1.15 2.54 3.43 0.03 1.19 0.03 3.17 0.03 3.17 0.03 3-0.1 6.47-0.1 6.47h2.54c0 0 0.07-4.49 0.07-6.96 0-1.48 0.03-2.97-0.1-4.46C63.31 7.43 61.49 6.57 58.82 6.57M72.12 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.15-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.68 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C69.77 10.2 71.85 9.18 72.12 9.01M92.74 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.88-1.95-4.88-5.51v-0.49H92.74M84.59 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C83.24 10.27 84.39 9.37 84.59 9.18zM103.9 1.98h-7.13v19.93h6.83c7.26 0 9.77-5.68 9.77-10.03C113.37 7.33 110.93 1.98 103.9 1.98M103.14 19.8h-3.76V4.1h4.09c5.38 0 6.96 4.39 6.96 7.79C110.43 16.87 108.19 19.8 103.14 19.8zM118.38 0.7c-0.92 0-1.65 0.92-1.65 1.81 0 0.93 0.76 1.85 1.65 1.85 0.89 0 1.69-0.96 1.69-1.88C120.07 1.55 119.28 0.7 118.38 0.7"></path><rect x="117.13" y="6.97" fill="#EB6500" width="2.54" height="14.95"></rect><path fill="#EB6500" d="M130.2 6.6c-1.62 0-2.87 1.45-3.4 2.74l-0.43-2.37h-2.34l0.13 1.19c0.1 0.76 0.13 1.75 0.13 2.9v10.86h2.54v-9.51c0.53-1.29 1.72-3.7 3.17-3.7 0.96 0 1.06 0.99 1.06 1.22l2.08-0.6V9.18c0-0.03-0.03-0.17-0.06-0.4C132.8 7.36 131.91 6.6 130.2 6.6M145.87 14.13c0-5.18-1.42-7.56-6.01-7.56 -3.86 0-6.67 2.77-6.67 7.92 0 4.92 2.97 7.82 6.73 7.82 2.81 0 4.36-0.63 5.68-1.42l-0.2-2.31c-0.89 0.79-2.94 1.55-4.69 1.55 -3.14 0-4.89-1.95-4.89-5.51v-0.49H145.87M137.72 9.18c0.17-0.17 1.29-0.46 1.98-0.46 2.48 0 3.76 0.53 3.86 3.43h-7.46C136.37 10.27 137.52 9.37 137.72 9.18zM153.23 9.01c0.23-0.13 0.69-0.26 1.72-0.26 1.72 0 2.41 0.3 2.41 1.58h2.38c0-0.36 0-0.79-0.03-1.09 -0.23-1.98-2.14-2.67-4.88-2.67 -3 0-6.7 2.31-6.7 7.76 0 5.22 2.77 7.99 6.63 7.99 1.69 0 3.47-0.36 4.95-1.39l-0.2-2.31c-0.99 0.82-2.84 1.52-4.06 1.52 -2.15 0-4.55-1.71-4.55-5.91C150.89 10.2 152.97 9.18 153.23 9.01M170 19.44c-0.92 0.36-1.72 0.69-2.51 0.69 -1.16 0-1.58-0.66-1.58-2.34V8.95h3.93V6.97h-3.93V2.97h-2.48v3.99h-2.71v1.98h2.71v9.67c0 2.64 1.39 3.73 3.33 3.73 1.15 0 2.54-0.39 3.43-0.79L170 19.44M173.68 5.96c-1.09 0-2-0.87-2-1.97 0-1.1 0.91-1.97 2-1.97s1.98 0.88 1.98 1.98C175.66 5.09 174.77 5.96 173.68 5.96zM173.67 2.46c-0.85 0-1.54 0.67-1.54 1.52 0 0.85 0.69 1.54 1.54 1.54 0.85 0 1.54-0.69 1.54-1.54C175.21 3.13 174.52 2.46 173.67 2.46zM174.17 5.05c-0.09-0.09-0.17-0.19-0.25-0.3l-0.41-0.56h-0.16v0.87h-0.39V2.92c0.22-0.01 0.47-0.03 0.66-0.03 0.41 0 0.82 0.16 0.82 0.64 0 0.29-0.21 0.55-0.49 0.63 0.23 0.32 0.45 0.62 0.73 0.91H174.17zM173.56 3.22l-0.22 0.01v0.63h0.22c0.26 0 0.43-0.05 0.43-0.34C174 3.28 173.83 3.21 173.56 3.22z"></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label="links" class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action anchor-default" href="/browse/journals-and-books" data-aa-region="header" data-aa-name="Journals &amp; Books"><span class="anchor-text">Journals &amp; Books</span></a></li></ul></nav><nav aria-label="utilities" class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-move-to-spine gh-help-button gh-help-icon gh-nav-item"><div class="popover" id="gh-help-icon-popover"><div id="popover-trigger-gh-help-icon-popover"><input type="hidden" /><button class="button-link gh-nav-help-icon gh-icon-btn button-link-primary button-link-icon-only" type="button" aria-expanded="false" aria-label="ScienceDirect Support Center links"><svg focusable="false" viewBox="0 0 114 128" aria-hidden="true" alt="ScienceDirect help page" width="21.375" height="24" class="icon icon-help gh-icon"><path d="m57 8c-14.7 0-28.5 5.72-38.9 16.1-10.38 10.4-16.1 24.22-16.1 38.9 0 30.32 24.68 55 55 55 14.68 0 28.5-5.72 38.88-16.1 10.4-10.4 16.12-24.2 16.12-38.9 0-30.32-24.68-55-55-55zm0 1e1c24.82 0 45 20.18 45 45 0 12.02-4.68 23.32-13.18 31.82s-19.8 13.18-31.82 13.18c-24.82 0-45-20.18-45-45 0-12.02 4.68-23.32 13.18-31.82s19.8-13.18 31.82-13.18zm-0.14 14c-11.55 0.26-16.86 8.43-16.86 18v2h1e1v-2c0-4.22 2.22-9.66 8-9.24 5.5 0.4 6.32 5.14 5.78 8.14-1.1 6.16-11.78 9.5-11.78 20.5v6.6h1e1v-5.56c0-8.16 11.22-11.52 12-21.7 0.74-9.86-5.56-16.52-16-16.74-0.39-0.01-0.76-0.01-1.14 0zm-4.86 5e1v1e1h1e1v-1e1h-1e1z"></path></svg></button></div></div></li><li class="gh-search-toggle gh-nav-item search-button-link search-with-button-link"><a class="anchor button-link-primary gh-nav-action gh-icon-btn search-input-fallback-link anchor-default anchor-icon-only" href="/search" data-aa-button="search-in-header-opened-from-article" aria-label="Search ScienceDirect" role="button"><svg focusable="false" viewBox="0 0 100 128" aria-hidden="true" alt="Search" width="18.75" height="24" class="icon icon-search gh-icon"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></a><a class="link-button link-button-small search-button-outline link-button-primary link-button-icon-right" href="/search" data-aa-button="search-in-header-opened-from-article" aria-label="Search ScienceDirect" role="button"><span class="link-button-text">Search</span><svg focusable="false" viewBox="0 0 100 128" aria-hidden="true" alt="Search" width="18.75" height="24" class="icon icon-search"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></a></li></ul></nav></div></div><div class="gh-profile-container gh-move-to-spine u-hide-from-print"><a class="link-button link-button-secondary link-button-small u-margin-s-right link-button-icon-left" href="/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0306457321001527&amp;from=globalheader" id="gh-myaccount-btn" data-aa-region="header" data-aa-name="personalsignin"><svg focusable="false" viewBox="0 0 106 128" aria-hidden="true" width="19.875" height="24" class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><span class="link-button-text">My Account</span></a><a class="link-button link-button-primary link-button-small link-button-icon-left" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0306457321001527" id="gh-institutionalsignin-btn" data-aa-region="header" data-aa-name="institutionalsignin"><svg focusable="false" viewBox="0 0 106 128" aria-hidden="true" width="19.875" height="24" class="icon icon-institution"><path d="m84 98h1e1v1e1h-82v-1e1h1e1v-46h14v46h1e1v-46h14v46h1e1v-46h14v46zm-72-61.14l41-20.84 41 20.84v5.14h-82v-5.14zm92 15.14v-21.26l-51-25.94-51 25.94v21.26h1e1v36h-1e1v3e1h102v-3e1h-1e1v-36h1e1z"></path></svg><span class="link-button-text">Sign in</span></a></div><div id="gh-mobile-menu" class="mobile-menu u-hide-from-print"><div class="gh-hamburger u-fill-grey7 u-margin-m-left"><button class="button-link u-flex-center-ver button-link-primary button-link-icon-only" type="button" aria-label="Toggle mobile menu" aria-expanded="false"><svg class="gh-hamburger-svg-el gh-hamburger-closed" role="img" aria-hidden="true" height="18" width="40"><path d="M0 14h40v2H0zm0-7h40v2H0zm0-7h40v2H0z"></path></svg></button></div><div id="gh-overlay" class="mobile-menu-overlay u-overlay u-display-none" role="button" tabindex="-1"></div><div id="gh-drawer" aria-label="Mobile menu" class role="navigation"></div></div></div></header><div class="Article Preview" id="mathjax-container" role="main"><div class="accessbar-sticky"><div id="screen-reader-main-content"></div><div role="region" aria-label="Download options and search"><div class="accessbar"><div class="accessbar-label"></div><ul aria-label="PDF Options"><li class="accessbar-item-show-from-initial accessbar-item-show-from-xs accessbar-item-show-from-md RemoteAccess"><a class="link-button RemoteAccessButton accessbar-utility-component accessbar-utility-link link-button-primary link-button-icon-left" href="/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0306457321001527" aria-label="Access through your institution"><svg focusable="false" viewBox="0 0 253.88 253.99" aria-label="Seamless access" width="10" height="20" role="img" class="icon icon-seamless-access inst-icon"><g><path d="M37.58,97.76h178.73c5.7,0,10.68-3.56,12.46-8.9c1.42-5.34-0.36-11.04-4.99-14.24l-89.37-64.09 c-4.63-3.2-10.68-3.2-14.95,0L30.11,74.61c-3.2,2.49-5.34,6.05-5.34,10.33C24.77,92.06,30.46,97.76,37.58,97.76z M126.95,36.87 l49.49,35.25H77.46L126.95,36.87z"></path><polygon points="109.45,185.4 109.85,127.82 80.25,113.55 80.25,199.67"></polygon><polygon points="174.19,199.26 174.19,114.37 144.99,128.64 144.99,186.21"></polygon><path d="M242.07,226.42c-0.01,0-0.03,0-0.04,0h-10.75v-10.74c0-3.71-3.01-6.72-6.72-6.72l-10.1,0v-86.21 c0-8.2-6.71-14.9-14.9-14.9c-8.2,0-14.9,6.71-14.9,14.9v86.21l-115.42-0.02v-86.19c0-8.2-6.71-14.9-14.9-14.9 c-8.2,0-14.9,6.71-14.9,14.9v86.18l-10.08,0c-3.71,0-6.72,3.01-6.72,6.72v10.74H11.86c-3.71,0-6.72,3.01-6.72,6.72v11.68h243.62 v-11.63C248.77,229.45,245.78,226.43,242.07,226.42z"></path></g></svg><span class="link-button-text"><span>Access through&nbsp;<strong>your institution</strong></span></span></a></li><li class="accessbar-item-hide-from-initial accessbar-item-hide-from-xs accessbar-item-show-from-md OpenManuscript"><a class="link-button accessbar-utility-component accessbar-utility-link link-button-anchor link-button-text-only" href="/science/article/am/pii/S0306457321001527" target="_blank" aria-label="View Open Manuscript"><span class="link-button-text"><span>View Open Manuscript</span></span></a></li><li class="accessbar-item-hide-from-initial accessbar-item-hide-from-xs accessbar-item-show-from-md Divider"><span class="accessbar-divider"></span></li><li class="accessbar-item-hide-from-initial accessbar-item-hide-from-xs accessbar-item-show-from-md PurchasePDF"><a class="link-button accessbar-utility-component accessbar-utility-link link-button-anchor link-button-text-only" href="/getaccess/pii/S0306457321001527/purchase" target="_blank" aria-label="Purchase PDF" rel="noreferrer noopener"><span class="link-button-text"><span>Purchase PDF</span></span></a></li><li class="accessbar-item-show-from-initial accessbar-item-show-from-xs accessbar-item-hide-from-md OverflowPopover"><div class="popover accessbar-overflow-popover" id="OverflowPopoverAnchor"><div id="popover-trigger-OverflowPopoverAnchor"><button type="button" class="button accessbar-utility-component accessbar-utility-button button-anchor button-icon-right" aria-disabled="false" aria-label="Other access options"><span class="button-text"><span>Other access options</span></span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></div></li></ul><form class="QuickSearch" action="/search#submit" method="get" aria-label="form"><div class="search-input"><div class="search-input-container search-input-container-no-label"><label class="search-input-label u-hide-visually" for="article-quick-search">Search ScienceDirect</label><input type="search" id="article-quick-search" name="qs" value class="search-input-field" aria-label="Search ScienceDirect" aria-describedby="article-quick-search-description-message" placeholder="Search ScienceDirect" /></div><div class="search-input-message-container"><div aria-live="polite" class="search-input-validation-error"></div><div id="article-quick-search-description-message"></div></div></div><button type="submit" class="button small u-margin-xs-left button-primary button-icon-only" aria-disabled="false" aria-label="Submit search"><svg focusable="false" viewBox="0 0 100 128" height="20" width="18.75" class="icon icon-search"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></button><input type="hidden" name="origin" value="article" /><input type="hidden" name="zone" value="qSearch" /></form></div></div></div><div class="article-wrapper u-padding-s-top grid row"><div role="navigation" aria-label="Table of Contents" class="preview-sidebar u-show-from-lg col-lg-6"><div class="PreviewTableOfContents text-s"><h2 class="u-h4 preview-table-of-contents-title">Article preview</h2><ul class="preview-table-of-contents-list"><li id="preview-section-abstract-item" class><a class="anchor anchor-default" href="#preview-section-abstract"><span class="anchor-text">Abstract</span></a></li><li id="preview-section-introduction-item" class><a class="anchor anchor-default" href="#preview-section-introduction"><span class="anchor-text">Introduction</span></a></li><li id="preview-section-snippets-item" class><a class="anchor anchor-default" href="#preview-section-snippets"><span class="anchor-text">Section snippets</span></a></li><li id="preview-section-references-item" class><a class="anchor anchor-default" href="#preview-section-references"><span class="anchor-text">References (72)</span></a></li></ul></div></div><article class="col-lg-12 col-md-16 pad-left pad-right" lang="en"><div class="Publication" id="publication"><div class="publication-brand u-show-from-sm"><a class="anchor anchor-default" href="/journal/information-processing-and-management" title="Go to Information Processing &amp; Management on ScienceDirect"><span class="anchor-text"><img class="publication-brand-image" src="https://sciencedirect.elseviercdn.cn/prod/f624014e8238d0b70668b2e1dcc65fef7457d17f/image/elsevier-non-solus.png" alt="Elsevier" /></span></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id="publication-title"><a class="anchor publication-title-link anchor-navigation" href="/journal/information-processing-and-management" title="Go to Information Processing &amp; Management on ScienceDirect"><span class="anchor-text">Information Processing &amp; Management</span></a></h2><div class="text-xs"><a class="anchor anchor-default" href="/journal/information-processing-and-management/vol/58/issue/5" title="Go to table of contents for this volume/issue"><span class="anchor-text">Volume 58, Issue 5</span></a>, <!-- -->September 2021<!-- -->, 102664</div></div><div class="publication-cover u-show-from-sm"><a class="anchor anchor-default" href="/journal/information-processing-and-management/vol/58/issue/5"><span class="anchor-text"><img class="publication-cover-image" src="https://ars.els-cdn.com/content/image/1-s2.0-S0306457321X00039-cov150h.gif" alt="Information Processing &amp; Management" /></span></a></div></div><div class="PageDivider"></div><h1 id="screen-reader-main-title" class="Head u-font-serif u-h2 u-margin-s-ver"><span class="title-text">AOMD: An analogy-aware approach to offensive meme detection on social media</span></h1><div class="Banner" id="banner"><div class="wrapper truncated"><div class="AuthorGroups text-s"><div class="author-group" id="author-group"><span class="sr-only">Author links open overlay panel</span><button class="button-link button-link-primary" type="button" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000001"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Lanyu</span> <span class="text surname">Shang</span> </span><span class="author-ref" id="baff1"><sup>a</sup></span><svg focusable="false" viewBox="0 0 102 128" title="Author email or social media contact details icon" width="20" height="20" class="icon icon-envelope react-xocs-author-icon"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></button>, <button class="button-link button-link-primary" type="button" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000002"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Yang</span> <span class="text surname">Zhang</span> </span><span class="author-ref" id="baff2"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" title="Author email or social media contact details icon" width="20" height="20" class="icon icon-envelope react-xocs-author-icon"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></button>, <button class="button-link button-link-primary" type="button" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000003"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Yuheng</span> <span class="text surname">Zha</span> </span><span class="author-ref" id="baff2"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" title="Author email or social media contact details icon" width="20" height="20" class="icon icon-envelope react-xocs-author-icon"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></button>, <button class="button-link button-link-primary" type="button" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000004"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Yingxi</span> <span class="text surname">Chen</span> </span><span class="author-ref" id="baff2"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" title="Author email or social media contact details icon" width="20" height="20" class="icon icon-envelope react-xocs-author-icon"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></button>, <button class="button-link button-link-primary" type="button" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000005"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Christina</span> <span class="text surname">Youn</span> </span><span class="author-ref" id="baff2"><sup>b</sup></span><svg focusable="false" viewBox="0 0 102 128" title="Author email or social media contact details icon" width="20" height="20" class="icon icon-envelope react-xocs-author-icon"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></button>, <button class="button-link button-link-primary" type="button" data-sd-ui-side-panel-opener="true" data-xocs-content-type="author" data-xocs-content-id="au000006"><span class="button-link-text"><span class="react-xocs-alternative-link"><span class="given-name">Dong</span> <span class="text surname">Wang</span> </span><span class="author-ref" id="baff1"><sup>a</sup></span><svg focusable="false" viewBox="0 0 106 128" title="Correspondence author icon" width="20" height="20" class="icon icon-person react-xocs-author-icon"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><svg focusable="false" viewBox="0 0 102 128" title="Author email or social media contact details icon" width="20" height="20" class="icon icon-envelope react-xocs-author-icon"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></button></div></div></div><button class="button-link button-link-secondary u-margin-s-ver text-s show-more-button button-link-icon-right" type="button" id="show-more-btn" data-aa-button="icon-expand"><span class="button-link-text">Show more</span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom text-s"><button class="button-link AddToMendeley u-margin-s-right u-show-inline-from-md button-link-primary button-link-icon-left" type="button"><svg focusable="false" viewBox="0 0 86 128" height="16" width="16" class="icon icon-plus"><path d="m48 58v-38h-1e1v38h-38v1e1h38v38h1e1v-38h38v-1e1z"></path></svg><span class="button-link-text">Add to Mendeley</span></button><div class="Social u-display-inline-block" id="social"><div class="popover social-popover" id="social-popover"><div id="popover-trigger-social-popover"><button class="button-link u-margin-s-right button-link-primary button-link-icon-left" type="button" aria-expanded="false" aria-haspopup="true"><svg focusable="false" viewBox="0 0 128 128" height="16" width="16" class="icon icon-share"><path d="m9e1 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm-66-36c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-6e1c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48l-28.42-15.28c0.58-1.98 0.9-4.04 0.9-6.2s-0.32-4.22-0.9-6.2l28.42-15.28c4.04 4.58 9.92 7.48 16.48 7.48 12.14 0 22-9.86 22-22s-9.86-22-22-22-22 9.86-22 22c0 1.98 0.28 3.9 0.78 5.72l-28.64 15.38c-4.02-4.34-9.76-7.1-16.14-7.1-12.14 0-22 9.86-22 22s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-0.5 1.84-0.78 3.76-0.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class="button-link-text">Share</span></button></div></div></div><div class="ExportCitation u-display-inline-block" id="export-citation"><div class="popover export-citation-popover" id="export-citation-popover"><div id="popover-trigger-export-citation-popover"><button class="button-link button-link-primary button-link-icon-left" type="button" aria-expanded="false" aria-haspopup="true"><svg focusable="false" viewBox="0 0 106 128" height="16" width="16" class="icon icon-cited-by-66"><path xmlns="http://www.w3.org/2000/svg" d="m2 58.78v47.22h44v-42h-34v-5.22c0-18.5 17.08-26.78 34-26.78v-1e1c-25.9 0-44 15.12-44 36.78zm1e2 -26.78v-1e1c-25.9 0-44 15.12-44 36.78v47.22h44v-42h-34v-5.22c0-18.5 17.08-26.78 34-26.78z"></path></svg><span class="button-link-text">Cite</span></button></div></div></div></div></div><div class="ArticleIdentifierLinks u-margin-xs-bottom text-xs" id="article-identifier-links"><a class="anchor doi anchor-default" href="https://doi.org/10.1016/j.ipm.2021.102664" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span class="anchor-text">https://doi.org/10.1016/j.ipm.2021.102664</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em" class="icon icon-arrow-up-right arrow-external-link"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a><a class="anchor rights-and-content anchor-default" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0306457321001527&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span class="anchor-text">Get rights and content</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em" class="icon icon-arrow-up-right arrow-external-link"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a></div><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div id="preview-section-abstract"><div class="PageDivider"></div><div class="Abstracts u-font-serif text-s" id="abstracts"><div class="abstract author-highlights" id="d1e527"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Highlights</h2><div id="d1e530"><p id="d1e531"><ul class="list"><li class="react-xocs-list-item"><span class="list-label">•</span><span><p id="d1e537">We define a novel problem of detecting offensive analogy memes on online social media.</p></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><p id="d1e542">We design a principled scheme to jointly exploit multi-modal content in meme posts.</p></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><p id="d1e547">We explicitly model the relations between visual and textual content in memes.</p></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><p id="d1e552">We collect two real-world datasets on Reddit and Gab to extensively evaluate the detection performance.</p></span></li><li class="react-xocs-list-item"><span class="list-label">•</span><span><p id="d1e557">Evaluation results show significant performance gains of the proposed solution.</p></span></li></ul></p></div></div><div class="abstract author" id="d1e515"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id="d1e518"><p id="d1e519">This paper focuses on an important problem of detecting <em>offensive analogy meme</em> on online social media where the visual content and the texts/captions of the meme together make an <em>analogy</em><span> to convey the offensive information. Existing offensive meme detection solutions often ignore the implicit relation between the visual and textual contents of the meme and are insufficient to identify the offensive analogy memes. Two important challenges exist in accurately detecting the offensive analogy memes: i) it is not trivial to capture the analogy that is often implicitly conveyed by a meme; ii) it is also challenging to effectively align the complex analogy across different data modalities in a meme. To address the above challenges, we develop a <a href="/topics/computer-science/deep-learning" title="Learn more about deep learning from ScienceDirect&#x27;s AI-generated Topic Pages" class="topic-link">deep learning</a> based Analogy-aware Offensive Meme Detection (AOMD) framework to learn the implicit analogy from the multi-modal contents of the meme and effectively detect offensive analogy memes. We evaluate AOMD on two real-world datasets from online social media. Evaluation results show that AOMD achieves significant performance gains compared to state-of-the-art baselines by detecting offensive analogy memes more accurately.</span></p></div></div></div></div><div id="preview-section-introduction"><div class="PageDivider"></div><div class="Introduction u-font-serif text-s u-margin-l-ver"><h2 class="u-h4 u-margin-s-bottom">Introduction</h2><section id="sec1"><p id="d1e578">As the popularity of social networks continues to increase, social media platforms become an attractive breeding ground for amplifying offensive activities (e.g., hate speech, cyberbullying). People are increasingly exposed to online offensive content in recent years. For example, approximately 44% of Americans were subjected to online hate and harassment in 2020, and 28% of online social media users have experienced severe purposeful online harassment (e.g., sexual harassment, stalking, physical threats).<sup>1</sup> Social media platforms and researchers have been endeavoring to combat online offensive content. Many solutions have been developed to address cyber offensive behaviors. Examples include hate speech detection (Van Hee et al., 2018), cyber racism recognition (Jakubowicz et al., 2017), and online harassment identification (Jhaver, Ghoshal, Bruckman, &amp; Gilbert, 2018). In this paper, we study an important problem of detecting <em>offensive analogy meme</em> on online social media where the visual content and the texts/captions of the meme together make an <em>analogy</em> to convey the offensive information. In particular, the offensive information is referred to as the explicit or implicit message deliberately expressed against individuals or a group of people (e.g., disparagement, animosity) (Suryawanshi, Chakravarthi, Arcan, &amp; Buitelaar, 2020).</p><p id="d1e618">Our problem is motivated by the prevalence of image-based content on online social media. Images often contain rich information and have been a key and attractive medium for people to create and share on social media. For example, an average of 95 million photos are uploaded to Instagram daily,<sup>2</sup> and more than 40% of tweets contain visual content.<sup>3</sup> In addition, social media posts with images are more likely to attract user’s attention than those without (e.g., tweets with images can achieve 150% more retweets than the tweets without images<sup>4</sup>). However, the widespread presence of images on social media also provides opportunities for the dissemination of offensive contents. In particular, sophisticated content creators increasingly favor the image as the carrier to propagate offensive information that is implicitly expressed with the accompanying text embedded in the image. Such kind of images can circumvent existing content censorship that focuses on the explicit indecent contents (e.g., sexual images, hateful vocabulary). Therefore, it is critical to effectively identify these image-driven offensive content to curb the spread of offensive information and reduce the propagation of extreme ideology on online platforms.</p><p id="d1e653">In this paper, we focus on an emerging phenomenon on social media where the visual content of an image together with the auxiliary text superimposed on or associated with the image jointly make an <em>analogy</em> to convey offensive information to the audience of the post. We refer to such kind of content as <em>offensive analogy meme</em>. Fig. 1 shows four examples of the offensive analogy memes from online social media. Fig. 1(a) used an analogy of a white shark to express the support of the Caucasian people. Fig. 1(b) delivered an implicit analogy that Jewish people are taking advantage of Black people to attack the White community. Similarly, Fig. 1(c) showed a favor of White privilege using an analogy of animals, and Fig. 1(d) leveraged the analogy of the quiet vs. energetic behavior on the train to reveal the hateful attitude against Black people. The identification of such offensive analogy memes on online social media can have multiple positive impacts in the real world. First, it can significantly reduce the spread of offensive content and ideology of hatred in online communities. For example, online users can be advised about the potential harmfulness of these offensive analogy memes before posting or sharing them on online platforms (e.g., online social media). Second, the effective identification of the analogy expressed in the multi-modal memes can also be helpful for better understanding and analyzing multi-modal content which can be further applied to other real-world applications (e.g., image captioning Hossain, Sohel, Shiratuddin, &amp; Laga, 2019, image retrieval Zhou, Li, &amp; Tian, 2017). For example, the identified analogical relations between the visual and textual content in images can greatly enhance the image captioning tools for generating more meaningful captions.</p><p id="d1e691">Recently, several multi-modal solutions (Chauhan et al., 2020, Sabat et al., 2019, Sharma et al., 2020, Suryawanshi et al., 2020, Velioglu and Rose, 2020) have been proposed to address the offensive meme detection problem on social media. A representative set of existing solutions are leveraging pre-trained neural network models (e.g., convolutional neural network based model (Simonyan &amp; Zisserman, 2014) for visual input, BERT (Devlin, Chang, Lee, &amp; Toutanova, 2018) or transformer-based encoder (Vaswani et al., 2017) for text input) to extract the latent features from the visual content and embedded captions in the multi-modal memes. The latent features are concatenated as the input to a classifier (e.g., Multi-layer Perceptron (Noriega, 2005)) for classifying offensive meme posts. However, the above solutions only focus on a direct combination of the multi-modal features from the visual content and embedded captions but ignore the implicit relation between them as well as the analogy they deliver together. One important observation of the above examples (Fig. 1) is that these offensive analogy memes do not necessarily contain any explicit offensive or hateful content (e.g., hate speech or image) that can be leveraged to quickly detect them. Therefore, the detection of offensive analogy meme is a non-trivial problem and cannot be fully addressed by existing solutions. We elaborate the key challenges of solving this problem below.</p><p id="d1e727"><em>Analogy Awareness.</em> The first challenge of detecting offensive analogy meme lies in correctly capturing and understanding the analogy expressed by the meme. For example, the analogy between the “black bowling ball hits the white bowling pins” and the “Black people ruin the White community” in Fig. 1(b) is critical to detect that offensive meme. However, the extraction of such analogy often requires a holistic analysis of the visual content, embedded caption, and user comments of the meme if available (Sharma et al., 2020). Moreover, the analogy of the offensive meme can also hide in the contextual information (e.g., the text description associated with the meme). For example, Fig. 1(c) will go undetected if we ignore the analogy between the “WHITE” caption in the image and the “white man” in the text description. Such a meme can be completely appropriate when it appears in the wildlife protection forum. The existing solutions that focus on the image or text content itself are often insufficient to capture the analogy in such offensive memes (Sharma et al., 2020). Therefore, such an analogy has to be carefully captured and considered in the process of offensive meme detection on social media.</p><p id="d1e750"><em>Complex Multi-modal Analogy Alignment.</em> The second challenge of detecting offensive analogy meme lies in the accurate alignment of the complex analogy across different data modalities in a meme post. For example, existing solutions for embedded caption extraction highly rely on the optical character recognition (OCR) technique (Islam, Islam, &amp; Noor, 2017). However, the OCR technique only focuses on recognizing all the characters in an image and can often recognize irrelevant content (e.g., copyright watermark). Such irrelevant OCR texts may lead to the identification of incorrect analogy in the meme. Moreover, it is also important to accurately capture the analogical relation between the visual and textual content in the meme. For example, as the image shown in Fig. 1(b), the implicit offensive content against Jewish people and Black people cannot be captured if the visual content and textual captions are incorrectly matched. The positions of the visual content and embedded captions have to be carefully considered to capture the analogy (i.e., bowler – “Jews”, black bowling ball – “Black people”, white bowling pins – “A quiet, peaceful, functioning society” in the above example). However, current multi-modal meme solutions that simply integrate visual and textual features of a meme are insufficient to capture the analogy embedded across different data modalities in the meme (Kiela et al., 2020).</p><p id="d1e769">To address the above challenges, we develop a deep learning based Analogy-aware Offensive Meme Detection (AOMD) framework that can effectively identify offensive analogy memes on online social media. In particular, to address the analogy awareness challenge, we develop an analogy-aware multi-modal representation learning module to incorporate the <em>content</em> (i.e., image, embedded caption) and <em>contextual information</em> (i.e., text description, user comments) to identify the analogy expressed in the meme. To address the complex multi-modal analogy alignment challenge, we develop an attentive multi-modal analogy alignment module to explicitly model the relation between the visual content and textual caption in the memes. To the best of our knowledge, AOMD is the first analogy-aware deep learning based solution to detect offensive analogy memes on social media. We evaluate the AOMD framework on two real-world datasets collected from Gab and Reddit. Evaluation results show that AOMD achieves significant performance gains compared to state-of-the-art baseline methods by detecting offensive analogy memes more accurately.</p></section></div></div><div id="preview-section-snippets"><div class="PageDivider"></div><div class="Snippets u-font-serif text-s"><h2 class="u-h4 u-margin-l-ver">Section snippets</h2><section><section id="sec2"><section id="sec2.1"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Social media misbehavior</h2><p id="d1e787">Misbehavior has become a severe issue on social media platforms (Wang et al., 2015, Wang et al., 2012, Wang et al., 2019). Examples of social media misbehavior include cyberbullying (Englander et al., 2017, Van Hee et al., 2018), trolling (Paavola, Helo, Jalonen, Sartonen, &amp; Huhtinen, 2016), hateful content (Magu et al., 2017, Ribeiro et al., 2017), rumors (Choi, Chun, Oh, Han, et al., 2020), and fake news (Shu et al., 2017, Zhang et al., 2018). For example, Yao et al. proposed an online</p></section></section></section><section><section id="sec3"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Problem definition</h2><p id="d1e1016">In this section, we formally present the offensive analogy meme detection problem on social media. We first define a few key terms that will be used in the problem definition.</p><p id="d1e1018"><section><p id="dfn1"><strong>Definition 1</strong><span> <h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Meme Post <span class="math"><math><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math></span></h2></span></p><p id="d1e1034">A meme post (<span class="math"><math><msub is="true"><mrow is="true"><mi is="true">P</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math></span>) on social media contains three major components: (i) <em>meme (</em><span class="math"><math><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math></span><em>)</em>, (ii) <em>text description (</em><span class="math"><math><msub is="true"><mrow is="true"><mi is="true">D</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math></span><em>)</em>, and (iii) <em>user comments (</em><span class="math"><math><msub is="true"><mrow is="true"><mi is="true">U</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math></span><em>)</em>. An example of the meme post on social media is shown in Fig. 2.</p></section></p><p id="d1e1094"><section><p id="dfn2"><strong>Definition 2</strong><span> <h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Meme <span class="math"><math><msub is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></math></span></h2></span></p><p id="d1e1110">The image attachment of a meme post. The meme attachment contains two parts: the <em>visual content (</em><span class="math"><math><msubsup is="true"><mrow is="true"><mi is="true">M</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mi is="true">V</mi></mrow></msubsup></math></span><em>)</em> and <em>embedded caption </em></p></section></p></section></section><section><section id="sec4"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Solution</h2><p id="d1e1572">In this section, we present the Analogy-aware Offensive Meme Detection (AOMD) framework to address the offensive analogy meme detection problem defined in the previous section. An overview of the AOMD framework is shown in Fig. 3. The AOMD framework contains three major components. First, the <em>analogy-aware multi-modal representation learning</em> module is designed to extract the visual, textual, and contextual features from the content of meme posts and encode these multi-modal features into the</p></section></section><section><section id="sec5"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Data</h2><p id="d1e3776">In this section, we present the real-world datasets and labels we collected for evaluation. We observe that mainstream social media platforms (e.g., Twitter) contain a massive amount of memes. However, the collection of offensive memes on those platforms has experienced a long tail issue (i.e., only a small portion of the collected memes are actually offensive) (Zhang &amp; Luo, 2019). In addition, we observe that existing datasets for offensive memes (e.g., MultiOFF (Suryawanshi et al., 2020),</p></section></section><section><section id="sec6"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Evaluation</h2><p id="d1e3839">In this section, we evaluate the detection performance of the AOMD framework on two real-world datasets described in Section 5. In particular, we first investigate the performance of AOMD and state-of-the-art uni-modal and multi-modal baselines in identifying offensive memes on social media. We further study AOMD’s effectiveness in detecting offensive meme posts that contain analogy. The results show that the AOMD framework achieves significant performance gains in terms of the offensive</p></section></section><section><section id="sec7"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">Conclusion</h2><p id="d1e4085">In this paper, we develop AOMD, the first analogy-aware deep learning based solution to address offensive analogy memes on social media. The AOMD framework is designed to effectively capture the analogy conveyed by different data modalities of the meme, and detect the offensiveness implicitly expressed in the meme. We evaluate the AOMD framework with two real-world datasets collected from Gab and Reddit. The evaluation results demonstrate that our scheme outperforms the state-of-the-art</p></section></section><section><section id="d1e4087"><h2 class="section-title u-h4 u-margin-l-top u-margin-xs-bottom">CRediT authorship contribution statement</h2><p id="d1e4090"><strong>Lanyu Shang:</strong> Conceptualization, Methodology, Formal analysis, Writing - original draft. <strong>Yang Zhang:</strong> Conceptualization, Writing - original draft. <strong>Yuheng Zha:</strong> Data curation, Formal analysis. <strong>Yingxi Chen:</strong> Data curation. <strong>Christina Youn:</strong> Data curation. <strong>Dong Wang:</strong> Supervision, Writing -review &amp; editing.</p></section></section><section><section id="d1e4109"><h2 id="d1e4110" class="u-h4 u-margin-l-top u-margin-xs-bottom">Acknowledgments</h2><p id="d1e4112">This research is supported in part by the <span id="GS1">National Science Foundation, USA</span> under Grant No. <span>IIS-2008228</span>, <span>CNS-1845639</span>, <span>CNS-1831669</span>, <span id="GS2">Army Research Office, USA</span>
under Grant <span>W911NF-17-1-0409</span>. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for</p></section></section></div></div><div class="related-content-links u-hide-from-md"><button type="button" class="button button-anchor" aria-disabled="false"><span class="button-text">Recommended articles</span></button></div><div class="Tail text-s"></div><div id="preview-section-references"><div class="paginatedReferences u-font-serif text-s"><div class="PageDivider"></div><header><h2 class="u-h4 u-margin-l-ver"><span>References</span><span> (72)</span></h2></header><ul><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Graves<span>Alex </span></span><em> et al.</em></span><h3><a class="anchor title anchor-default" href="/science/article/pii/S0893608005001206" target="_self"><span class="anchor-text">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Neural Networks</h3></div><div class="series">(2005)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Wu<span>Qi </span></span><em> et al.</em></span><h3><a class="anchor title anchor-default" href="/science/article/pii/S1077314217300772" target="_self"><span class="anchor-text">Visual question answering: A survey of methods and datasets</span></a></h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Computer Vision and Image Understanding</h3></div><div class="series">(2017)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Baltrušaitis<span>Tadas </span></span><em> et al.</em></span><h3 class="title">Multimodal machine learning: A survey and taxonomy</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">IEEE Transactions on Pattern Analysis and Machine Intelligence</h3></div><div class="series">(2018)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="author u-font-sans">Börzsei<span>Linda K. </span></span><h3 class="title">Makes a meme instead</h3><span class="host u-clr-grey6 u-font-sans"></span></li><li class="bib-reference u-margin-s-bottom"><span class="author u-font-sans">Castaño Díaz<span>Carlos Mauricio </span></span><h3 class="title">Defining and characterizing the concept of internet meme</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Ces Psicología</h3></div><div class="series">(2013)</div></span></li><li class="bib-reference u-margin-s-bottom"><span>Chauhan, Dushyant Singh, Dhanush, SR, Ekbal, Asif, &amp; Bhattacharyya, Pushpak (2020). All-in-One: A deep attentive...</span></li><li class="bib-reference u-margin-s-bottom"><span>Chen, Dapeng, Li, Hongsheng, Liu, Xihui, Shen, Yantao, Shao, Jing, &amp; Yuan, Zejian, et al. (2018). Improving deep visual...</span></li><li class="bib-reference u-margin-s-bottom"><span>Cheng, Justin, Bernstein, Michael, Danescu-Niculescu-Mizil, Cristian, &amp; Leskovec, Jure (2017). Anyone can become a...</span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Choi<span>Daejin </span></span><em> et al.</em></span><h3 class="title">Rumor propagation is amplified by echo chambers in social media</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Scientific Reports</h3></div><div class="series">(2020)</div></span></li><li class="bib-reference u-margin-s-bottom"><span>Davidson, Thomas, Warmsley, Dana, Macy, Michael, &amp; Weber, Ingmar (2017). Automated hate speech detection and the...</span></li></ul><div class="u-display-none"><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Devlin<span>Jacob </span></span><em> et al.</em></span><h3 class="title">Bert: Pre-training of deep bidirectional transformers for language understanding</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2018)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Englander<span>Elizabeth </span></span><em> et al.</em></span><h3 class="title">Defining cyberbullying</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Pediatrics</h3></div><div class="series">(2017)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Fortuna<span>Paula </span></span><em> et al.</em></span><h3 class="title">A survey on automatic detection of hate speech in text</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">ACM Computing Surveys</h3></div><div class="series">(2018)</div></span></li><li class="bib-reference u-margin-s-bottom"><span>He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, &amp; Sun, Jian (2016). Deep residual learning for image recognition. In...</span></li><li class="bib-reference u-margin-s-bottom"><span class="author u-font-sans">Hofstadter<span>Douglas R. </span></span><h3 class="title">Analogy as the core of cognition</h3><span class="host u-clr-grey6 u-font-sans"></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Hossain<span>MD Zakir </span></span><em> et al.</em></span><h3 class="title">A comprehensive survey of deep learning for image captioning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">ACM Computing Surveys</h3></div><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Islam<span>Noman </span></span><em> et al.</em></span><h3 class="title">A survey on optical character recognition system</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2017)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="author u-font-sans">Jakubowicz<span>Andrew </span></span><h3 class="title">Alt_right white lite: Trolling, hate speech and cyber racism on social media</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Cosmopolitan Civil Societies: An Interdisciplinary Journal</h3></div><div class="series">(2017)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Jhaver<span>Shagun </span></span><em> et al.</em></span><h3 class="title">Online harassment and content moderation: The case of blocklists</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">ACM Transactions on Computer-Human Interaction</h3></div><div class="series">(2018)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Kiela<span>Douwe </span></span><em> et al.</em></span><h3 class="title">The hateful memes challenge: Detecting hate speech in multimodal memes</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2020)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Kou<span>Ziyi </span></span><em> et al.</em></span><h3 class="title">ExFaux: A weakly supervised approach to explainable fauxtography detection</h3><span class="host u-clr-grey6 u-font-sans"></span></li><li class="bib-reference u-margin-s-bottom"><span>Kumar, Sumeet, &amp; Carley, Kathleen M. (2019). Tree LSTMs with convolution units to predict stance and rumor vera in...</span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Li<span>Liunian Harold </span></span><em> et al.</em></span><h3 class="title">Visualbert: A simple and performant baseline for vision and language</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span>Li, Kunpeng, Zhang, Yulun, Li, Kai, Li, Yuanyuan, &amp; Fu, Yun (2019). Visual semantic reasoning for image-text matching....</span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Lin<span>Tsung -Yi </span></span><em> et al.</em></span><h3 class="title">Microsoft coco: Common objects in context</h3><span class="host u-clr-grey6 u-font-sans"></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Lipton<span>Zachary C. </span></span><em> et al.</em></span><h3 class="title">A critical review of recurrent neural networks for sequence learning</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2015)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Loshchilov<span>Ilya </span></span><em> et al.</em></span><h3 class="title">Decoupled weight decay regularization</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2017)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Lu<span>Jiasen </span></span><em> et al.</em></span><h3 class="title">Hierarchical question-image co-attention for visual question answering</h3><span class="host u-clr-grey6 u-font-sans"></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">MacAvaney<span>Sean </span></span><em> et al.</em></span><h3 class="title">Hate speech detection: Challenges and solutions</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">PLoS One</h3></div><div class="series">(2019)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Magu<span>Rijul </span></span><em> et al.</em></span><h3 class="title">Detecting the hate code on social media</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2017)</div></span></li><li class="bib-reference u-margin-s-bottom"><span>Mathew, Binny, Dutt, Ritam, Goyal, Pawan, &amp; Mukherjee, Animesh (2019). Spread of hate speech in online social media. In...</span></li><li class="bib-reference u-margin-s-bottom"><span>Ngiam, Jiquan, Khosla, Aditya, Kim, Mingyu, Nam, Juhan, Lee, Honglak, &amp; Ng, Andrew Y (2011). Multimodal deep learning....</span></li><li class="bib-reference u-margin-s-bottom"><span class="author u-font-sans">Noriega<span>Leonardo </span></span><h3 class="title">Multilayer perceptron tutorial</h3><span class="host u-clr-grey6 u-font-sans"><div class="series">(2005)</div></span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Paavola<span>Jarkko </span></span><em> et al.</em></span><h3 class="title">Understanding the trolling phenomenon: The automated detection of bots and cyborgs in the social media</h3><span class="host u-clr-grey6 u-font-sans"><div class="series"><h3 class="title">Journal of Information Warfare</h3></div><div class="series">(2016)</div></span></li><li class="bib-reference u-margin-s-bottom"><span>Pennington, Jeffrey, Socher, Richard, &amp; Manning, Christopher D. (2014). Glove: Global vectors for word representation....</span></li><li class="bib-reference u-margin-s-bottom"><span class="u-font-sans"><span class="author u-font-sans">Pronobis<span>Andrzej </span></span><em> et al.</em></span><h3 class="title">Large-scale semantic mapping and reasoning with heterogeneous modalities</h3><span class="host u-clr-grey6 u-font-sans"></span></li></div><button class="button-alternative button-alternative-secondary text-m u-font-sans u-margin-l-bottom button-alternative-icon-left" type="button" id="show-more-refs-btn"><span class="button-alternative-icon"><svg focusable="false" viewBox="0 0 92 128" width="17.25" height="24" class="icon icon-navigate-down u-fill-grey8"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></span><span class="button-alternative-text">View more references</span></button></div></div><div id="preview-section-cited-by"><section aria-label="Cited by" class="ListArticles preview"><div class="PageDivider"></div><header id="citing-articles-header"><h2 class="u-h4 u-margin-l-ver u-font-serif">Cited by (0)</h2></header><div aria-describedby="citing-articles-header"></div></section></div><div class="PageDivider"></div><a class="anchor full-text-link text-s anchor-default" href="/science/article/pii/S0306457321001527" aria-disabled="true" tabindex="-1"><span class="anchor-text">View full text</span></a><div class="Copyright"><span class="copyright-line">© 2021 Elsevier Ltd. All rights reserved.</span></div></article><div class="u-show-from-md col-lg-6 col-md-8 pad-right u-padding-s-top"><aside class="RelatedContent u-clr-grey8" aria-label="Related content"><section class="RelatedContentPanel u-margin-s-bottom"><header id="recommended-articles-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link related-content-panel-toggle is-up button-link-primary button-link-icon-right" type="button" aria-expanded="true" data-aa-button="sd:product:journal:article:location=recommended-articles:type=close"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Recommended articles</span></h2></span><svg focusable="false" viewBox="0 0 92 128" width="24" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class aria-hidden="false" aria-describedby="recommended-articles-header"><div id="recommended-articles" class="text-xs"><ul><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article0-title"><a class="anchor u-clamp-2-lines anchor-default" href="/science/article/pii/S0140673618331805" title="Entrance examination misogyny in Japanese medical schools"><span class="anchor-text"><span>Entrance examination misogyny in Japanese medical schools</span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">The Lancet, Volume 393, Issue 10179, 2019, p. 1416</div></div><div class="authors"><span>Kumi</span> <span>Oshima</span>, …, <span>Tetsuya</span> <span>Tanimoto</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article1-title"><a class="anchor u-clamp-2-lines anchor-default" href="/science/article/pii/S0020025522003917" title="BMP: A blockchain assisted meme prediction method through exploring contextual factors from social networks"><span class="anchor-text"><span>BMP: A blockchain assisted meme prediction method through exploring contextual factors from social networks</span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Information Sciences, Volume 603, 2022, pp. 262-288</div></div><div class="authors"><span>Fan</span> <span>Yang</span>, …, <span>Xiao</span> <span>Wang</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-block"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article2-title"><a class="anchor u-clamp-2-lines anchor-default" href="/science/article/pii/S0378216620302587" title="“Cats be outside, how about meow”: Multimodal humor and creativity in an internet meme"><span class="anchor-text"><span>“Cats be outside, how about meow”: Multimodal humor and creativity in an internet meme</span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Journal of Pragmatics, Volume 171, 2021, pp. 101-117</div></div><div class="authors"><span>Camilla</span> <span>Vásquez</span>, <span>Erhan</span> <span>Aslan</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article3-title"><a class="anchor u-clamp-2-lines anchor-default" href="/science/article/pii/S095741742100871X" title="Deep Learning for predicting neutralities in Offensive Language Identification Dataset"><span class="anchor-text"><span>Deep Learning for predicting neutralities in Offensive Language Identification Dataset<figure class="inline-figure"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S095741742100871X-fx999.jpg" height="22" alt /></figure></span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Expert Systems with Applications, Volume 185, 2021, Article 115458</div></div><div class="authors"><span>Mayukh</span> <span>Sharma</span>, …, <span>Vasantha</span> <span>Kandasamy</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article4-title"><a class="anchor u-clamp-2-lines anchor-default" href="/science/article/pii/S0925231221017306" title="Does aggression lead to hate? Detecting and reasoning offensive traits in hinglish code-mixed texts"><span class="anchor-text"><span><em>Does aggression lead to hate?</em> Detecting and reasoning offensive traits in hinglish code-mixed texts</span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Neurocomputing, Volume 488, 2022, pp. 598-617</div></div><div class="authors"><span>Ayan</span> <span>Sengupta</span>, …, <span>Tanmoy</span> <span>Chakraborty</span></div></div><div class="buttons"></div></li><li class="RelatedContentPanelItem u-display-none"><div class="sub-heading u-padding-xs-bottom"><h3 class="related-content-panel-list-entry-outline-padding text-s u-font-serif" id="recommended-articles-article5-title"><a class="anchor u-clamp-2-lines anchor-default" href="/science/article/pii/S0306457320308554" title="Misogyny Detection in Twitter: a Multilingual and Cross-Domain Study"><span class="anchor-text"><span>Misogyny Detection in Twitter: a Multilingual and Cross-Domain Study</span></span></a></h3><div class="article-source u-clr-grey6"><div class="source">Information Processing &amp; Management, Volume 57, Issue 6, 2020, Article 102360</div></div><div class="authors"><span>Endang Wahyu</span> <span>Pamungkas</span>, …, <span>Viviana</span> <span>Patti</span></div></div><div class="buttons"></div></li></ul></div><button class="button-link more-recommendations-button button-link-secondary text-s u-margin-s-bottom button-link-icon-right" type="button"><span class="button-link-text">Show 3 more articles</span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></section><section class="RelatedContentPanel u-margin-s-bottom hidden"><header id="metrics-header" class="related-content-panel-header u-margin-s-bottom"><button class="button-link related-content-panel-toggle is-up button-link-primary button-link-icon-right" type="button" aria-expanded="true"><span class="button-link-text"><h2 class="section-title u-h4"><span class="related-content-panel-title-text">Article Metrics</span></h2></span><svg focusable="false" viewBox="0 0 92 128" width="24" height="24" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div class aria-hidden="false" aria-describedby="metrics-header"><a href="https://plu.mx/plum/a/?doi=10.1016/j.ipm.2021.102664" class="plumx-summary plum-sciencedirect-theme" data-pass-hidden-categories="true" data-hide-usage="true" data-orientation="vertical" data-hide-print="true" data-site="plum" data-on-success="onMetricsWidgetSuccess">View article metrics</a></div></section></aside></div></div></div></div><footer role="contentinfo" class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a class="anchor anchor-default anchor-icon-only" href="https://www.elsevier.com/" target="_blank" aria-label="Elsevier home page (opens in a new tab)" rel="nofollow"><img class="footer-logo" src="https://sciencedirect.elseviercdn.cn/shared-assets/47/images/elsevier-non-solus-new-with-wordmark.svg" alt="Elsevier logo with wordmark" height="64" width="58" loading="lazy" /></a></div><div class="els-footer-content"><div class="u-remove-if-print"><ul class="els-footer-links u-margin-xs-bottom" style="list-style:none"><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-default" href="https://www.elsevier.com/solutions/sciencedirect" target="_blank" id="els-footer-about-science-direct" rel="nofollow"><span class="anchor-text">About ScienceDirect</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em" class="icon icon-arrow-up-right arrow-external-link"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a></li><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-default" href="/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0306457321001527" id="els-footer-remote-access" rel="nofollow"><span class="anchor-text">Remote access</span></a></li><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-default" href="https://sd-cart.elsevier.com/?" target="_blank" id="els-footer-shopping-cart" rel="nofollow"><span class="anchor-text">Shopping cart</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em" class="icon icon-arrow-up-right arrow-external-link"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a></li><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-default" href="http://elsmediakits.com" target="_blank" id="els-footer-advertise" rel="nofollow"><span class="anchor-text">Advertise</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em" class="icon icon-arrow-up-right arrow-external-link"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a></li><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-default" href="https://service.elsevier.com/app/contact/supporthub/sciencedirect/" target="_blank" id="els-footer-contact-support" rel="nofollow"><span class="anchor-text">Contact and support</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em" class="icon icon-arrow-up-right arrow-external-link"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a></li><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-default" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" target="_blank" id="els-footer-terms-condition" rel="nofollow"><span class="anchor-text">Terms and conditions</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em" class="icon icon-arrow-up-right arrow-external-link"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a></li><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md anchor-default" href="https://www.elsevier.com/legal/privacy-policy" target="_blank" id="els-footer-privacy-policy" rel="nofollow"><span class="anchor-text">Privacy policy</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em" class="icon icon-arrow-up-right arrow-external-link"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a></li></ul></div><p id="els-footer-cookie-message" class="u-remove-if-print">Cookies are used by this site. <!-- --> <button class="button-link ot-sdk-show-settings cookie-btn button-link-primary" type="button" id="ot-sdk-btn"><span class="button-link-text"><strong>Cookie Settings</strong></span></button></p><p id="els-footer-copyright">All content on this site: Copyright © <!-- -->2024<!-- --> Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the Creative Commons licensing terms apply.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a class="anchor anchor-default anchor-icon-only" href="https://www.relx.com/" target="_blank" aria-label="RELX home page (opens in a new tab)" id="els-footer-relx" rel="nofollow"><img loading="lazy" src="https://sciencedirect.elseviercdn.cn/shared-assets/60/images/logo-relx-tm.svg" width="93" height="20" alt="RELX group home page" /></a></div></footer></div></div></div></div>
<script type="application/json" data-iso-key="_0">{"abstracts":{"content":[{"$$":[{"$":{"id":"d1e516"},"#name":"section-title","_":"Abstract"},{"$$":[{"$$":[{"#name":"__text__","_":"This paper focuses on an important problem of detecting "},{"#name":"italic","_":"offensive analogy meme"},{"#name":"__text__","_":" on online social media where the visual content and the texts/captions of the meme together make an "},{"#name":"italic","_":"analogy"},{"#name":"text","$$":[{"#name":"__text__","_":" to convey the offensive information. Existing offensive meme detection solutions often ignore the implicit relation between the visual and textual contents of the meme and are insufficient to identify the offensive analogy memes. Two important challenges exist in accurately detecting the offensive analogy memes: i) it is not trivial to capture the analogy that is often implicitly conveyed by a meme; ii) it is also challenging to effectively align the complex analogy across different data modalities in a meme. To address the above challenges, we develop a "},{"#name":"topic-link","_":"deep learning","$":{"href":"/topics/computer-science/deep-learning","term":"deep learning"}},{"#name":"__text__","_":" based Analogy-aware Offensive Meme Detection (AOMD) framework to learn the implicit analogy from the multi-modal contents of the meme and effectively detect offensive analogy memes. We evaluate AOMD on two real-world datasets from online social media. Evaluation results show that AOMD achieves significant performance gains compared to state-of-the-art baselines by detecting offensive analogy memes more accurately."}]}],"$":{"view":"all","id":"d1e519"},"#name":"simple-para"}],"$":{"view":"all","id":"d1e518"},"#name":"abstract-sec"}],"$":{"view":"all","id":"d1e515","class":"author"},"#name":"abstract"},{"$$":[{"$":{"id":"d1e528"},"#name":"section-title","_":"Highlights"},{"$$":[{"$$":[{"$$":[{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e537"},"#name":"para","_":"We define a novel problem of detecting offensive analogy memes on online social media."}],"$":{"id":"d1e534"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e542"},"#name":"para","_":"We design a principled scheme to jointly exploit multi-modal content in meme posts."}],"$":{"id":"d1e539"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e547"},"#name":"para","_":"We explicitly model the relations between visual and textual content in memes."}],"$":{"id":"d1e544"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e552"},"#name":"para","_":"We collect two real-world datasets on Reddit and Gab to extensively evaluate the detection performance."}],"$":{"id":"d1e549"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e557"},"#name":"para","_":"Evaluation results show significant performance gains of the proposed solution."}],"$":{"id":"d1e554"},"#name":"list-item"}],"$":{"id":"d1e533"},"#name":"list"}],"$":{"view":"all","id":"d1e531"},"#name":"simple-para"}],"$":{"view":"all","id":"d1e530"},"#name":"abstract-sec"}],"$":{"view":"all","id":"d1e527","class":"author-highlights"},"#name":"abstract"}],"floats":[],"footnotes":[],"attachments":[]},"accessbarConfig":{"fallback":false,"id":"accessbar","version":"0.0.1","analytics":{"location":"accessbar","eventName":"ctaImpression"},"ariaLabel":{"accessbar":"Download options and search","componentsList":"PDF Options"},"banner":{"id":"Banner"},"banners":[{"id":"Banner"},{"id":"BannerSsrn"}],"components":[{"analytics":[{"ids":["accessbar:clinicalkeycta:inst-known:sdcust-unknown:ent-no:ra-no:source-linkinghub:method-ip"],"eventName":"ctaClick"}],"label":"Access through&nbsp;**ClinicalKey**","id":"ClinicalKey"},{"ariaLabel":"Access through your institution","institutionLogoAltText":"Seamless access","analytics":[{"ids":["accessbar:accesscta:inst-unknown:sdcust-unknown:ent-unknown:ra-yes:source-seamlessaccess:method-shib"],"eventName":"ctaClick"}],"labelPrefix":"Access through","defaultLabelSuffix":"your institution","href":"/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0306457321001527","id":"RemoteAccess"},{"target":"_blank","analytics":[{"ids":["accessbar:article:chorus-am"],"eventName":"ctaClick"}],"label":"View Open Manuscript","href":"/science/article/am/pii/S0306457321001527","id":"OpenManuscript"},{"analytics":[{"ids":["accessbar:purchase-pdf"],"eventName":"ctaClick"}],"label":"Purchase PDF","href":"/getaccess/pii/S0306457321001527/purchase","rel":"noreferrer noopener","target":"_blank","id":"PurchasePDF"},{"analytics":[{"ids":["accessbar:another-institution"],"eventName":"ctaClick"}],"label":"Access through another institution","href":"/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0306457321001527","id":"RemoteAccessOther"},{"id":"LinkResolver","analytics":[{"eventName":"navigationClick","ids":["sd:accessbar-linkresolver"]}],"rel":"noreferrer noopener","target":"_blank","href":"https://www.elsevier.com/?ctx_ver=Z39.88-2004&ctx_enc=info:ofi/enc:UTF-8&url_ver=Z39.88-2004&rfr_id=info:sid/Elsevier:SD&svc_val_fmt=info:ofi/fmt:kev:mtx:sch_svc&rft_val_fmt=info:ofi/fmt:kev:mtx:journal&rft.aulast=SHANG&rft.auinit=L&rft.date=2021&rft.issn=03064573&rft.volume=58&rft.issue=5&rft.spage=102664&rft.title=Information%20Processing%20%26%20Management&rft.atitle=AOMD%3A%20An%20analogy-aware%20approach%20to%20offensive%20meme%20detection%20on%20social%20media&rft_id=info:doi/10.1016/j.ipm.2021.102664","label":"Find it here!"}],"search":{"inputPlaceHolder":"Search ScienceDirect","ariaLabel":{"input":"Search ScienceDirect","submit":"Submit search"},"formAction":"/search#submit","analytics":[{"ids":["accessbar:search"],"eventName":"searchStart"}],"id":"QuickSearch"}},"adobeTarget":{"header-sign-in":{"variation":"#2","enabled":true}},"article":{"accessOptions":{"linkType":"PURCHASE","linkUrl":"/getaccess/pii/S0306457321001527","price":{"currency":"USD","totalPrice":24.95},"purchaseNonDiscounted":{"linkUrl":"/getaccess/pii/S0306457321001527?corporate=true","price":{"currency":"USD","totalPrice":34.95}}},"analyticsMetadata":{"accountId":"228598","accountName":"ScienceDirect Guests","loginStatus":"anonymous","userId":"12975512","isLoggedIn":false},"article-number":"102664","cid":"271647","content-family":"serial","copyright-line":"© 2021 Elsevier Ltd. All rights reserved.","cover-date-years":["2021"],"cover-date-start":"2021-09-01","cover-date-text":"September 2021","document-subtype":"fla","document-type":"article","eid":"1-s2.0-S0306457321001527","doi":"10.1016/j.ipm.2021.102664","first-fp":"102664","hub-eid":"1-s2.0-S0306457321X00039","issuePii":"S0306457321X00039","iss-first":"5","item-weight":"FULL-TEXT","language":"en","last-author":{"#name":"last-author","$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"$$":[{"#name":"author","$":{"id":"au000006","author-id":"S0306457321001527-10c305bd736ec42c74c26e82f3c55db2","orcid":"0000-0002-9599-8023"},"$$":[{"#name":"given-name","_":"Dong"},{"#name":"surname","_":"Wang"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"cross-ref","$":{"refid":"aff1","id":"d1e466"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"cor1","id":"d1e469"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTYlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBZHdhbmcyNCU0MGlsbGlub2lzLmVkdSUyMiU3RCUyQyUyMl8lMjIlM0ElMjJkd2FuZzI0JTQwaWxsaW5vaXMuZWR1JTIyJTdE"}]}]},"normalized-first-auth-initial":"L","normalized-first-auth-surname":"SHANG","open-research":{"#name":"open-research","$":{"xmlns:xocs":true},"$$":[{"#name":"or-agreements","$$":[{"#name":"or-agreement","$$":[{"#name":"or-agreement-name","_":"CHU_DOD"},{"#name":"or-agreement-terms","_":"publishAcceptedManuscriptIndexable"}]}]},{"#name":"or-agreement-license","_":"http://www.elsevier.com/open-access/userlicense/1.0/"}]},"pages":[{"first-page":"102664"}],"pii":"S0306457321001527","self-archiving":{"#name":"self-archiving","$":{"xmlns:xocs":true},"$$":[{"#name":"sa-start-date","_":"2023-06-25T00:00:00.000Z"},{"#name":"sa-user-license","_":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}]},"srctitle":"Information Processing & Management","timestamp":"2022-12-12T15:47:08.40663Z","title":{"content":[{"#name":"title","$":{"id":"d1e385"},"_":"AOMD: An analogy-aware approach to offensive meme detection on social media"}],"floats":[],"footnotes":[],"attachments":[]},"vol-first":"58","vol-iss-suppl-text":"Volume 58, Issue 5","userSettings":{"forceAbstract":false,"creditCardPurchaseAllowed":true,"blockFullTextForAnonymousAccess":false,"disableWholeIssueDownload":false,"preventTransactionalAccess":false,"preventDocumentDelivery":true},"contentType":"JL","crossmark":true,"document-references":72,"freeHtmlGiven":false,"userProfile":{"departmentName":"ScienceDirect Guests","accessType":"GUEST","accountId":"228598","webUserId":"12975512","accountName":"ScienceDirect Guests","departmentId":"291352","userType":"NORMAL","hasMultipleOrganizations":false},"access":{"openAccess":false,"openArchive":false},"aipType":"none","articleEntitlement":{"entitled":false,"isCasaUser":false,"usageInfo":"(12975512,U|291352,D|228598,A|3,P|2,PL)(SDFE,CON|0b21ef2b2105d648b248079085e8a6a5496dgxrqa,SSO|ANON_GUEST,ACCESS_TYPE)"},"crawlerInformation":{"canCrawlPDFContent":false,"isCrawler":false},"dates":{"Available online":"25 June 2021","Received":"3 March 2021","Revised":["1 June 2021"],"Accepted":"14 June 2021","Publication date":"1 September 2021","Version of Record":"25 June 2021"},"downloadFullIssue":false,"entitlementReason":"unsubscribed","hasBody":true,"has-large-authors":false,"hasScholarlyAbstract":true,"headerConfig":{"contactUrl":"https://service.elsevier.com/app/contact/supporthub/sciencedirect/","userName":"","userEmail":"","orgName":"ScienceDirect Guests","webUserId":"12975512","libraryBanner":{},"shib_regUrl":"","tick_regUrl":"","recentInstitutions":[],"canActivatePersonalization":false,"hasInstitutionalAssociation":false,"hasMultiOrg":false,"userType":"GUEST","userAnonymity":"ANON_GUEST","allowCart":true,"environment":"prod","cdnAssetsHost":"https://sciencedirect.elseviercdn.cn"},"isCorpReq":false,"isPdfFullText":false,"issn":"03064573","issn-primary-formatted":"0306-4573","issRange":"5","isThirdParty":false,"pageCount":14,"publication-content":{"noElsevierLogo":false,"imprintPublisher":{"displayName":"Pergamon","id":"67"},"isSpecialIssue":false,"isSampleIssue":false,"transactionsBlocked":false,"publicationOpenAccess":{"oaStatus":"","oaArticleCount":153,"openArchiveStatus":false,"openArchiveArticleCount":12,"openAccessStartDate":"","oaAllowsAuthorPaid":true},"issue-cover":{"attachment":[{"attachment-eid":"1-s2.0-S0306457321X00039-cov200h.gif","file-basename":"cov200h","extension":"gif","filename":"cov200h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306457321X00039/cover/DOWNSAMPLED200/image/gif/63fbda0c805eb6eeda68b56a825e1a40/cov200h.gif"],"attachment-type":"IMAGE-COVER-H200","filesize":"5793","pixel-height":"200","pixel-width":"147"},{"attachment-eid":"1-s2.0-S0306457321X00039-cov150h.gif","file-basename":"cov150h","extension":"gif","filename":"cov150h.gif","ucs-locator":["https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0306457321X00039/cover/DOWNSAMPLED/image/gif/6de6c86b599b4ad8299e61bf44d16dd7/cov150h.gif"],"attachment-type":"IMAGE-COVER-H150","filesize":"3753","pixel-height":"150","pixel-width":"110"}]},"smallCoverUrl":"https://ars.els-cdn.com/content/image/S03064573.gif","title":"information-processing-and-management","contentTypeCode":"JL","images":{"coverImage":"https://ars.els-cdn.com/content/image/1-s2.0-S0306457321X00039-cov150h.gif","logo":"https://sciencedirect.elseviercdn.cn/prod/f624014e8238d0b70668b2e1dcc65fef7457d17f/image/elsevier-non-solus.png","logoAltText":"Elsevier"},"publicationCoverImageUrl":"https://ars.els-cdn.com/content/image/1-s2.0-S0306457321X00039-cov150h.gif"},"volRange":"58","features":["aamAttachments","keywords","references","preview"],"titleString":"AOMD: An analogy-aware approach to offensive meme detection on social media","openManuscriptUrl":"/science/article/am/pii/S0306457321001527","ssrn":{},"renderingMode":"Preview","isAbstract":true,"isContentVisible":false,"ajaxLinks":{"referredToBy":true,"authorMetadata":true},"eligibleForUniversalPdf":true,"pdfEmbed":false,"displayViewFullText":false},"askCopilot":{},"authors":{"content":[{"#name":"author-group","$":{"id":"d1e388"},"$$":[{"#name":"author","$":{"id":"au000001","author-id":"S0306457321001527-0a6a80a7ea1a47cbc21cd3b9491bb772","orcid":"0000-0002-7480-6889"},"$$":[{"#name":"given-name","_":"Lanyu"},{"#name":"surname","_":"Shang"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Formal_analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_original_draft"},"_":"Writing - original draft"},{"#name":"cross-ref","$":{"refid":"aff1","id":"d1e402"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTElMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBbHNoYW5nMyU0MGlsbGlub2lzLmVkdSUyMiU3RCUyQyUyMl8lMjIlM0ElMjJsc2hhbmczJTQwaWxsaW5vaXMuZWR1JTIyJTdE"}]},{"#name":"author","$":{"id":"au000002","author-id":"S0306457321001527-35feadd80c241072dd2af26fe5f6c8ce"},"$$":[{"#name":"given-name","_":"Yang"},{"#name":"surname","_":"Zhang"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_original_draft"},"_":"Writing - original draft"},{"#name":"cross-ref","$":{"refid":"aff2","id":"d1e416"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTIlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBeXpoYW5nNDIlNDBuZC5lZHUlMjIlN0QlMkMlMjJfJTIyJTNBJTIyeXpoYW5nNDIlNDBuZC5lZHUlMjIlN0Q="}]},{"#name":"author","$":{"id":"au000003","author-id":"S0306457321001527-a5dea0b1c37b2852bef017a0a483bbc4"},"$$":[{"#name":"given-name","_":"Yuheng"},{"#name":"surname","_":"Zha"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Data_curation"},"_":"Data curation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Formal_analysis"},"_":"Formal analysis"},{"#name":"cross-ref","$":{"refid":"aff2","id":"d1e430"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTMlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBeXpoYSU0MG5kLmVkdSUyMiU3RCUyQyUyMl8lMjIlM0ElMjJ5emhhJTQwbmQuZWR1JTIyJTdE"}]},{"#name":"author","$":{"id":"au000004","author-id":"S0306457321001527-cb86d354c77a41639af33f17744203b6"},"$$":[{"#name":"given-name","_":"Yingxi"},{"#name":"surname","_":"Chen"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Data_curation"},"_":"Data curation"},{"#name":"cross-ref","$":{"refid":"aff2","id":"d1e442"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTQlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBeWNoZW40NSU0MG5kLmVkdSUyMiU3RCUyQyUyMl8lMjIlM0ElMjJ5Y2hlbjQ1JTQwbmQuZWR1JTIyJTdE"}]},{"#name":"author","$":{"id":"au000005","author-id":"S0306457321001527-4bcdba583a25e2f635ac339af02a0357"},"$$":[{"#name":"given-name","_":"Christina"},{"#name":"surname","_":"Youn"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Data_curation"},"_":"Data curation"},{"#name":"cross-ref","$":{"refid":"aff2","id":"d1e454"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTUlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBY3lvdW4lNDBuZC5lZHUlMjIlN0QlMkMlMjJfJTIyJTNBJTIyY3lvdW4lNDBuZC5lZHUlMjIlN0Q="}]},{"#name":"author","$":{"id":"au000006","author-id":"S0306457321001527-10c305bd736ec42c74c26e82f3c55db2","orcid":"0000-0002-9599-8023"},"$$":[{"#name":"given-name","_":"Dong"},{"#name":"surname","_":"Wang"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"cross-ref","$":{"refid":"aff1","id":"d1e466"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"cor1","id":"d1e469"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"encoded-e-address","__encoded":"JTdCJTIyJTIzbmFtZSUyMiUzQSUyMmUtYWRkcmVzcyUyMiUyQyUyMiUyNCUyMiUzQSU3QiUyMnhtbG5zJTNBeGxpbmslMjIlM0F0cnVlJTJDJTIyaWQlMjIlM0ElMjJlYTYlMjIlMkMlMjJ0eXBlJTIyJTNBJTIyZW1haWwlMjIlMkMlMjJocmVmJTIyJTNBJTIybWFpbHRvJTNBZHdhbmcyNCU0MGlsbGlub2lzLmVkdSUyMiU3RCUyQyUyMl8lMjIlM0ElMjJkd2FuZzI0JTQwaWxsaW5vaXMuZWR1JTIyJTdE"}]},{"#name":"affiliation","$":{"affiliation-id":"S0306457321001527-28696b5caf61d6d4c7133137072e7f2d","id":"aff1"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"School of Information Sciences, University of Illinois Urbana-Champaign, Champaign, IL, USA"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Information Sciences, University of Illinois Urbana-Champaign"},{"#name":"city","_":"Champaign"},{"#name":"state","_":"IL"},{"#name":"country","_":"USA"}]},{"#name":"source-text","$":{"id":"afs73"},"_":"School of Information Sciences University of Illinois Urbana-Champaign, Champaign, IL, USA"}]},{"#name":"affiliation","$":{"affiliation-id":"S0306457321001527-40cc0bb75fc5412081eeea8dd8a77d30","id":"aff2"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computer Science and Engineering, University of Notre Dame"},{"#name":"city","_":"Notre Dame"},{"#name":"state","_":"IN"},{"#name":"country","_":"USA"}]},{"#name":"source-text","$":{"id":"afs74"},"_":"Department of Computer Science and Engineering University of Notre Dame, Notre Dame, IN, USA"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"affiliations":{"aff1":{"#name":"affiliation","$":{"affiliation-id":"S0306457321001527-28696b5caf61d6d4c7133137072e7f2d","id":"aff1"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"School of Information Sciences, University of Illinois Urbana-Champaign, Champaign, IL, USA"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Information Sciences, University of Illinois Urbana-Champaign"},{"#name":"city","_":"Champaign"},{"#name":"state","_":"IL"},{"#name":"country","_":"USA"}]},{"#name":"source-text","$":{"id":"afs73"},"_":"School of Information Sciences University of Illinois Urbana-Champaign, Champaign, IL, USA"}]},"aff2":{"#name":"affiliation","$":{"affiliation-id":"S0306457321001527-40cc0bb75fc5412081eeea8dd8a77d30","id":"aff2"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computer Science and Engineering, University of Notre Dame"},{"#name":"city","_":"Notre Dame"},{"#name":"state","_":"IN"},{"#name":"country","_":"USA"}]},{"#name":"source-text","$":{"id":"afs74"},"_":"Department of Computer Science and Engineering University of Notre Dame, Notre Dame, IN, USA"}]}},"correspondences":{"cor1":{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author."}]}},"attachments":[],"scopusAuthorIds":{},"articles":{}},"authorMetadata":[],"banner":{"expanded":false},"biographies":{},"body":{},"browser":{"name":"Edge","engine":"Blink"},"chapters":{"toc":[],"isLoading":false},"changeViewLinks":{"showFullTextLink":true,"showAbstractLink":false},"citingArticles":{"hitCount":0,"viewMoreUrl":"","articles":[]},"combinedContentItems":{"content":[{"#name":"keywords","$$":[{"#name":"keywords","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:xoe":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"class":"keyword","view":"all","id":"d1e559"},"$$":[{"#name":"section-title","$":{"id":"d1e560"},"_":"Keywords"},{"#name":"keyword","$":{"id":"d1e562"},"$$":[{"#name":"text","_":"Offensive meme"}]},{"#name":"keyword","$":{"id":"d1e565"},"$$":[{"#name":"text","_":"Analogy-aware"}]},{"#name":"keyword","$":{"id":"d1e568"},"$$":[{"#name":"text","_":"Multi-modal learning"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"crossMark":{"isOpen":false},"domainConfig":{"cdnAssetsHost":"https://sciencedirect.elseviercdn.cn","assetRoute":"https://sciencedirect.elseviercdn.cn/prod/f624014e8238d0b70668b2e1dcc65fef7457d17f"},"downloadIssue":{"openOnPageLoad":false,"isOpen":false,"downloadCapOpen":false,"articles":[],"selected":[]},"enrichedContent":{"tableOfContents":false,"researchData":{"hasResearchData":false,"dataProfile":{},"openData":{},"mendeleyData":{},"databaseLinking":{}},"geospatialData":{"attachments":[]},"interactiveCaseInsights":{},"virtualMicroscope":{}},"entitledRecommendations":{"openOnPageLoad":false,"isOpen":false,"articles":[],"selected":[],"currentPage":1,"totalPages":1},"exam":{},"helpText":{"keyDates":{"html":"<div class=\"text-s key-dates-help\"><h3 class=\"u-margin-s-bottom u-h4\">Publication milestones</h3><p class=\"u-margin-m-bottom\">The dates displayed for an article provide information on when various publication milestones were reached at the journal that has published the article. Where applicable, activities on preceding journals at which the article was previously under consideration are not shown (for instance submission, revisions, rejection).</p><p class=\"u-margin-xs-bottom\">The publication milestones include:</p><ul class=\"key-dates-help-list u-margin-m-bottom u-padding-s-left\"><li><span class=\"u-text-italic\">Received</span>: The date the article was originally submitted to the journal.</li><li><span class=\"u-text-italic\">Revised</span>: The date the most recent revision of the article was submitted to the journal. Dates corresponding to intermediate revisions are not shown.</li><li><span class=\"u-text-italic\">Accepted</span>: The date the article was accepted for publication in the journal.</li><li><span class=\"u-text-italic\">Available online</span>: The date a version of the article was made available online in the journal.</li><li><span class=\"u-text-italic\">Version of Record</span>: The date the finalized version of the article was made available in the journal.</li></ul><p>More information on publishing policies can be found on the <a class=\"anchor u-display-inline anchor-alternative\" href=\"https://www.elsevier.com/about/policies-and-standards/publishing-ethics\" target=\"_blank\"><span class=\"anchor-text\">Publishing Ethics Policies</span></a> page. View our <a class=\"anchor u-display-inline anchor-alternative\" href=\"https://www.elsevier.com/researcher/author/submit-your-paper\" target=\"_blank\"><span class=\"anchor-text\">Publishing with Elsevier: step-by-step</span></a> page to learn more about the publishing process. For any questions on your own submission or other questions related to publishing an article, <a class=\"anchor u-display-inline anchor-alternative\" href=\"https://service.elsevier.com/app/phone/supporthub/publishing\" target=\"_blank\"><span class=\"anchor-text\">contact our Researcher support team.</span></a></p></div>","title":"What do these dates mean?"}},"glossary":{},"issueNavigation":{"previous":{},"next":{}},"linkingHubLinks":{},"metrics":{"isOpen":true},"preview":{"content":[{"#name":"introduction","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec1","role":"introduction","view":"all"},"$$":[{"#name":"label","_":"1"},{"#name":"section-title","$":{"id":"d1e576"},"_":"Introduction"},{"#name":"para","$":{"view":"all","id":"d1e578"},"$$":[{"#name":"__text__","_":"As the popularity of social networks continues to increase, social media platforms become an attractive breeding ground for amplifying offensive activities (e.g., hate speech, cyberbullying). People are increasingly exposed to online offensive content in recent years. For example, approximately 44% of Americans were subjected to online hate and harassment in 2020, and 28% of online social media users have experienced severe purposeful online harassment (e.g., sexual harassment, stalking, physical threats)."},{"#name":"sup","$":{"loc":"post"},"_":"1"},{"#name":"footnote","$":{"id":"fn1"},"$$":[{"#name":"label","_":"1"},{"#name":"note-para","$":{"view":"all","id":"d1e586"},"$$":[{"#name":"inter-ref","$":{"id":"interref1","href":"https://www.adl.org/online-hate-2020","type":"simple"},"_":"https://www.adl.org/online-hate-2020"},{"#name":"__text__","_":"."}]}]},{"#name":"__text__","_":" Social media platforms and researchers have been endeavoring to combat online offensive content. Many solutions have been developed to address cyber offensive behaviors. Examples include hate speech detection (Van Hee et al., 2018), cyber racism recognition (Jakubowicz et al., 2017), and online harassment identification (Jhaver, Ghoshal, Bruckman, & Gilbert, 2018). In this paper, we study an important problem of detecting "},{"#name":"italic","_":"offensive analogy meme"},{"#name":"__text__","_":" on online social media where the visual content and the texts/captions of the meme together make an "},{"#name":"italic","_":"analogy"},{"#name":"__text__","_":" to convey the offensive information. In particular, the offensive information is referred to as the explicit or implicit message deliberately expressed against individuals or a group of people (e.g., disparagement, animosity) (Suryawanshi, Chakravarthi, Arcan, & Buitelaar, 2020)."}]},{"#name":"para","$":{"view":"all","id":"d1e618"},"$$":[{"#name":"__text__","_":"Our problem is motivated by the prevalence of image-based content on online social media. Images often contain rich information and have been a key and attractive medium for people to create and share on social media. For example, an average of 95 million photos are uploaded to Instagram daily,"},{"#name":"sup","$":{"loc":"post"},"_":"2"},{"#name":"footnote","$":{"id":"fn2"},"$$":[{"#name":"label","_":"2"},{"#name":"note-para","$":{"view":"all","id":"d1e626"},"$$":[{"#name":"inter-ref","$":{"id":"interref2","href":"https://www.wired.co.uk/article/instagram-doubles-to-half-billion-users","type":"simple"},"_":"https://www.wired.co.uk/article/instagram-doubles-to-half-billion-users"},{"#name":"__text__","_":"."}]}]},{"#name":"__text__","_":" and more than 40% of tweets contain visual content."},{"#name":"sup","$":{"loc":"post"},"_":"3"},{"#name":"footnote","$":{"id":"fn3"},"$$":[{"#name":"label","_":"3"},{"#name":"note-para","$":{"view":"all","id":"d1e637"},"$$":[{"#name":"inter-ref","$":{"id":"interref3","href":"https://unionmetrics.com/blog/2017/11/include-image-video-tweets/","type":"simple"},"_":"https://unionmetrics.com/blog/2017/11/include-image-video-tweets/"},{"#name":"__text__","_":"."}]}]},{"#name":"__text__","_":" In addition, social media posts with images are more likely to attract user’s attention than those without (e.g., tweets with images can achieve 150% more retweets than the tweets without images"},{"#name":"sup","$":{"loc":"post"},"_":"4"},{"#name":"footnote","$":{"id":"fn4"},"$$":[{"#name":"label","_":"4"},{"#name":"note-para","$":{"view":"all","id":"d1e648"},"$$":[{"#name":"inter-ref","$":{"id":"interref4","href":"https://blog.hubspot.com/marketing/visual-content-marketing-strategy","type":"simple"},"_":"https://blog.hubspot.com/marketing/visual-content-marketing-strategy"},{"#name":"__text__","_":"."}]}]},{"#name":"__text__","_":"). However, the widespread presence of images on social media also provides opportunities for the dissemination of offensive contents. In particular, sophisticated content creators increasingly favor the image as the carrier to propagate offensive information that is implicitly expressed with the accompanying text embedded in the image. Such kind of images can circumvent existing content censorship that focuses on the explicit indecent contents (e.g., sexual images, hateful vocabulary). Therefore, it is critical to effectively identify these image-driven offensive content to curb the spread of offensive information and reduce the propagation of extreme ideology on online platforms."}]},{"#name":"para","$":{"view":"all","id":"d1e653"},"$$":[{"#name":"__text__","_":"In this paper, we focus on an emerging phenomenon on social media where the visual content of an image together with the auxiliary text superimposed on or associated with the image jointly make an "},{"#name":"italic","_":"analogy"},{"#name":"__text__","_":" to convey offensive information to the audience of the post. We refer to such kind of content as "},{"#name":"italic","_":"offensive analogy meme"},{"#name":"__text__","_":". Fig. 1 shows four examples of the offensive analogy memes from online social media. Fig. 1(a) used an analogy of a white shark to express the support of the Caucasian people. Fig. 1(b) delivered an implicit analogy that Jewish people are taking advantage of Black people to attack the White community. Similarly, Fig. 1(c) showed a favor of White privilege using an analogy of animals, and Fig. 1(d) leveraged the analogy of the quiet vs. energetic behavior on the train to reveal the hateful attitude against Black people. The identification of such offensive analogy memes on online social media can have multiple positive impacts in the real world. First, it can significantly reduce the spread of offensive content and ideology of hatred in online communities. For example, online users can be advised about the potential harmfulness of these offensive analogy memes before posting or sharing them on online platforms (e.g., online social media). Second, the effective identification of the analogy expressed in the multi-modal memes can also be helpful for better understanding and analyzing multi-modal content which can be further applied to other real-world applications (e.g., image captioning Hossain, Sohel, Shiratuddin, & Laga, 2019, image retrieval Zhou, Li, & Tian, 2017). For example, the identified analogical relations between the visual and textual content in images can greatly enhance the image captioning tools for generating more meaningful captions."}]},{"#name":"para","$":{"view":"all","id":"d1e691"},"_":"Recently, several multi-modal solutions (Chauhan et al., 2020, Sabat et al., 2019, Sharma et al., 2020, Suryawanshi et al., 2020, Velioglu and Rose, 2020) have been proposed to address the offensive meme detection problem on social media. A representative set of existing solutions are leveraging pre-trained neural network models (e.g., convolutional neural network based model (Simonyan & Zisserman, 2014) for visual input, BERT (Devlin, Chang, Lee, & Toutanova, 2018) or transformer-based encoder (Vaswani et al., 2017) for text input) to extract the latent features from the visual content and embedded captions in the multi-modal memes. The latent features are concatenated as the input to a classifier (e.g., Multi-layer Perceptron (Noriega, 2005)) for classifying offensive meme posts. However, the above solutions only focus on a direct combination of the multi-modal features from the visual content and embedded captions but ignore the implicit relation between them as well as the analogy they deliver together. One important observation of the above examples (Fig. 1) is that these offensive analogy memes do not necessarily contain any explicit offensive or hateful content (e.g., hate speech or image) that can be leveraged to quickly detect them. Therefore, the detection of offensive analogy meme is a non-trivial problem and cannot be fully addressed by existing solutions. We elaborate the key challenges of solving this problem below."},{"#name":"para","$":{"view":"all","id":"d1e727"},"$$":[{"#name":"italic","_":"Analogy Awareness."},{"#name":"__text__","_":" The first challenge of detecting offensive analogy meme lies in correctly capturing and understanding the analogy expressed by the meme. For example, the analogy between the “black bowling ball hits the white bowling pins” and the “Black people ruin the White community” in Fig. 1(b) is critical to detect that offensive meme. However, the extraction of such analogy often requires a holistic analysis of the visual content, embedded caption, and user comments of the meme if available (Sharma et al., 2020). Moreover, the analogy of the offensive meme can also hide in the contextual information (e.g., the text description associated with the meme). For example, Fig. 1(c) will go undetected if we ignore the analogy between the “WHITE” caption in the image and the “white man” in the text description. Such a meme can be completely appropriate when it appears in the wildlife protection forum. The existing solutions that focus on the image or text content itself are often insufficient to capture the analogy in such offensive memes (Sharma et al., 2020). Therefore, such an analogy has to be carefully captured and considered in the process of offensive meme detection on social media."}]},{"#name":"para","$":{"view":"all","id":"d1e750"},"$$":[{"#name":"italic","_":"Complex Multi-modal Analogy Alignment."},{"#name":"__text__","_":" The second challenge of detecting offensive analogy meme lies in the accurate alignment of the complex analogy across different data modalities in a meme post. For example, existing solutions for embedded caption extraction highly rely on the optical character recognition (OCR) technique (Islam, Islam, & Noor, 2017). However, the OCR technique only focuses on recognizing all the characters in an image and can often recognize irrelevant content (e.g., copyright watermark). Such irrelevant OCR texts may lead to the identification of incorrect analogy in the meme. Moreover, it is also important to accurately capture the analogical relation between the visual and textual content in the meme. For example, as the image shown in Fig. 1(b), the implicit offensive content against Jewish people and Black people cannot be captured if the visual content and textual captions are incorrectly matched. The positions of the visual content and embedded captions have to be carefully considered to capture the analogy (i.e., bowler – “Jews”, black bowling ball – “Black people”, white bowling pins – “A quiet, peaceful, functioning society” in the above example). However, current multi-modal meme solutions that simply integrate visual and textual features of a meme are insufficient to capture the analogy embedded across different data modalities in the meme (Kiela et al., 2020)."}]},{"#name":"para","$":{"view":"all","id":"d1e769"},"$$":[{"#name":"__text__","_":"To address the above challenges, we develop a deep learning based Analogy-aware Offensive Meme Detection (AOMD) framework that can effectively identify offensive analogy memes on online social media. In particular, to address the analogy awareness challenge, we develop an analogy-aware multi-modal representation learning module to incorporate the "},{"#name":"italic","_":"content"},{"#name":"__text__","_":" (i.e., image, embedded caption) and "},{"#name":"italic","_":"contextual information"},{"#name":"__text__","_":" (i.e., text description, user comments) to identify the analogy expressed in the meme. To address the complex multi-modal analogy alignment challenge, we develop an attentive multi-modal analogy alignment module to explicitly model the relation between the visual content and textual caption in the memes. To the best of our knowledge, AOMD is the first analogy-aware deep learning based solution to detect offensive analogy memes on social media. We evaluate the AOMD framework on two real-world datasets collected from Gab and Reddit. Evaluation results show that AOMD achieves significant performance gains compared to state-of-the-art baseline methods by detecting offensive analogy memes more accurately."}]}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec2","view":"all"},"$$":[{"#name":"label","_":"2"},{"#name":"section-title","$":{"id":"d1e780"},"_":"Related work"},{"#name":"section","$":{"id":"sec2.1","view":"all"},"$$":[{"#name":"label","_":"2.1"},{"#name":"section-title","$":{"id":"d1e785"},"_":"Social media misbehavior"},{"#name":"para","$":{"view":"all","id":"d1e787"},"_":"Misbehavior has become a severe issue on social media platforms (Wang et al., 2015, Wang et al., 2012, Wang et al., 2019). Examples of social media misbehavior include cyberbullying (Englander et al., 2017, Van Hee et al., 2018), trolling (Paavola, Helo, Jalonen, Sartonen, & Huhtinen, 2016), hateful content (Magu et al., 2017, Ribeiro et al., 2017), rumors (Choi, Chun, Oh, Han, et al., 2020), and fake news (Shu et al., 2017, Zhang et al., 2018). For example, Yao et al. proposed an online"}]}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec3","view":"all"},"$$":[{"#name":"label","_":"3"},{"#name":"section-title","$":{"id":"d1e1014"},"_":"Problem definition"},{"#name":"para","$":{"view":"all","id":"d1e1016"},"_":"In this section, we formally present the offensive analogy meme detection problem on social media. We first define a few key terms that will be used in the problem definition."},{"#name":"para","$":{"view":"all","id":"d1e1018"},"$$":[{"#name":"enunciation","$":{"id":"dfn1"},"$$":[{"#name":"label","_":"Definition 1"},{"#name":"section-title","$":{"id":"d1e1023"},"$$":[{"#name":"__text__","_":"Meme Post "},{"#name":"math","$":{"display":"inline","id":"d1e1026","altimg":"si1.svg"},"$$":[{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"P"}]},{"#name":"mrow","$$":[{"#name":"mi","_":"i"}]}]}]}]},{"#name":"para","$":{"view":"all","id":"d1e1034"},"$$":[{"#name":"__text__","_":"A meme post ("},{"#name":"math","$":{"display":"inline","id":"d1e1038","altimg":"si1.svg"},"$$":[{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"P"}]},{"#name":"mrow","$$":[{"#name":"mi","_":"i"}]}]}]},{"#name":"__text__","_":") on social media contains three major components: (i) "},{"#name":"italic","_":"meme ("},{"#name":"math","$":{"display":"inline","id":"d1e1050","altimg":"si3.svg"},"$$":[{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"M"}]},{"#name":"mrow","$$":[{"#name":"mi","_":"i"}]}]}]},{"#name":"italic","_":")"},{"#name":"__text__","_":", (ii) "},{"#name":"italic","_":"text description ("},{"#name":"math","$":{"display":"inline","id":"d1e1065","altimg":"si4.svg"},"$$":[{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"D"}]},{"#name":"mrow","$$":[{"#name":"mi","_":"i"}]}]}]},{"#name":"italic","_":")"},{"#name":"__text__","_":", and (iii) "},{"#name":"italic","_":"user comments ("},{"#name":"math","$":{"display":"inline","id":"d1e1079","altimg":"si5.svg"},"$$":[{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"U"}]},{"#name":"mrow","$$":[{"#name":"mi","_":"i"}]}]}]},{"#name":"italic","_":")"},{"#name":"__text__","_":". An example of the meme post on social media is shown in Fig. 2."}]}]}]},{"#name":"para","$":{"view":"all","id":"d1e1094"},"$$":[{"#name":"enunciation","$":{"id":"dfn2"},"$$":[{"#name":"label","_":"Definition 2"},{"#name":"section-title","$":{"id":"d1e1099"},"$$":[{"#name":"__text__","_":"Meme "},{"#name":"math","$":{"display":"inline","id":"d1e1102","altimg":"si3.svg"},"$$":[{"#name":"msub","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"M"}]},{"#name":"mrow","$$":[{"#name":"mi","_":"i"}]}]}]}]},{"#name":"para","$":{"view":"all","id":"d1e1110"},"$$":[{"#name":"__text__","_":"The image attachment of a meme post. The meme attachment contains two parts: the "},{"#name":"italic","_":"visual content ("},{"#name":"math","$":{"display":"inline","id":"d1e1117","altimg":"si7.svg"},"$$":[{"#name":"msubsup","$$":[{"#name":"mrow","$$":[{"#name":"mi","_":"M"}]},{"#name":"mrow","$$":[{"#name":"mi","_":"i"}]},{"#name":"mrow","$$":[{"#name":"mi","_":"V"}]}]}]},{"#name":"italic","_":")"},{"#name":"__text__","_":" and "},{"#name":"italic","_":"embedded caption "}]}]}]}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec4","view":"all"},"$$":[{"#name":"label","_":"4"},{"#name":"section-title","$":{"id":"d1e1570"},"_":"Solution"},{"#name":"para","$":{"view":"all","id":"d1e1572"},"$$":[{"#name":"__text__","_":"In this section, we present the Analogy-aware Offensive Meme Detection (AOMD) framework to address the offensive analogy meme detection problem defined in the previous section. An overview of the AOMD framework is shown in Fig. 3. The AOMD framework contains three major components. First, the "},{"#name":"italic","_":"analogy-aware multi-modal representation learning"},{"#name":"__text__","_":" module is designed to extract the visual, textual, and contextual features from the content of meme posts and encode these multi-modal features into the"}]}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec5","view":"all"},"$$":[{"#name":"label","_":"5"},{"#name":"section-title","$":{"id":"d1e3774"},"_":"Data"},{"#name":"para","$":{"view":"all","id":"d1e3776"},"_":"In this section, we present the real-world datasets and labels we collected for evaluation. We observe that mainstream social media platforms (e.g., Twitter) contain a massive amount of memes. However, the collection of offensive memes on those platforms has experienced a long tail issue (i.e., only a small portion of the collected memes are actually offensive) (Zhang & Luo, 2019). In addition, we observe that existing datasets for offensive memes (e.g., MultiOFF (Suryawanshi et al., 2020),"}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec6","view":"all"},"$$":[{"#name":"label","_":"6"},{"#name":"section-title","$":{"id":"d1e3837"},"_":"Evaluation"},{"#name":"para","$":{"view":"all","id":"d1e3839"},"_":"In this section, we evaluate the detection performance of the AOMD framework on two real-world datasets described in Section 5. In particular, we first investigate the performance of AOMD and state-of-the-art uni-modal and multi-modal baselines in identifying offensive memes on social media. We further study AOMD’s effectiveness in detecting offensive meme posts that contain analogy. The results show that the AOMD framework achieves significant performance gains in terms of the offensive"}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"sec7","role":"conclusion","view":"all"},"$$":[{"#name":"label","_":"7"},{"#name":"section-title","$":{"id":"d1e4083"},"_":"Conclusion"},{"#name":"para","$":{"view":"all","id":"d1e4085"},"_":"In this paper, we develop AOMD, the first analogy-aware deep learning based solution to address offensive analogy memes on social media. The AOMD framework is designed to effectively capture the analogy conveyed by different data modalities of the meme, and detect the offensiveness implicitly expressed in the meme. We evaluate the AOMD framework with two real-world datasets collected from Gab and Reddit. The evaluation results demonstrate that our scheme outperforms the state-of-the-art"}]}]},{"#name":"section","$$":[{"#name":"section","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"view":"all","id":"d1e4087"},"$$":[{"#name":"section-title","$":{"id":"d1e4088"},"_":"CRediT authorship contribution statement"},{"#name":"para","$":{"view":"all","id":"d1e4090"},"$$":[{"#name":"bold","_":"Lanyu Shang:"},{"#name":"__text__","_":" Conceptualization, Methodology, Formal analysis, Writing - original draft. "},{"#name":"bold","_":"Yang Zhang:"},{"#name":"__text__","_":" Conceptualization, Writing - original draft. "},{"#name":"bold","_":"Yuheng Zha:"},{"#name":"__text__","_":" Data curation, Formal analysis. "},{"#name":"bold","_":"Yingxi Chen:"},{"#name":"__text__","_":" Data curation. "},{"#name":"bold","_":"Christina Youn:"},{"#name":"__text__","_":" Data curation. "},{"#name":"bold","_":"Dong Wang:"},{"#name":"__text__","_":" Supervision, Writing -review & editing."}]}]}]},{"#name":"section","$$":[{"#name":"acknowledgment","$":{"xmlns:ce":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"view":"all","id":"d1e4109"},"$$":[{"#name":"section-title","$":{"id":"d1e4110"},"_":"Acknowledgments"},{"#name":"para","$":{"view":"all","id":"d1e4112"},"$$":[{"#name":"__text__","_":"This research is supported in part by the "},{"#name":"grant-sponsor","$":{"sponsor-id":"http://dx.doi.org/10.13039/100000001","id":"GS1","type":"simple","role":"http://www.elsevier.com/xml/linking-roles/grant-sponsor"},"_":"National Science Foundation, USA"},{"#name":"__text__","_":" under Grant No. "},{"#name":"grant-number","$":{"refid":"GS1","id":"d1e4117"},"_":"IIS-2008228"},{"#name":"__text__","_":", "},{"#name":"grant-number","$":{"refid":"GS1","id":"d1e4120"},"_":"CNS-1845639"},{"#name":"__text__","_":", "},{"#name":"grant-number","$":{"refid":"GS1","id":"d1e4123"},"_":"CNS-1831669"},{"#name":"__text__","_":", "},{"#name":"grant-sponsor","$":{"sponsor-id":"http://dx.doi.org/10.13039/100000183","id":"GS2","type":"simple","role":"http://www.elsevier.com/xml/linking-roles/grant-sponsor"},"_":"Army Research Office, USA"},{"#name":"__text__","_":"\n under Grant "},{"#name":"grant-number","$":{"refid":"GS2","id":"d1e4132"},"_":"W911NF-17-1-0409"},{"#name":"__text__","_":". The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for"}]}]}]}],"floats":[],"footnotes":[{"#name":"footnote","$":{"id":"fn1"},"$$":[{"#name":"label","_":"1"},{"#name":"note-para","$":{"view":"all","id":"d1e586"},"$$":[{"#name":"inter-ref","$":{"id":"interref1","href":"https://www.adl.org/online-hate-2020","type":"simple"},"_":"https://www.adl.org/online-hate-2020"},{"#name":"__text__","_":"."}]}]},{"#name":"footnote","$":{"id":"fn2"},"$$":[{"#name":"label","_":"2"},{"#name":"note-para","$":{"view":"all","id":"d1e626"},"$$":[{"#name":"inter-ref","$":{"id":"interref2","href":"https://www.wired.co.uk/article/instagram-doubles-to-half-billion-users","type":"simple"},"_":"https://www.wired.co.uk/article/instagram-doubles-to-half-billion-users"},{"#name":"__text__","_":"."}]}]},{"#name":"footnote","$":{"id":"fn3"},"$$":[{"#name":"label","_":"3"},{"#name":"note-para","$":{"view":"all","id":"d1e637"},"$$":[{"#name":"inter-ref","$":{"id":"interref3","href":"https://unionmetrics.com/blog/2017/11/include-image-video-tweets/","type":"simple"},"_":"https://unionmetrics.com/blog/2017/11/include-image-video-tweets/"},{"#name":"__text__","_":"."}]}]},{"#name":"footnote","$":{"id":"fn4"},"$$":[{"#name":"label","_":"4"},{"#name":"note-para","$":{"view":"all","id":"d1e648"},"$$":[{"#name":"inter-ref","$":{"id":"interref4","href":"https://blog.hubspot.com/marketing/visual-content-marketing-strategy","type":"simple"},"_":"https://blog.hubspot.com/marketing/visual-content-marketing-strategy"},{"#name":"__text__","_":"."}]}]}],"attachments":[]},"questionsAndAnswers":{},"rawtext":"","recommendations":{"articles":[{"pii":"S0140673618331805","doi":"10.1016/S0140-6736(18)33180-5","journalTitle":"The Lancet","publicationYear":"2019","publicationDate":"2019-04-06","volumeSupText":"Volume 393, Issue 10179","articleNumber":"","pageRange":"1416","trace-token":"AAAAQCUFiolE--HS9d_fUxr2nfOvEqRrJCuoAe0haiDCs1bRZbN7s9KbOL5DVeXeHAWMLWnZ3OLyIr9vpAGjuTVeDhvh9KqQi-_9BuShgiBZ4eELIvl5ew","authors":{"content":[{"#name":"author-group","$":{"id":"aug10"},"$$":[{"#name":"author","$":{"id":"au10","author-id":"S0140673618331805-a15903ef3e2b92f7a9415e2238ef31e6"},"$$":[{"#name":"given-name","_":"Kumi"},{"#name":"surname","_":"Oshima"},{"#name":"cross-ref","$":{"id":"cecref10","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ceeadd10","type":"email","href":"mailto:kumioshima-tky@umin.ac.jp"},"_":"kumioshima-tky@umin.ac.jp"}]},{"#name":"author","$":{"id":"au20","author-id":"S0140673618331805-1fd2cc00f387abdc61b7da21cc7f88ce"},"$$":[{"#name":"given-name","_":"Akihiko"},{"#name":"surname","_":"Ozaki"},{"#name":"cross-ref","$":{"id":"cecref20","refid":"aff2"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]}]},{"#name":"author","$":{"id":"au30","author-id":"S0140673618331805-c8939d9ad96509ad452d3a1745fea630"},"$$":[{"#name":"given-name","_":"Jinichi"},{"#name":"surname","_":"Mori"},{"#name":"cross-ref","$":{"id":"cecref30","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au40","author-id":"S0140673618331805-6f17c25176c478fd6dcf9389a1eb8a86"},"$$":[{"#name":"given-name","_":"Morihito"},{"#name":"surname","_":"Takita"},{"#name":"cross-ref","$":{"id":"cecref40","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au50","author-id":"S0140673618331805-4ec778ab787044e4de3d83daacbf6cf7"},"$$":[{"#name":"given-name","_":"Tetsuya"},{"#name":"surname","_":"Tanimoto"},{"#name":"cross-ref","$":{"id":"cecref50","refid":"aff3"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]}]},{"#name":"affiliation","$":{"id":"aff1","affiliation-id":"S0140673618331805-e4786ffb94fd01a7d502f68d5b102ec0"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Department of Hematology, Jyoban Hospital, Tokiwa Foundation, Fukushima 972–8322, Japan"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Hematology"},{"#name":"organization","_":"Jyoban Hospital"},{"#name":"organization","_":"Tokiwa Foundation"},{"#name":"city","_":"Fukushima"},{"#name":"postal-code","_":"972–8322"},{"#name":"country","_":"Japan"}]}]},{"#name":"affiliation","$":{"id":"aff2","affiliation-id":"S0140673618331805-ac4a6e3c9363b1c1a9f54394914c5e64"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Department of Surgery, Jyoban Hospital, Tokiwa Foundation, Fukushima 972–8322, Japan"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Surgery"},{"#name":"organization","_":"Jyoban Hospital"},{"#name":"organization","_":"Tokiwa Foundation"},{"#name":"city","_":"Fukushima"},{"#name":"postal-code","_":"972–8322"},{"#name":"country","_":"Japan"}]}]},{"#name":"affiliation","$":{"id":"aff3","affiliation-id":"S0140673618331805-e06c685f1e904d410e6911d218860ae5"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"Department of Internal Medicine, Jyoban Hospital, Tokiwa Foundation, Fukushima 972–8322, Japan"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Internal Medicine"},{"#name":"organization","_":"Jyoban Hospital"},{"#name":"organization","_":"Tokiwa Foundation"},{"#name":"city","_":"Fukushima"},{"#name":"postal-code","_":"972–8322"},{"#name":"country","_":"Japan"}]}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"id":"au50","author-id":"S0140673618331805-4ec778ab787044e4de3d83daacbf6cf7"},"$$":[{"#name":"given-name","_":"Tetsuya"},{"#name":"surname","_":"Tanimoto"},{"#name":"cross-ref","$":{"id":"cecref50","refid":"aff3"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"dochead","$":{"id":"cedoch10"},"$$":[{"#name":"textfn","_":"Correspondence"}]},{"#name":"title","$":{"id":"cetitle10"},"_":"Entrance examination misogyny in Japanese medical schools"}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"cor","content-family":"serial","contentType":"JL","entitlementType":"","abstract":{},"pdf":{"urlType":null},"iss-first":"10179","vol-first":"393","isThirdParty":false,"language":"en","issn-primary-unformatted":"01406736","issn-primary-formatted":"0140-6736"},{"pii":"S0020025522003917","doi":"10.1016/j.ins.2022.04.039","journalTitle":"Information Sciences","publicationYear":"2022","publicationDate":"2022-07-01","volumeSupText":"Volume 603","articleNumber":"","pageRange":"262-288","trace-token":"AAAAQCUFiolE--HS9d_fUxr2nfPd_SZZ_RjJZOJwBfzlVYtI9RLqRCoVL6vY_rha6BPZNrfEd7XZWqsbFpOflDWug5VavAewla2PCI2fY3ag7OfPUDSiUw","authors":{"content":[{"#name":"author-group","$":{"id":"ag005"},"$$":[{"#name":"author","$":{"id":"au005","author-id":"S0020025522003917-faa78cf43f254f3931bca3f51bc986b6"},"$$":[{"#name":"given-name","_":"Fan"},{"#name":"surname","_":"Yang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/data-curation"},"_":"Data curation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing – original draft"},{"#name":"cross-ref","$":{"id":"ar005","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"af010","id":"c8890"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"em005","type":"email","href":"mailto:f.yang@liacs.leidenuniv.nl"},"_":"f.yang@liacs.leidenuniv.nl"}]},{"#name":"author","$":{"id":"au010","author-id":"S0020025522003917-a4087635d715a242e517379fc59b6210"},"$$":[{"#name":"given-name","_":"Yanan"},{"#name":"surname","_":"Qiao"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar010","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"ar015","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"em010","type":"email","href":"mailto:qiaoyanan@mail.xjtu.edu.cn"},"_":"qiaoyanan@mail.xjtu.edu.cn"}]},{"#name":"author","$":{"id":"au555","author-id":"S0020025522003917-c74e10e30d0c2112600022968a477ee3"},"$$":[{"#name":"given-name","_":"Yong"},{"#name":"surname","_":"Qi"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"refid":"af005","id":"c9905"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au655","author-id":"S0020025522003917-930443b6ae14e76836a89615f8aa1f5e"},"$$":[{"#name":"given-name","_":"Junge"},{"#name":"surname","_":"Bo"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"cross-ref","$":{"refid":"af005","id":"c88145"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"author","$":{"id":"au7790","author-id":"S0020025522003917-4e7a092482df90d52c720c8e187174a9"},"$$":[{"#name":"given-name","_":"Xiao"},{"#name":"surname","_":"Wang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"cross-ref","$":{"refid":"af005","id":"c7890"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]},{"#name":"affiliation","$":{"id":"af005","affiliation-id":"S0020025522003917-ab7eefd986673f36c5b7f6a256b4dda3"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"School of Computer Science and Technology, Xi’an Jiaotong University, No.28, Xianning West Road, Xi’an, Shaanxi 710049, PR China"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Computer Science and Technology"},{"#name":"organization","_":"Xi’an Jiaotong University"},{"#name":"address-line","_":"No.28, Xianning West Road"},{"#name":"city","_":"Xi’an"},{"#name":"state","_":"Shaanxi"},{"#name":"postal-code","_":"710049"},{"#name":"country","_":"PR China"}]},{"#name":"source-text","$":{"id":"str005"},"_":"School of Computer Science and Technology, Xi’an Jiaotong University, No.28, Xianning West Road, Xi’an, Shaanxi, 710049, P.R. China"}]},{"#name":"affiliation","$":{"id":"af010","affiliation-id":"S0020025522003917-1faa6e5d5883fe9994a85c2ae838b81e"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Leiden Institute of Advanced Computer Science (LIACS), Leiden University, Leiden, the Netherlands"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Leiden Institute of Advanced Computer Science (LIACS)"},{"#name":"organization","_":"Leiden University"},{"#name":"city","_":"Leiden"},{"#name":"country","_":"the Netherlands"}]},{"#name":"source-text","$":{"id":"srctPC_affgACJYFFUtM"},"_":"Leiden Institute of Advanced Computer Science (LIACS), Leiden University, Leiden, the Netherlands"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"id":"au7790","author-id":"S0020025522003917-4e7a092482df90d52c720c8e187174a9"},"$$":[{"#name":"given-name","_":"Xiao"},{"#name":"surname","_":"Wang"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"cross-ref","$":{"refid":"af005","id":"c7890"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"title","$":{"id":"tm005"},"_":"BMP: A blockchain assisted meme prediction method through exploring contextual factors from social networks"}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","abstract":{"$$":[{"$$":[{"$":{"id":"st210"},"#name":"section-title","_":"Abstract"},{"$$":[{"$":{"view":"all","id":"sp005"},"#name":"simple-para","_":"In the era of social networks, the scale and speed of online information dissemination have been greatly enhanced, thus leading to a large number of meme being generated, spread and popularized in the Internet. Social networks strongly promote the replication and dissemination of modalities, which are powerfully explosive and can be copied and spread in large numbers in a short period of time. Because of the typical decentralized nature of meme propagation in social networks, the development of blockchain technology provides a reliable environment for meme propagation models. However, there are various hurdles to using online user behavior data to predict meme popularity, such as the fact that there are only a few user comments and retweets beneath a meme topic, making it difficult to predict meme popularity directly by basic social data mining at this moment. In this scenario, mining the contextual information of web users can considerably increase meme prediction performance. Because social data contains a wealth of contextual and user relationship characteristics, we offer for the first time a blockchain-assisted based meme popularity prediction (BMP) mechanism based on empirical approach. To begin, we suggest a blockchain-based data storage approach to mimic a decentralized ecosystem. Following that, we assess meme popularity in terms of four contextually based web user characteristics. Finally, using a probabilistic linear model, we present a meme popularity prediction model that integrates the four contextual characteristics. By making a comparison of comprehensive tests with existing methodologies, we illustrate the usefulness and accuracy of our proposed model. The experimental results indicate that the proposed meme prediction approach can provide a meme prediction service with high accuracy, a well-defined decentralized environment, and steady performance, making it a reliable service for recommendation systems and web-based information dissemination."}],"$":{"view":"all","id":"as005"},"#name":"abstract-sec"}],"$":{"view":"all","id":"ab005","lang":"en","class":"author"},"#name":"abstract"}],"$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"#name":"abstracts"},"pdf":{"urlType":null},"iss-first":"","vol-first":"603","isThirdParty":false,"language":"en","issn-primary-unformatted":"00200255","issn-primary-formatted":"0020-0255"},{"pii":"S0378216620302587","doi":"10.1016/j.pragma.2020.10.006","journalTitle":"Journal of Pragmatics","publicationYear":"2021","publicationDate":"2021-01-01","volumeSupText":"Volume 171","articleNumber":"","pageRange":"101-117","trace-token":"AAAAQCUFiolE--HS9d_fUxr2nfMQFO8NP0q_nauSqqE0wIbrwNpIdBKpQf1K9BX3XRHsKhVKVzsZbonmidksuuB_UfX4msk04uMi_fgcbdzBcORzq43M_w","authors":{"content":[{"#name":"author-group","$":{"id":"augrp0010"},"$$":[{"#name":"author","$":{"id":"au1","biographyid":"vt1","author-id":"S0378216620302587-e2fa46dc52312d02efb777b1459d9536"},"$$":[{"#name":"given-name","_":"Camilla"},{"#name":"surname","_":"Vásquez"},{"#name":"cross-ref","$":{"id":"crosref0010","refid":"aff1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"eadd0010","type":"email","href":"mailto:cvasquez@usf.edu"},"_":"cvasquez@usf.edu"}]},{"#name":"author","$":{"id":"au2","biographyid":"vt2","orcid":"0000-0002-4174-5493","author-id":"S0378216620302587-24a93a0533962092fbb07976bf94c91a"},"$$":[{"#name":"given-name","_":"Erhan"},{"#name":"surname","_":"Aslan"},{"#name":"cross-ref","$":{"id":"crosref0015","refid":"aff2"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"id":"crosref0020","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"∗"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"eadd0015","type":"email","href":"mailto:erhan.aslan@reading.ac.uk"},"_":"erhan.aslan@reading.ac.uk"}]},{"#name":"affiliation","$":{"id":"aff1","affiliation-id":"S0378216620302587-6290a911aac308ee9dbfbef836832e8b"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Applied Linguistics, Department of World Languages, University of South Florida, Tampa, 33620, USA"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Applied Linguistics"},{"#name":"organization","_":"Department of World Languages"},{"#name":"organization","_":"University of South Florida"},{"#name":"city","_":"Tampa"},{"#name":"postal-code","_":"33620"},{"#name":"country","_":"USA"}]},{"#name":"source-text","$":{"id":"srct0010"},"_":"Applied Linguistics, Department of World Languages, University of South Florida, Tampa, 33620, USA"}]},{"#name":"affiliation","$":{"id":"aff2","affiliation-id":"S0378216620302587-17716dbdfef0043e82756d2470be3658"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"TESOL and Applied Linguistics, University of Reading, 210B Edith Morley, Whiteknights, Reading, RG6 6AA, UK"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"TESOL and Applied Linguistics"},{"#name":"organization","_":"University of Reading"},{"#name":"address-line","_":"210B Edith Morley"},{"#name":"city","_":"Whiteknights"},{"#name":"state","_":"Reading"},{"#name":"postal-code","_":"RG6 6AA"},{"#name":"country","_":"UK"}]},{"#name":"source-text","$":{"id":"srct0015"},"_":"TESOL and Applied Linguistics, University of Reading, 210B Edith Morley, Whiteknights, Reading, RG6 6AA, UK"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"∗"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"id":"au2","biographyid":"vt2","orcid":"0000-0002-4174-5493","author-id":"S0378216620302587-24a93a0533962092fbb07976bf94c91a"},"$$":[{"#name":"given-name","_":"Erhan"},{"#name":"surname","_":"Aslan"},{"#name":"cross-ref","$":{"id":"crosref0015","refid":"aff2"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"id":"crosref0020","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"∗"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"eadd0015","type":"email","href":"mailto:erhan.aslan@reading.ac.uk"},"_":"erhan.aslan@reading.ac.uk"}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"title","$":{"id":"title0010"},"_":"“Cats be outside, how about meow”: Multimodal humor and creativity in an internet meme"}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","abstract":{"$$":[{"$$":[{"$":{"id":"sectitle0010"},"#name":"section-title","_":"Abstract"},{"$$":[{"$$":[{"#name":"__text__","_":"Adding to research on internet memes and humor, this study examines a set of image macros related to a specific viral media event (i.e., “cash me ousside”). This particular meme is linked to a popular catchphrase uttered by a young teenage girl, who appeared on a 2016 episode of the U.S. television talk show, "},{"#name":"italic","_":"Dr. Phil"},{"#name":"__text__","_":". We compiled a dataset of 220 image macros related to this media event from three online platforms. Our analysis focuses on various forms of linguistic humor, which most often rely on multimodal interactions between textual and visual elements. Our findings reveal multiple instances of wordplay involving paronymy as well as register-based humor and humor involving the voicing of recognizable figures. In addition, multimodal play often involves semiotic blends of this particular meme with other popular memes. Overall, our study illustrates unique combinations of unexpected elements and semiotic extensions of a non-standard catchphrase with other visual images, thereby establishing incongruity and generating humorous meanings."}],"$":{"view":"all","id":"abspara0010"},"#name":"simple-para"}],"$":{"view":"all","id":"abssec0010"},"#name":"abstract-sec"}],"$":{"view":"all","id":"abs0010","lang":"en","class":"author"},"#name":"abstract"},{"$$":[{"$":{"id":"sectitle0015"},"#name":"section-title","_":"Highlights"},{"$$":[{"$$":[{"$$":[{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0010"},"#name":"para","_":"Creative mechanisms of linguistic humor in internet memes may involve wordplay and shifts in linguistic register or style."}],"$":{"id":"u0010"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0015"},"#name":"para","_":"Creative humor involves interactions between textual or visual components of memes along with other user-generated content."}],"$":{"id":"u0015"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0020"},"#name":"para","_":"Image macros exhibit unique multimodal juxtapositions extending the prototypical text-image structure."}],"$":{"id":"u0020"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0025"},"#name":"para","_":"Multiple layers of meaning are embedded in memes and their interpretation depends on advanced inferential strategies."}],"$":{"id":"u0025"},"#name":"list-item"}],"$":{"id":"ulist0010"},"#name":"list"}],"$":{"view":"all","id":"abspara0015"},"#name":"simple-para"}],"$":{"view":"all","id":"abssec0015"},"#name":"abstract-sec"}],"$":{"view":"all","id":"abs0015","lang":"en","class":"author-highlights"},"#name":"abstract"}],"$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"#name":"abstracts"},"pdf":{"urlType":null},"iss-first":"","vol-first":"171","isThirdParty":false,"language":"en","issn-primary-unformatted":"03782166","issn-primary-formatted":"0378-2166"},{"pii":"S095741742100871X","doi":"10.1016/j.eswa.2021.115458","journalTitle":"Expert Systems with Applications","publicationYear":"2021","publicationDate":"2021-12-15","volumeSupText":"Volume 185","articleNumber":"115458","pageRange":"115458","trace-token":"AAAAQCUFiolE--HS9d_fUxr2nfPMm202ZbGFtPojoNKBV50hBL0RNzxxrhw1w4WXt_yHySO0QookE_rDlu13JduH-ZQ7D0kA8sqpUaTzDJIU8Yk76Op69g","authors":{"content":[{"#name":"author-group","$":{"id":"d1e1781"},"$$":[{"#name":"author","$":{"id":"au000001","author-id":"S095741742100871X-2feb3cce07facd434520afdb567622a3"},"$$":[{"#name":"given-name","_":"Mayukh"},{"#name":"surname","_":"Sharma"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/visualization"},"_":"Visualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea1","type":"email","href":"mailto:mayukh.sharma2016@vitalum.ac.in"},"_":"mayukh.sharma2016@vitalum.ac.in"}]},{"#name":"author","$":{"id":"au000002","author-id":"S095741742100871X-6e1c46f88cb637ae50549afcc9c3bef8","orcid":"0000-0003-4826-9466"},"$$":[{"#name":"given-name","_":"Ilanthenral"},{"#name":"surname","_":"Kandasamy"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/formal-analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/data-curation"},"_":"Data curation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing – original draft"},{"#name":"cross-ref","$":{"refid":"cor1","id":"d1e1808"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea2","type":"email","href":"mailto:ilanthenral.k@vit.ac.in"},"_":"ilanthenral.k@vit.ac.in"}]},{"#name":"author","$":{"id":"au000003","author-id":"S095741742100871X-399a3d82d79c340e44c1447334d0f2b8","orcid":"0000-0001-9832-1475"},"$$":[{"#name":"given-name","_":"Vasantha"},{"#name":"surname","_":"Kandasamy"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea3","type":"email","href":"mailto:vasantha.wb@vit.ac.in"},"_":"vasantha.wb@vit.ac.in"}]},{"#name":"affiliation","$":{"affiliation-id":"S095741742100871X-d33aae103ffa962c6d540232af5d0035","id":"aff1"},"$$":[{"#name":"textfn","_":"School of Computer Science and Engineering, VIT, Vellore, Tamil Nadu, 632014, India"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"School of Computer Science and Engineering, VIT"},{"#name":"city","_":"Vellore"},{"#name":"state","_":"Tamil Nadu"},{"#name":"postal-code","_":"632014"},{"#name":"country","_":"India"}]},{"#name":"source-text","$":{"id":"afs78"},"_":"School of Computer Science and Engineering, VIT, Vellore, Tamil Nadu, India 632014"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"id":"au000003","author-id":"S095741742100871X-399a3d82d79c340e44c1447334d0f2b8","orcid":"0000-0001-9832-1475"},"$$":[{"#name":"given-name","_":"Vasantha"},{"#name":"surname","_":"Kandasamy"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ea3","type":"email","href":"mailto:vasantha.wb@vit.ac.in"},"_":"vasantha.wb@vit.ac.in"}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"article-footnote","$":{"id":"aep-article-footnote-id1"},"$$":[{"#name":"note-para","$":{"view":"all","id":"d1e1767"},"$$":[{"#name":"__text__","_":"The code (and data) in this article has been certified as Reproducible by Code Ocean: ("},{"#name":"inter-ref","$":{"xmlns:xlink":true,"id":"interref1","href":"https://codeocean.com/","type":"simple"},"_":"https://codeocean.com/"},{"#name":"__text__","_":"). More information on the Reproducibility Badge Initiative is available at "},{"#name":"inter-ref","$":{"xmlns:xlink":true,"id":"interref2","href":"https://www.elsevier.com/physical-sciences-and-engineering/computer-science/journals","type":"simple"},"_":"https://www.elsevier.com/physical-sciences-and-engineering/computer-science/journals"},{"#name":"__text__","_":"."}]}]},{"#name":"title","$":{"id":"d1e1775"},"$$":[{"#name":"__text__","_":"Deep Learning for predicting neutralities in Offensive Language Identification Dataset"},{"#name":"inline-figure","$":{"baseline":"0.0"},"$$":[{"#name":"link","$":{"xmlns:xlink":true,"locator":"fx999","href":"pii:S095741742100871X/fx999","role":"http://data.elsevier.com/vocabulary/ElsevierContentTypes/23.4","id":"d1e1779"}}]}]}],"floats":[],"footnotes":[{"#name":"article-footnote","$":{"id":"aep-article-footnote-id1"},"$$":[{"#name":"note-para","$":{"view":"all","id":"d1e1767"},"$$":[{"#name":"__text__","_":"The code (and data) in this article has been certified as Reproducible by Code Ocean: ("},{"#name":"inter-ref","$":{"xmlns:xlink":true,"id":"interref1","href":"https://codeocean.com/","type":"simple"},"_":"https://codeocean.com/"},{"#name":"__text__","_":"). More information on the Reproducibility Badge Initiative is available at "},{"#name":"inter-ref","$":{"xmlns:xlink":true,"id":"interref2","href":"https://www.elsevier.com/physical-sciences-and-engineering/computer-science/journals","type":"simple"},"_":"https://www.elsevier.com/physical-sciences-and-engineering/computer-science/journals"},{"#name":"__text__","_":"."}]}]}],"attachments":[{"attachment-eid":"1-s2.0-S095741742100871X-fx999.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S095741742100871X/fx999/DOWNSAMPLED/image/jpeg/8b92a6b07f0446c0a1d15ebe20b5c9c4/fx999.jpg","file-basename":"fx999","filename":"fx999.jpg","extension":"jpg","filesize":"10364","pixel-height":"22","pixel-width":"19","attachment-type":"IMAGE-DOWNSAMPLED"},{"attachment-eid":"1-s2.0-S095741742100871X-fx999.sml","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S095741742100871X/fx999/THUMBNAIL/image/gif/bf7b0e4554a6317d7422db8371d0bdbf/fx999.sml","file-basename":"fx999","filename":"fx999.sml","extension":"sml","filesize":"8816","pixel-height":"59","pixel-width":"51","attachment-type":"IMAGE-THUMBNAIL"},{"attachment-eid":"1-s2.0-S095741742100871X-fx999_lrg.jpg","ucs-locator":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S095741742100871X/fx999/HIGHRES/image/jpeg/739a46bca272eb7a8ab3b787e60ff6c5/fx999_lrg.jpg","file-basename":"fx999","filename":"fx999_lrg.jpg","extension":"jpg","filesize":"12556","pixel-height":"59","pixel-width":"51","attachment-type":"IMAGE-HIGH-RES"}]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","abstract":{"$$":[{"$$":[{"$":{"id":"d1e1854"},"#name":"section-title","_":"Abstract"},{"$$":[{"$":{"view":"all","id":"d1e1857"},"#name":"simple-para","_":"Deep learning is advancing rapidly; it has aided in solving problems that were thought impossible. Natural language understanding is one such task that has evolved with the advancement of deep learning systems. There have been several sentiment analysis attempts, but they aim to classify it as a single emotion. Human emotion in natural language is generally a complex combination of emotions, which may be indeterminate or neutral at times. Neutrosophy is a branch of philosophy that identifies neutralities and uses membership functions (positive, negative, neutral) to quantify a sample into Single Valued Neutrosophic Set (SVNS) values. Our work aims to combine the power of deep learning with SVNS to represent a sample’s sentiment into membership functions of SVNS. We have worked on the Offensive Language Identification Dataset (OLID). Combining the power of state-of-the-art neural network techniques with neutrosophy allowed us to quantify the sentiments and identify the transition phase between positive and negative ones. We used the transition phase to capture neutral samples, which is beneficial if we want to obtain purely positive/negative samples. We performed experiments using Bi-directional Long Short Term Memory (BiLSTM) with attention, Bidirectional Encoder Representations from Transformers (BERT), A Lite BERT (ALBERT), A Robustly Optimised BERT Approach (RoBERTa), and MPNet. Our SVNS model performed equivalent to state-of-the-art neural network models on the OLID dataset. Here, we propose a novel framework that can integrate with any neural network model and quantify sentiments using SVNS."}],"$":{"view":"all","id":"d1e1856"},"#name":"abstract-sec"}],"$":{"view":"all","id":"d1e1853","class":"author"},"#name":"abstract"},{"$$":[{"$":{"id":"d1e1860"},"#name":"section-title","_":"Highlights"},{"$$":[{"$$":[{"$$":[{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e1869"},"#name":"para","_":"Sentiment analysis model using deep learning, neutrosophy, and transfer learning."}],"$":{"id":"d1e1866"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e1874"},"#name":"para","_":"Analysing tweets as a combination of sentiments identifying neutralities."}],"$":{"id":"d1e1871"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e1879"},"#name":"para","_":"Quantifying tweets into Single Valued Neutrosophic Sets (SVNS) for OLID dataset."}],"$":{"id":"d1e1876"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e1884"},"#name":"para","_":"Experimental analysis using BiLSTM, BERT, RoBERTa, ALBERT, and MPNet."}],"$":{"id":"d1e1881"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"d1e1889"},"#name":"para","_":"Gaussian Mixture Model and k-means clustering algorithm for SVNS calculation."}],"$":{"id":"d1e1886"},"#name":"list-item"}],"$":{"id":"d1e1865"},"#name":"list"}],"$":{"view":"all","id":"d1e1863"},"#name":"simple-para"}],"$":{"view":"all","id":"d1e1862"},"#name":"abstract-sec"}],"$":{"view":"all","id":"d1e1859","class":"author-highlights"},"#name":"abstract"}],"$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"#name":"abstracts"},"pdf":{"urlType":null},"iss-first":"","vol-first":"185","isThirdParty":false,"language":"en","issn-primary-unformatted":"09574174","issn-primary-formatted":"0957-4174"},{"pii":"S0925231221017306","doi":"10.1016/j.neucom.2021.11.053","journalTitle":"Neurocomputing","publicationYear":"2022","publicationDate":"2022-06-01","volumeSupText":"Volume 488","articleNumber":"","pageRange":"598-617","trace-token":"AAAAQCUFiolE--HS9d_fUxr2nfOPTwEvG0JV8Ho766z0jhgTeYRpUZnRe7H1Irg8JbpD136hGEySvqsjCR3z1TfF2lSyQZawEEs33O5KGZeOFnIvL8xcKA","authors":{"content":[{"#name":"author-group","$":{"id":"ag005"},"$$":[{"#name":"author","$":{"id":"au005","biographyid":"bg005","author-id":"S0925231221017306-0c71640e4f65d367c32c935ede6c9a62"},"$$":[{"#name":"given-name","_":"Ayan"},{"#name":"surname","_":"Sengupta"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-original-draft"},"_":"Writing – original draft"},{"#name":"cross-ref","$":{"id":"ar005","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"id":"ar010","refid":"af010"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"b"}]},{"#name":"cross-ref","$":{"id":"ar015","refid":"cor1"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"type":"email","href":"mailto:ayans@iiitd.ac.in","id":"em005"},"_":"ayans@iiitd.ac.in"}]},{"#name":"author","$":{"id":"au010","biographyid":"bg010","author-id":"S0925231221017306-83e6385ec5d7f7e63c7cc400fc80dd9c"},"$$":[{"#name":"given-name","_":"Sourabh Kumar"},{"#name":"surname","_":"Bhattacharjee"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/software"},"_":"Software"},{"#name":"cross-ref","$":{"id":"ar020","refid":"af100"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"c"}]},{"#name":"cross-ref","$":{"refid":"fn100","id":"ce.cross-ref_pw3_th5_trb"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"1"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"em010","type":"email","href":"mailto:skb5275@nyu.edu"},"_":"skb5275@nyu.edu"}]},{"#name":"author","$":{"id":"au015","biographyid":"bg015","author-id":"S0925231221017306-462a29a3752834a692b370ad9e639631"},"$$":[{"#name":"given-name","_":"Md. Shad"},{"#name":"surname","_":"Akhtar"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar025","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"em015","type":"email","href":"mailto:shad.akhtar@iiitd.ac.in"},"_":"shad.akhtar@iiitd.ac.in"}]},{"#name":"author","$":{"id":"au020","biographyid":"bg020","author-id":"S0925231221017306-3be9059769b0acc800fc44c43e669946"},"$$":[{"#name":"given-name","_":"Tanmoy"},{"#name":"surname","_":"Chakraborty"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar030","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"fn200","id":"ce.cross-ref_gy2_5h5_trb"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"2"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"em020","type":"email","href":"mailto:tanmoy@iiitd.ac.in"},"_":"tanmoy@iiitd.ac.in"}]},{"#name":"affiliation","$":{"id":"af005","affiliation-id":"S0925231221017306-ad3787767b247a64fc67e83b1c9f2697"},"$$":[{"#name":"label","_":"a"},{"#name":"textfn","_":"Dept. of CSE, IIIT-Delhi, India"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Dept. of CSE"},{"#name":"organization","_":"IIIT-Delhi"},{"#name":"country","_":"India"}]},{"#name":"source-text","$":{"id":"str005"},"_":"Dept. of CSE, IIIT-Delhi, India"}]},{"#name":"affiliation","$":{"id":"af010","affiliation-id":"S0925231221017306-5023e64c3880e5e135cf87895ae8d502"},"$$":[{"#name":"label","_":"b"},{"#name":"textfn","_":"Optum Global Solutions (India) Pvt. Ltd., India"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Optum Global Solutions (India) Pvt. Ltd."},{"#name":"country","_":"India"}]},{"#name":"source-text","$":{"id":"str010"},"_":"Optum Global Solutions (India) Pvt. Ltd."}]},{"#name":"affiliation","$":{"id":"af100","affiliation-id":"S0925231221017306-0334ac9cee8d4c91791c9579df09152c"},"$$":[{"#name":"label","_":"c"},{"#name":"textfn","_":"New York University, USA"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"New York University"},{"#name":"country","_":"USA"}]},{"#name":"source-text","$":{"id":"srctPC_aff0eOuZ4aI1q"},"_":"New York University, USA"}]},{"#name":"correspondence","$":{"id":"cor1"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","_":"Corresponding author at: Dept. of CSE, IIIT-Delhi, India."},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Dept. of CSE"},{"#name":"organization","_":"IIIT-Delhi"},{"#name":"country","_":"India"}]}]},{"#name":"footnote","$":{"id":"fn100"},"$$":[{"#name":"label","_":"1"},{"#name":"note-para","$":{"id":"ce.note-para_mtt_zh5_trb","view":"all"},"_":"During the work S. Bhattacharjee was an employee of Optum Global Solutions (India) Pvt. Ltd."}]},{"#name":"footnote","$":{"id":"fn200"},"$$":[{"#name":"label","_":"2"},{"#name":"note-para","$":{"id":"ce.note-para_qc4_zh5_trb","view":"all"},"_":"T. Chakraborty would like to acknowledge the support of the Ramanujan Fellowship, Infosys Centre for AI at IIIT-Delhi, and ihub-Anubhuti-iiitd Foundation set up under the NM-ICPS scheme of the Department of Science and Technology, India."}]}]}],"floats":[],"footnotes":[{"#name":"footnote","$":{"id":"fn100"},"$$":[{"#name":"label","_":"1"},{"#name":"note-para","$":{"id":"ce.note-para_mtt_zh5_trb","view":"all"},"_":"During the work S. Bhattacharjee was an employee of Optum Global Solutions (India) Pvt. Ltd."}]},{"#name":"footnote","$":{"id":"fn200"},"$$":[{"#name":"label","_":"2"},{"#name":"note-para","$":{"id":"ce.note-para_qc4_zh5_trb","view":"all"},"_":"T. Chakraborty would like to acknowledge the support of the Ramanujan Fellowship, Infosys Centre for AI at IIIT-Delhi, and ihub-Anubhuti-iiitd Foundation set up under the NM-ICPS scheme of the Department of Science and Technology, India."}]}],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"id":"au020","biographyid":"bg020","author-id":"S0925231221017306-3be9059769b0acc800fc44c43e669946"},"$$":[{"#name":"given-name","_":"Tanmoy"},{"#name":"surname","_":"Chakraborty"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://credit.niso.org/contributor-roles/writing-review-editing"},"_":"Writing – review & editing"},{"#name":"cross-ref","$":{"id":"ar030","refid":"af005"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"a"}]},{"#name":"cross-ref","$":{"refid":"fn200","id":"ce.cross-ref_gy2_5h5_trb"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"2"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"em020","type":"email","href":"mailto:tanmoy@iiitd.ac.in"},"_":"tanmoy@iiitd.ac.in"}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"title","$":{"id":"tm005"},"$$":[{"#name":"italic","_":"Does aggression lead to hate?"},{"#name":"__text__","_":" Detecting and reasoning offensive traits in hinglish code-mixed texts"}]}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","abstract":{"$$":[{"$$":[{"$$":[{"#name":"attachment-eid","_":"1-s2.0-S0925231221017306-si10.svg"},{"#name":"ucs-locator","_":"https://s3-eu-west-1.amazonaws.com/prod-ucs-content-store-eu-west/content/pii:S0925231221017306/STRIPIN/image/svg+xml/6ec6f25fa4c5bc0cef53548e60656b01/si10.svg"},{"#name":"file-basename","_":"si10"},{"#name":"abstract-attachment","_":"true"},{"#name":"filename","_":"si10.svg"},{"#name":"extension","_":"svg"},{"#name":"filesize","_":"5386"},{"#name":"attachment-type","_":"ALTIMG"}],"$":{"xmlns:xocs":true},"#name":"attachment"}],"#name":"attachments"},{"$$":[{"$":{"id":"st135"},"#name":"section-title","_":"Abstract"},{"$$":[{"$$":[{"#name":"__text__","_":"Aggression is a prominent trait of human beings that can affect social harmony in a negative way. The hate mongers misuse the freedom of speech in social media platforms to flood with their venomous comments in many forms. Identifying different traits of online offense is thus inevitable and the need of the hour. Existing studies usually handle one or two offense traits at a time, mainly due to the lack of a combined annotated dataset and a scientific study that provides insights into the relationship among the traits. In this paper, we study the relationship among five offense traits – "},{"#name":"italic","_":"aggression"},{"#name":"__text__","_":", "},{"#name":"italic","_":"hate"},{"#name":"__text__","_":", "},{"#name":"italic","_":"sarcasm"},{"#name":"__text__","_":", "},{"#name":"italic","_":"humor"},{"#name":"__text__","_":", and "},{"#name":"italic","_":"stance"},{"#name":"__text__","_":" in Hinglish (Hindi-English) social media code-mixed texts. We employ various state-of-the-art deep learning systems at different morphological granularities for the classification across five offense traits. Our evaluation of the unified framework suggests "},{"$$":[{"$$":[{"#name":"mo","_":"∼"},{"#name":"mn","_":"90"},{"#name":"mo","_":"%"}],"#name":"mrow"}],"$":{"xmlns:mml":true,"altimg":"si10.svg"},"#name":"math"},{"#name":"__text__","_":" performance across all major traits. Furthermore, we propose a novel notion of "},{"#name":"italic","_":"causal importance score"},{"#name":"__text__","_":" to quantify the effect of different abusive keywords and the overall context on the offensiveness of the texts."}],"$":{"view":"all","id":"sp005"},"#name":"simple-para"}],"$":{"view":"all","id":"as005"},"#name":"abstract-sec"}],"$":{"view":"all","id":"ab005","lang":"en","class":"author"},"#name":"abstract"}],"$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"#name":"abstracts"},"pdf":{"urlType":null},"iss-first":"","vol-first":"488","isThirdParty":false,"language":"en","issn-primary-unformatted":"09252312","issn-primary-formatted":"0925-2312"},{"pii":"S0306457320308554","doi":"10.1016/j.ipm.2020.102360","journalTitle":"Information Processing & Management","publicationYear":"2020","publicationDate":"2020-11-01","volumeSupText":"Volume 57, Issue 6","articleNumber":"102360","pageRange":"102360","trace-token":"AAAAQCUFiolE--HS9d_fUxr2nfNjkB7cJ8QQsg3AElJSeX5kt-XVdSp8vz4-KflIjvpXNVFd-5cCxDfaSSNS9lOkVIyhLF63BX0aoeeZsqWD3cGO492-2g","authors":{"content":[{"#name":"author-group","$":{"id":"aut0001"},"$$":[{"#name":"author","$":{"id":"au0001","author-id":"S0306457320308554-49cd939d71e1746292006da9d700e81d"},"$$":[{"#name":"given-name","_":"Endang Wahyu"},{"#name":"surname","_":"Pamungkas"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Data_curation"},"_":"Data curation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Formal_analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Resources"},"_":"Resources"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Software"},"_":"Software"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Visualization"},"_":"Visualization"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_original_draft"},"_":"Writing - original draft"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"cross-ref","$":{"id":"crf0021","refid":"cor0001"},"$$":[{"#name":"sup","$":{"loc":"post"},"_":"⁎"}]},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ead0001","type":"email","href":"mailto:pamungka@di.unito.it"},"_":"pamungka@di.unito.it"}]},{"#name":"author","$":{"id":"au0002","author-id":"S0306457320308554-df72cf9b4ce11cf8d2ae3903d73791d6"},"$$":[{"#name":"given-name","_":"Valerio"},{"#name":"surname","_":"Basile"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Formal_analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_original_draft"},"_":"Writing - original draft"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ead0002","type":"email","href":"mailto:valerio.basile@unito.it"},"_":"valerio.basile@unito.it"}]},{"#name":"author","$":{"id":"au0003","author-id":"S0306457320308554-b7af9d506e952c2686320eda32150f5a"},"$$":[{"#name":"given-name","_":"Viviana"},{"#name":"surname","_":"Patti"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Formal_analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Funding_acquisition"},"_":"Funding acquisition"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Project_administration"},"_":"Project administration"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_original_draft"},"_":"Writing - original draft"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ead0003","type":"email","href":"mailto:viviana.patti@unito.it"},"_":"viviana.patti@unito.it"}]},{"#name":"affiliation","$":{"id":"aff0001","affiliation-id":"S0306457320308554-df53c9a1e819261e235246ee3a2a578f"},"$$":[{"#name":"textfn","$":{"id":"textfn0001"},"_":"Department of Computer Science, University of Turin, Italy"},{"#name":"affiliation","$":{"xmlns:sa":true},"$$":[{"#name":"organization","_":"Department of Computer Science"},{"#name":"organization","_":"University of Turin"},{"#name":"country","_":"Italy"}]},{"#name":"source-text","$":{"id":"st0001"},"_":"Department of Computer Science, University of Turin, Italy"}]},{"#name":"correspondence","$":{"id":"cor0001"},"$$":[{"#name":"label","_":"⁎"},{"#name":"text","$":{"id":"cor1"},"_":"Corresponding author."}]}]}],"floats":[],"footnotes":[],"attachments":[]},"lastAuthor":{"content":[{"#name":"author","$":{"id":"au0003","author-id":"S0306457320308554-b7af9d506e952c2686320eda32150f5a"},"$$":[{"#name":"given-name","_":"Viviana"},{"#name":"surname","_":"Patti"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Conceptualization"},"_":"Conceptualization"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Formal_analysis"},"_":"Formal analysis"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Funding_acquisition"},"_":"Funding acquisition"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Investigation"},"_":"Investigation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Methodology"},"_":"Methodology"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Project_administration"},"_":"Project administration"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Supervision"},"_":"Supervision"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Validation"},"_":"Validation"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_original_draft"},"_":"Writing - original draft"},{"#name":"contributor-role","$":{"role":"http://dictionary.casrai.org/Contributor_Roles/Writing_%E2%80%93_review_%26_editing"},"_":"Writing - review & editing"},{"#name":"e-address","$":{"xmlns:xlink":true,"id":"ead0003","type":"email","href":"mailto:viviana.patti@unito.it"},"_":"viviana.patti@unito.it"}]}],"floats":[],"footnotes":[],"attachments":[]},"title":{"content":[{"#name":"title","$":{"id":"ct0001"},"_":"Misogyny Detection in Twitter: a Multilingual and Cross-Domain Study"}],"floats":[],"footnotes":[],"attachments":[]},"openArchive":false,"openAccess":false,"document-subtype":"fla","content-family":"serial","contentType":"JL","entitlementType":"","abstract":{"$$":[{"$$":[{"$":{"id":"sctt0001"},"#name":"section-title","_":"Highlights"},{"$$":[{"$$":[{"$$":[{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0001"},"#name":"para","_":"We conduct a broad and in-depth study on online misogyny, a relevant and timely task given that more and more episodes of hate speech and online harassment happen in social media."}],"$":{"id":"lstitem0001"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0002"},"#name":"para","_":"An extensive review of the state of the art in misogyny detection is presented."}],"$":{"id":"lstitem0002"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0003"},"#name":"para","_":"A state-of-the-art model to detect misogyny in social media is developed, and evaluated on three different languages, English, Italian, and Spanish."}],"$":{"id":"lstitem0003"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0004"},"#name":"para","_":"We investigate the most predictive linguistic features to distinguish misogynistic content from not-misogynistic content."}],"$":{"id":"lstitem0004"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0005"},"#name":"para","_":"Relationships between misogyny and other abusive language phenomena are postulated, and empirically investigated with cross-dataset experiments."}],"$":{"id":"lstitem0005"},"#name":"list-item"},{"$$":[{"#name":"label","_":"•"},{"$":{"view":"all","id":"p0006"},"#name":"para","_":"The feasibility of detecting misogyny in a multilingual environment is explored."}],"$":{"id":"lstitem0006"},"#name":"list-item"}],"$":{"id":"lst0008"},"#name":"list"}],"$":{"view":"all","id":"sp0001"},"#name":"simple-para"}],"$":{"view":"all","id":"abssec0001"},"#name":"abstract-sec"}],"$":{"view":"all","id":"absh001","class":"author-highlights"},"#name":"abstract"},{"$$":[{"$":{"id":"sctt0002"},"#name":"section-title","_":"Abstract"},{"$$":[{"$":{"view":"all","id":"sp0002"},"#name":"simple-para","_":"The freedom of expression given by social media has a dark side: the growing proliferation of abusive contents on these platforms. Misogynistic speech is a kind of abusive language, which can be simplified as hate speech targeting women, and it is becoming a more and more relevant issue in recent years. AMI IberEval 2018 and AMI EVALITA 2018 were two shared tasks which mainly focused on tackling the problem of misogyny in Twitter, in three different languages, namely English, Italian, and Spanish. In this paper, we present an in-depth study on the phenomena of misogyny in those three languages, by focusing on three main objectives. Firstly, we investigate the most important features to detect misogyny and the issues which contribute to the difficulty of misogyny detection, by proposing a novel system and conducting a broad evaluation on this task. Secondly, we study the relationship between misogyny and other abusive language phenomena, by conducting a series of cross-domain classification experiments. Finally, we explore the feasibility of detecting misogyny in a multilingual environment, by carrying out cross-lingual classification experiments. Our system succeeded to outperform all state of the art systems in all benchmark AMI datasets both subtask A and subtask B. Moreover, intriguing insights emerged from error analysis, in particular about the interaction between different but related abusive phenomena. Based on our cross-domain experiment, we conclude that misogyny is quite a specific kind of abusive language, while we experimentally found that it is different from sexism. Lastly, our cross-lingual experiments show promising results. Our proposed joint-learning architecture obtained a robust performance across languages, worth to be explored in further investigation."}],"$":{"view":"all","id":"abssec0002"},"#name":"abstract-sec"}],"$":{"view":"all","id":"abs0001","class":"author"},"#name":"abstract"}],"$":{"xmlns:ce":true,"xmlns:dm":true,"xmlns:sb":true},"#name":"abstracts"},"pdf":{"urlType":null},"iss-first":"6","vol-first":"57","isThirdParty":false,"language":"en","issn-primary-unformatted":"03064573","issn-primary-formatted":"0306-4573"}]},"references":{"content":[{"#name":"bibliography","$":{"xmlns:ce":true,"xmlns:aep":true,"xmlns:mml":true,"xmlns:xs":true,"xmlns:xlink":true,"xmlns:xocs":true,"xmlns:tb":true,"xmlns:xsi":true,"xmlns:cals":true,"xmlns:sb":true,"xmlns:sa":true,"xmlns:ja":true,"xmlns":true,"id":"bib1","view":"all"},"$$":[{"#name":"section-title","$":{"id":"d1e4137"},"_":"References"},{"#name":"bibliography-sec","$":{"view":"all","id":"d1e4139"},"$$":[{"#name":"bib-reference","$":{"id":"b1"},"$$":[{"#name":"label","_":"Baltrušaitis et al., 2018"},{"#name":"reference","$":{"id":"sb1","refId":"1"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Baltrušaitis"},{"#name":"given-name","_":"Tadas"}]},{"#name":"author","$$":[{"#name":"surname","_":"Ahuja"},{"#name":"given-name","_":"Chaitanya"}]},{"#name":"author","$$":[{"#name":"surname","_":"Morency"},{"#name":"given-name","_":"Louis -Philippe"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multimodal machine learning: A survey and taxonomy"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Transactions on Pattern Analysis and Machine Intelligence"}]},{"#name":"volume-nr","_":"41"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2018"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"423"},{"#name":"last-page","_":"443"}]}]}]},{"#name":"source-text","$":{"id":"afs43"},"_":"T. Baltrušaitis, C. Ahuja, L.-P. Morency, Multimodal machine learning: A survey and taxonomy, IEEE transactions on pattern analysis and machine intelligence 41 (2) (2018) 423–443."}]},{"#name":"bib-reference","$":{"id":"b2"},"$$":[{"#name":"label","_":"Börzsei, 2013"},{"#name":"reference","$":{"id":"sb2","refId":"2"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Börzsei"},{"#name":"given-name","_":"Linda K."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Makes a meme instead"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"The selected works of Linda Börzsei"}]},{"#name":"date","_":"2013"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"28"}]}]}]},{"#name":"source-text","$":{"id":"afs39"},"_":"L. K. Börzsei, Makes a meme instead, The Selected Works of Linda Börzsei (2013) 1–28."}]},{"#name":"bib-reference","$":{"id":"b3"},"$$":[{"#name":"label","_":"Castaño Díaz, 2013"},{"#name":"reference","$":{"id":"sb3","refId":"3"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Castaño Díaz"},{"#name":"given-name","_":"Carlos Mauricio"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Defining and characterizing the concept of internet meme"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Ces Psicología"}]},{"#name":"volume-nr","_":"6"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2013"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"82"},{"#name":"last-page","_":"104"}]}]}]},{"#name":"source-text","$":{"id":"afs38"},"_":"C. M. Castaño Díaz, Defining and characterizing the concept of internet meme, Ces Psicología 6 (2) (2013) 82–104."}]},{"#name":"bib-reference","$":{"id":"b4"},"$$":[{"#name":"label","_":"Chauhan et al., 2020"},{"#name":"other-ref","$":{"id":"sb4","refId":"4"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Chauhan, Dushyant Singh, Dhanush, SR, Ekbal, Asif, & Bhattacharyya, Pushpak (2020). All-in-One: A deep attentive multi-task learning framework for humour, sarcasm, offensive, motivation, and sentiment on memes. In "},{"#name":"italic","_":"Proceedings of the 1st conference of the Asia-Pacific chapter of the association for computational linguistics and the 10th international joint conference on natural language processing"},{"#name":"__text__","_":" (pp. 281–290)."}]}]}]},{"#name":"bib-reference","$":{"id":"b5"},"$$":[{"#name":"label","_":"Chen et al., 2018"},{"#name":"other-ref","$":{"id":"sb5","refId":"5"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Chen, Dapeng, Li, Hongsheng, Liu, Xihui, Shen, Yantao, Shao, Jing, & Yuan, Zejian, et al. (2018). Improving deep visual representation for person re-identification by global and local image-language association. In "},{"#name":"italic","_":"Proceedings of the European conference on computer vision"},{"#name":"__text__","_":" (pp. 54–70)."}]}]}]},{"#name":"bib-reference","$":{"id":"b6"},"$$":[{"#name":"label","_":"Cheng et al., 2017"},{"#name":"other-ref","$":{"id":"sb6","refId":"6"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Cheng, Justin, Bernstein, Michael, Danescu-Niculescu-Mizil, Cristian, & Leskovec, Jure (2017). Anyone can become a troll: Causes of trolling behavior in online discussions. In "},{"#name":"italic","_":"Proceedings of the 2017 ACM conference on computer supported cooperative work and social computing"},{"#name":"__text__","_":" (pp. 1217–1230)."}]}]}]},{"#name":"bib-reference","$":{"id":"b7"},"$$":[{"#name":"label","_":"Choi et al., 2020"},{"#name":"reference","$":{"id":"sb7","refId":"7"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Choi"},{"#name":"given-name","_":"Daejin"}]},{"#name":"author","$$":[{"#name":"surname","_":"Chun"},{"#name":"given-name","_":"Selin"}]},{"#name":"author","$$":[{"#name":"surname","_":"Oh"},{"#name":"given-name","_":"Hyunchul"}]},{"#name":"author","$$":[{"#name":"surname","_":"Han"},{"#name":"given-name","_":"Jinyoung"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Rumor propagation is amplified by echo chambers in social media"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Scientific Reports"}]},{"#name":"volume-nr","_":"10"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2020"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"10"}]}]}]},{"#name":"source-text","$":{"id":"afs24"},"_":"D. Choi, S. Chun, H. Oh, J. Han, et al., Rumor propagation is amplified by echo chambers in social media, Scientific Reports 10 (1) (2020) 1–10."}]},{"#name":"bib-reference","$":{"id":"b8"},"$$":[{"#name":"label","_":"Davidson et al., 2017"},{"#name":"other-ref","$":{"id":"sb8","refId":"8"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Davidson, Thomas, Warmsley, Dana, Macy, Michael, & Weber, Ingmar (2017). Automated hate speech detection and the problem of offensive language. In "},{"#name":"italic","_":"Eleventh international aaai conference on web and social media"},{"#name":"__text__","_":"."}]}]}]},{"#name":"bib-reference","$":{"id":"b9"},"$$":[{"#name":"label","_":"Devlin et al., 2018"},{"#name":"reference","$":{"id":"sb9","refId":"9"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Devlin"},{"#name":"given-name","_":"Jacob"}]},{"#name":"author","$$":[{"#name":"surname","_":"Chang"},{"#name":"given-name","_":"Ming -Wei"}]},{"#name":"author","$$":[{"#name":"surname","_":"Lee"},{"#name":"given-name","_":"Kenton"}]},{"#name":"author","$$":[{"#name":"surname","_":"Toutanova"},{"#name":"given-name","_":"Kristina"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Bert: Pre-training of deep bidirectional transformers for language understanding"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2018"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref8","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1810.04805","type":"simple"},"_":"arXiv:1810.04805"}]}]},{"#name":"source-text","$":{"id":"afs12"},"_":"J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805."}]},{"#name":"bib-reference","$":{"id":"b10"},"$$":[{"#name":"label","_":"Englander et al., 2017"},{"#name":"reference","$":{"id":"sb10","refId":"10"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Englander"},{"#name":"given-name","_":"Elizabeth"}]},{"#name":"author","$$":[{"#name":"surname","_":"Donnerstein"},{"#name":"given-name","_":"Edward"}]},{"#name":"author","$$":[{"#name":"surname","_":"Kowalski"},{"#name":"given-name","_":"Robin"}]},{"#name":"author","$$":[{"#name":"surname","_":"Lin"},{"#name":"given-name","_":"Carolyn A"}]},{"#name":"author","$$":[{"#name":"surname","_":"Parti"},{"#name":"given-name","_":"Katalin"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Defining cyberbullying"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Pediatrics"}]},{"#name":"volume-nr","_":"140"}]},{"#name":"issue-nr","_":"Supplement 2"},{"#name":"date","_":"2017"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"S148"},{"#name":"last-page","_":"S151"}]}]}]},{"#name":"source-text","$":{"id":"afs20"},"_":"E. Englander, E. Donnerstein, R. Kowalski, C. A. Lin, K. Parti, Defining cyberbullying, Pediatrics 140 (Supplement 2) (2017) S148–S151."}]},{"#name":"bib-reference","$":{"id":"b11"},"$$":[{"#name":"label","_":"Fortuna and Nunes, 2018"},{"#name":"reference","$":{"id":"sb11","refId":"11"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Fortuna"},{"#name":"given-name","_":"Paula"}]},{"#name":"author","$$":[{"#name":"surname","_":"Nunes"},{"#name":"given-name","_":"Sérgio"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A survey on automatic detection of hate speech in text"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"ACM Computing Surveys"}]},{"#name":"volume-nr","_":"51"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"2018"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"30"}]}]}]},{"#name":"source-text","$":{"id":"afs33"},"_":"P. Fortuna, S. Nunes, A survey on automatic detection of hate speech in text, ACM Computing Surveys (CSUR) 51 (4) (2018) 1–30."}]},{"#name":"bib-reference","$":{"id":"b12"},"$$":[{"#name":"label","_":"Graves and Schmidhuber, 2005"},{"#name":"reference","$":{"id":"sb12","refId":"12"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Graves"},{"#name":"given-name","_":"Alex"}]},{"#name":"author","$$":[{"#name":"surname","_":"Schmidhuber"},{"#name":"given-name","_":"Jürgen"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Framewise phoneme classification with bidirectional LSTM and other neural network architectures"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Neural Networks"}]},{"#name":"volume-nr","_":"18"}]},{"#name":"issue-nr","_":"5–6"},{"#name":"date","_":"2005"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"602"},{"#name":"last-page","_":"610"}]}]}]},{"#name":"source-text","$":{"id":"afs64"},"_":"A. Graves, J. Schmidhuber, Framewise phoneme classification with bidirectional lstm and other neural network architectures, Neural networks 18 (5-6) (2005) 602–610."}]},{"#name":"bib-reference","$":{"id":"b13"},"$$":[{"#name":"label","_":"He et al., 2016"},{"#name":"other-ref","$":{"id":"sb13","refId":"13"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, & Sun, Jian (2016). Deep residual learning for image recognition. In "},{"#name":"italic","_":"Proceedings of the IEEE conference on computer vision and pattern recognition"},{"#name":"__text__","_":" (pp. 770–778)."}]}]}]},{"#name":"bib-reference","$":{"id":"b14"},"$$":[{"#name":"label","_":"Hofstadter, 2001"},{"#name":"reference","$":{"id":"sb14","refId":"14"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hofstadter"},{"#name":"given-name","_":"Douglas R."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Analogy as the core of cognition"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"The analogical mind: Perspectives from cognitive science"}]},{"#name":"date","_":"2001"},{"#name":"publisher","$$":[{"#name":"name","_":"Cambridge, MA"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"499"},{"#name":"last-page","_":"538"}]}]}]},{"#name":"source-text","$":{"id":"afs59"},"_":"D. R. Hofstadter, Analogy as the core of cognition, The analogical mind: Perspectives from cognitive science (2001) 499–538."}]},{"#name":"bib-reference","$":{"id":"b15"},"$$":[{"#name":"label","_":"Hossain et al., 2019"},{"#name":"reference","$":{"id":"sb15","refId":"15"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Hossain"},{"#name":"given-name","_":"MD Zakir"}]},{"#name":"author","$$":[{"#name":"surname","_":"Sohel"},{"#name":"given-name","_":"Ferdous"}]},{"#name":"author","$$":[{"#name":"surname","_":"Shiratuddin"},{"#name":"given-name","_":"Mohd Fairuz"}]},{"#name":"author","$$":[{"#name":"surname","_":"Laga"},{"#name":"given-name","_":"Hamid"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A comprehensive survey of deep learning for image captioning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"ACM Computing Surveys"}]},{"#name":"volume-nr","_":"51"}]},{"#name":"issue-nr","_":"6"},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"36"}]}]}]},{"#name":"source-text","$":{"id":"afs5"},"_":"M. Z. Hossain, F. Sohel, M. F. Shiratuddin, H. Laga, A comprehensive survey of deep learning for image captioning, ACM Computing Surveys (CSUR) 51 (6) (2019) 1–36."}]},{"#name":"bib-reference","$":{"id":"b16"},"$$":[{"#name":"label","_":"Islam et al., 2017"},{"#name":"reference","$":{"id":"sb16","refId":"16"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Islam"},{"#name":"given-name","_":"Noman"}]},{"#name":"author","$$":[{"#name":"surname","_":"Islam"},{"#name":"given-name","_":"Zeeshan"}]},{"#name":"author","$$":[{"#name":"surname","_":"Noor"},{"#name":"given-name","_":"Nazia"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A survey on optical character recognition system"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2017"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref9","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1710.05703","type":"simple"},"_":"arXiv:1710.05703"}]}]},{"#name":"source-text","$":{"id":"afs15"},"_":"N. Islam, Z. Islam, N. Noor, A survey on optical character recognition system, arXiv preprint arXiv:1710.05703."}]},{"#name":"bib-reference","$":{"id":"b17"},"$$":[{"#name":"label","_":"Jakubowicz et al., 2017"},{"#name":"reference","$":{"id":"sb17","refId":"17"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Jakubowicz"},{"#name":"given-name","_":"Andrew"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Alt_right white lite: Trolling, hate speech and cyber racism on social media"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Cosmopolitan Civil Societies: An Interdisciplinary Journal"}]},{"#name":"volume-nr","_":"9"}]},{"#name":"issue-nr","_":"3"},{"#name":"date","_":"2017"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"41"}]}]}]},{"#name":"source-text","$":{"id":"afs2"},"_":"A. Jakubowicz, et al., Alt_right white lite: trolling, hate speech and cyber racism on social media, Cosmopolitan Civil Societies: An Interdisciplinary Journal 9 (3) (2017) 41."}]},{"#name":"bib-reference","$":{"id":"b18"},"$$":[{"#name":"label","_":"Jhaver et al., 2018"},{"#name":"reference","$":{"id":"sb18","refId":"18"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Jhaver"},{"#name":"given-name","_":"Shagun"}]},{"#name":"author","$$":[{"#name":"surname","_":"Ghoshal"},{"#name":"given-name","_":"Sucheta"}]},{"#name":"author","$$":[{"#name":"surname","_":"Bruckman"},{"#name":"given-name","_":"Amy"}]},{"#name":"author","$$":[{"#name":"surname","_":"Gilbert"},{"#name":"given-name","_":"Eric"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Online harassment and content moderation: The case of blocklists"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"ACM Transactions on Computer-Human Interaction"}]},{"#name":"volume-nr","_":"25"}]},{"#name":"issue-nr","_":"2"},{"#name":"date","_":"2018"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"33"}]}]}]},{"#name":"source-text","$":{"id":"afs3"},"_":"S. Jhaver, S. Ghoshal, A. Bruckman, E. Gilbert, Online harassment and content moderation: The case of blocklists, ACM Transactions on Computer-Human Interaction (TOCHI) 25 (2) (2018) 1–33."}]},{"#name":"bib-reference","$":{"id":"b19"},"$$":[{"#name":"label","_":"Kiela et al., 2020"},{"#name":"reference","$":{"id":"sb19","refId":"19"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Kiela"},{"#name":"given-name","_":"Douwe"}]},{"#name":"author","$$":[{"#name":"surname","_":"Firooz"},{"#name":"given-name","_":"Hamed"}]},{"#name":"author","$$":[{"#name":"surname","_":"Mohan"},{"#name":"given-name","_":"Aravind"}]},{"#name":"author","$$":[{"#name":"surname","_":"Goswami"},{"#name":"given-name","_":"Vedanuj"}]},{"#name":"author","$$":[{"#name":"surname","_":"Singh"},{"#name":"given-name","_":"Amanpreet"}]},{"#name":"author","$$":[{"#name":"surname","_":"Ringshia"},{"#name":"given-name","_":"Pratik"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"The hateful memes challenge: Detecting hate speech in multimodal memes"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2020"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref10","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:2005.04790","type":"simple"},"_":"arXiv:2005.04790"}]}]},{"#name":"source-text","$":{"id":"afs16"},"_":"D. Kiela, H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, D. Testuggine, The hateful memes challenge: Detecting hate speech in multimodal memes, arXiv preprint arXiv:2005.04790."}]},{"#name":"bib-reference","$":{"id":"b20"},"$$":[{"#name":"label","_":"Kou et al., 2020"},{"#name":"reference","$":{"id":"sb20","refId":"20"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Kou"},{"#name":"given-name","_":"Ziyi"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Daniel Yue"}]},{"#name":"author","$$":[{"#name":"surname","_":"Shang"},{"#name":"given-name","_":"Lanyu"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Dong"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"ExFaux: A weakly supervised approach to explainable fauxtography detection"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"2020 IEEE international conference on big data"}]},{"#name":"date","_":"2020"},{"#name":"publisher","$$":[{"#name":"name","_":"IEEE"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"631"},{"#name":"last-page","_":"636"}]}]}]},{"#name":"source-text","$":{"id":"afs49"},"_":"Z. Kou, D. Y. Zhang, L. Shang, D. Wang, Exfaux: A weakly supervised approach to explainable fauxtography detection, in: 2020 IEEE International Conference on Big Data (Big Data), IEEE, 2020, pp. 631–636."}]},{"#name":"bib-reference","$":{"id":"b21"},"$$":[{"#name":"label","_":"Kumar and Carley, 2019"},{"#name":"other-ref","$":{"id":"sb21","refId":"21"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Kumar, Sumeet, & Carley, Kathleen M. (2019). Tree LSTMs with convolution units to predict stance and rumor vera in social media conversations. In "},{"#name":"italic","_":"Proceedings of the 57th annual meeting of the association for computational linguistics"},{"#name":"__text__","_":" (pp. 5047–5058)."}]}]}]},{"#name":"bib-reference","$":{"id":"b22"},"$$":[{"#name":"label","_":"Li, Yatskar, Yin, Hsieh, and Chang, 2019"},{"#name":"reference","$":{"id":"sb22","refId":"22"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Liunian Harold"}]},{"#name":"author","$$":[{"#name":"surname","_":"Yatskar"},{"#name":"given-name","_":"Mark"}]},{"#name":"author","$$":[{"#name":"surname","_":"Yin"},{"#name":"given-name","_":"Da"}]},{"#name":"author","$$":[{"#name":"surname","_":"Hsieh"},{"#name":"given-name","_":"Cho -Jui"}]},{"#name":"author","$$":[{"#name":"surname","_":"Chang"},{"#name":"given-name","_":"Kai -Wei"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Visualbert: A simple and performant baseline for vision and language"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2019"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"ArXiv Preprint "},{"#name":"inter-ref","$":{"id":"interref11","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1908.03557","type":"simple"},"_":"arXiv:1908.03557"}]}]},{"#name":"source-text","$":{"id":"afs72"},"_":"L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, K.-W. Chang, Visualbert: A simple and performant baseline for vision and language, arXiv preprint arXiv:1908.03557."}]},{"#name":"bib-reference","$":{"id":"b23"},"$$":[{"#name":"label","_":"Li, Zhang, Li, Li, and Fu, 2019"},{"#name":"other-ref","$":{"id":"sb23","refId":"23"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Li, Kunpeng, Zhang, Yulun, Li, Kai, Li, Yuanyuan, & Fu, Yun (2019). Visual semantic reasoning for image-text matching. In "},{"#name":"italic","_":"Proceedings of the IEEE international conference on computer vision"},{"#name":"__text__","_":" (pp. 4654–4662)."}]}]}]},{"#name":"bib-reference","$":{"id":"b24"},"$$":[{"#name":"label","_":"Lin et al., 2014"},{"#name":"reference","$":{"id":"sb24","refId":"24"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Lin"},{"#name":"given-name","_":"Tsung -Yi"}]},{"#name":"author","$$":[{"#name":"surname","_":"Maire"},{"#name":"given-name","_":"Michael"}]},{"#name":"author","$$":[{"#name":"surname","_":"Belongie"},{"#name":"given-name","_":"Serge"}]},{"#name":"author","$$":[{"#name":"surname","_":"Hays"},{"#name":"given-name","_":"James"}]},{"#name":"author","$$":[{"#name":"surname","_":"Perona"},{"#name":"given-name","_":"Pietro"}]},{"#name":"author","$$":[{"#name":"surname","_":"Ramanan"},{"#name":"given-name","_":"Deva"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Microsoft coco: Common objects in context"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"European conference on computer vision"}]},{"#name":"date","_":"2014"},{"#name":"publisher","$$":[{"#name":"name","_":"Springer"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"740"},{"#name":"last-page","_":"755"}]}]}]},{"#name":"source-text","$":{"id":"afs62"},"_":"T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, C. L. Zitnick, Microsoft coco: Common objects in context, in: European conference on computer vision, Springer, 2014, pp. 740–755."}]},{"#name":"bib-reference","$":{"id":"b25"},"$$":[{"#name":"label","_":"Lipton et al., 2015"},{"#name":"reference","$":{"id":"sb25","refId":"25"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Lipton"},{"#name":"given-name","_":"Zachary C."}]},{"#name":"author","$$":[{"#name":"surname","_":"Berkowitz"},{"#name":"given-name","_":"John"}]},{"#name":"author","$$":[{"#name":"surname","_":"Elkan"},{"#name":"given-name","_":"Charles"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A critical review of recurrent neural networks for sequence learning"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2015"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref12","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1506.00019","type":"simple"},"_":"arXiv:1506.00019"}]}]},{"#name":"source-text","$":{"id":"afs47"},"_":"Z. C. Lipton, J. Berkowitz, C. Elkan, A critical review of recurrent neural networks for sequence learning, arXiv preprint arXiv:1506.00019."}]},{"#name":"bib-reference","$":{"id":"b26"},"$$":[{"#name":"label","_":"Loshchilov and Hutter, 2017"},{"#name":"reference","$":{"id":"sb26","refId":"26"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Loshchilov"},{"#name":"given-name","_":"Ilya"}]},{"#name":"author","$$":[{"#name":"surname","_":"Hutter"},{"#name":"given-name","_":"Frank"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Decoupled weight decay regularization"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2017"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref13","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1711.05101","type":"simple"},"_":"arXiv:1711.05101"}]}]},{"#name":"source-text","$":{"id":"afs67"},"_":"I. Loshchilov, F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.05101."}]},{"#name":"bib-reference","$":{"id":"b27"},"$$":[{"#name":"label","_":"Lu et al., 2016"},{"#name":"reference","$":{"id":"sb27","refId":"27"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Lu"},{"#name":"given-name","_":"Jiasen"}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"Jianwei"}]},{"#name":"author","$$":[{"#name":"surname","_":"Batra"},{"#name":"given-name","_":"Dhruv"}]},{"#name":"author","$$":[{"#name":"surname","_":"Parikh"},{"#name":"given-name","_":"Devi"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Hierarchical question-image co-attention for visual question answering"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Advances in neural information processing systems"}]},{"#name":"date","_":"2016"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"289"},{"#name":"last-page","_":"297"}]}]}]},{"#name":"source-text","$":{"id":"afs66"},"_":"J. Lu, J. Yang, D. Batra, D. Parikh, Hierarchical question-image co-attention for visual question answering, in: Advances in neural information processing systems, 2016, pp. 289–297."}]},{"#name":"bib-reference","$":{"id":"b28"},"$$":[{"#name":"label","_":"MacAvaney et al., 2019"},{"#name":"reference","$":{"id":"sb28","refId":"28"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"MacAvaney"},{"#name":"given-name","_":"Sean"}]},{"#name":"author","$$":[{"#name":"surname","_":"Yao"},{"#name":"given-name","_":"Hao -Ren"}]},{"#name":"author","$$":[{"#name":"surname","_":"Yang"},{"#name":"given-name","_":"Eugene"}]},{"#name":"author","$$":[{"#name":"surname","_":"Russell"},{"#name":"given-name","_":"Katina"}]},{"#name":"author","$$":[{"#name":"surname","_":"Goharian"},{"#name":"given-name","_":"Nazli"}]},{"#name":"author","$$":[{"#name":"surname","_":"Frieder"},{"#name":"given-name","_":"Ophir"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Hate speech detection: Challenges and solutions"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"PLoS One"}]},{"#name":"volume-nr","_":"14"}]},{"#name":"issue-nr","_":"8"},{"#name":"date","_":"2019"}]},{"#name":"article-number","_":"e0221152"}]}]},{"#name":"source-text","$":{"id":"afs35"},"_":"S. MacAvaney, H.-R. Yao, E. Yang, K. Russell, N. Goharian, O. Frieder, Hate speech detection: Challenges and solutions, PloS one 14 (8) (2019) e0221152."}]},{"#name":"bib-reference","$":{"id":"b29"},"$$":[{"#name":"label","_":"Magu et al., 2017"},{"#name":"reference","$":{"id":"sb29","refId":"29"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Magu"},{"#name":"given-name","_":"Rijul"}]},{"#name":"author","$$":[{"#name":"surname","_":"Joshi"},{"#name":"given-name","_":"Kshitij"}]},{"#name":"author","$$":[{"#name":"surname","_":"Luo"},{"#name":"given-name","_":"Jiebo"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Detecting the hate code on social media"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2017"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref14","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1703.05443","type":"simple"},"_":"arXiv:1703.05443"}]}]},{"#name":"source-text","$":{"id":"afs23"},"_":"R. Magu, K. Joshi, J. Luo, Detecting the hate code on social media, arXiv preprint arXiv:1703.05443."}]},{"#name":"bib-reference","$":{"id":"b30"},"$$":[{"#name":"label","_":"Mathew et al., 2019"},{"#name":"other-ref","$":{"id":"sb30","refId":"30"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Mathew, Binny, Dutt, Ritam, Goyal, Pawan, & Mukherjee, Animesh (2019). Spread of hate speech in online social media. In "},{"#name":"italic","_":"Proceedings of the 10th ACM conference on web science"},{"#name":"__text__","_":" (pp. 173–182)."}]}]}]},{"#name":"bib-reference","$":{"id":"b31"},"$$":[{"#name":"label","_":"Ngiam et al., 2011"},{"#name":"other-ref","$":{"id":"sb31","refId":"31"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Ngiam, Jiquan, Khosla, Aditya, Kim, Mingyu, Nam, Juhan, Lee, Honglak, & Ng, Andrew Y (2011). Multimodal deep learning. In "},{"#name":"italic","_":"ICML"},{"#name":"__text__","_":"."}]}]}]},{"#name":"bib-reference","$":{"id":"b32"},"$$":[{"#name":"label","_":"Noriega, 2005"},{"#name":"reference","$":{"id":"sb32","refId":"32"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Noriega"},{"#name":"given-name","_":"Leonardo"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multilayer perceptron tutorial"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2005"},{"#name":"publisher","$$":[{"#name":"name","_":"School of Computing. Staffordshire University"}]}]}]}]},{"#name":"source-text","$":{"id":"afs14"},"_":"L. Noriega, Multilayer perceptron tutorial, School of Computing. Staffordshire University."}]},{"#name":"bib-reference","$":{"id":"b33"},"$$":[{"#name":"label","_":"Paavola et al., 2016"},{"#name":"reference","$":{"id":"sb33","refId":"33"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Paavola"},{"#name":"given-name","_":"Jarkko"}]},{"#name":"author","$$":[{"#name":"surname","_":"Helo"},{"#name":"given-name","_":"Tuomo"}]},{"#name":"author","$$":[{"#name":"surname","_":"Jalonen"},{"#name":"given-name","_":"Harri"}]},{"#name":"author","$$":[{"#name":"surname","_":"Sartonen"},{"#name":"given-name","_":"Miika"}]},{"#name":"author","$$":[{"#name":"surname","_":"Huhtinen"},{"#name":"given-name","_":"AM"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Understanding the trolling phenomenon: The automated detection of bots and cyborgs in the social media"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Journal of Information Warfare"}]},{"#name":"volume-nr","_":"15"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"2016"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"100"},{"#name":"last-page","_":"111"}]}]}]},{"#name":"source-text","$":{"id":"afs21"},"_":"J. Paavola, T. Helo, H. Jalonen, M. Sartonen, A. Huhtinen, Understanding the trolling phenomenon: The automated detection of bots and cyborgs in the social media, Journal of Information Warfare 15 (4) (2016) 100–111."}]},{"#name":"bib-reference","$":{"id":"b34"},"$$":[{"#name":"label","_":"Pennington et al., 2014"},{"#name":"other-ref","$":{"id":"sb34","refId":"34"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Pennington, Jeffrey, Socher, Richard, & Manning, Christopher D. (2014). Glove: Global vectors for word representation. In "},{"#name":"italic","_":"Proceedings of the 2014 conference on empirical methods in natural language processing"},{"#name":"__text__","_":" (pp. 1532–1543)."}]}]}]},{"#name":"bib-reference","$":{"id":"b35"},"$$":[{"#name":"label","_":"Pronobis and Jensfelt, 2012"},{"#name":"reference","$":{"id":"sb35","refId":"35"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Pronobis"},{"#name":"given-name","_":"Andrzej"}]},{"#name":"author","$$":[{"#name":"surname","_":"Jensfelt"},{"#name":"given-name","_":"Patric"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Large-scale semantic mapping and reasoning with heterogeneous modalities"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"2012 IEEE international conference on robotics and automation"}]},{"#name":"date","_":"2012"},{"#name":"publisher","$$":[{"#name":"name","_":"IEEE"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"3515"},{"#name":"last-page","_":"3522"}]}]}]},{"#name":"source-text","$":{"id":"afs54"},"_":"A. Pronobis, P. Jensfelt, Large-scale semantic mapping and reasoning with heterogeneous modalities, in: 2012 IEEE International Conference on Robotics and Automation, IEEE, 2012, pp. 3515–3522."}]},{"#name":"bib-reference","$":{"id":"b36"},"$$":[{"#name":"label","_":"Qin et al., 2018"},{"#name":"reference","$":{"id":"sb36","refId":"36"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Qin"},{"#name":"given-name","_":"Zhuwei"}]},{"#name":"author","$$":[{"#name":"surname","_":"Yu"},{"#name":"given-name","_":"Fuxun"}]},{"#name":"author","$$":[{"#name":"surname","_":"Liu"},{"#name":"given-name","_":"Chenchen"}]},{"#name":"author","$$":[{"#name":"surname","_":"Chen"},{"#name":"given-name","_":"Xiang"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"How convolutional neural network see the world-A survey of convolutional neural network visualization methods"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2018"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref15","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1804.11191","type":"simple"},"_":"arXiv:1804.11191"}]}]},{"#name":"source-text","$":{"id":"afs46"},"_":"Z. Qin, F. Yu, C. Liu, X. Chen, How convolutional neural network see the world-a survey of convolutional neural network visualization methods, arXiv preprint arXiv:1804.11191."}]},{"#name":"bib-reference","$":{"id":"b37"},"$$":[{"#name":"label","_":"Ramachandram and Taylor, 2017"},{"#name":"reference","$":{"id":"sb37","refId":"37"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Ramachandram"},{"#name":"given-name","_":"Dhanesh"}]},{"#name":"author","$$":[{"#name":"surname","_":"Taylor"},{"#name":"given-name","_":"Graham W."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Deep multimodal learning: A survey on recent advances and trends"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"IEEE Signal Processing Magazine"}]},{"#name":"volume-nr","_":"34"}]},{"#name":"issue-nr","_":"6"},{"#name":"date","_":"2017"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"96"},{"#name":"last-page","_":"108"}]}]}]},{"#name":"source-text","$":{"id":"afs42"},"_":"D. Ramachandram, G. W. Taylor, Deep multimodal learning: A survey on recent advances and trends, IEEE Signal Processing Magazine 34 (6) (2017) 96–108."}]},{"#name":"bib-reference","$":{"id":"b38"},"$$":[{"#name":"label","_":"Relia et al., 2019"},{"#name":"reference","$":{"id":"sb38","refId":"38"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Relia"},{"#name":"given-name","_":"Kunal"}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Zhengyi"}]},{"#name":"author","$$":[{"#name":"surname","_":"Cook"},{"#name":"given-name","_":"Stephanie H."}]},{"#name":"author","$$":[{"#name":"surname","_":"Chunara"},{"#name":"given-name","_":"Rumi"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Race, ethnicity and national origin-based discrimination in social media and hate crimes across 100 US cities"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"book-series","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Proceedings of the international AAAI conference on web and social media"}]},{"#name":"volume-nr","_":"Vol. 13"}]}]},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"417"},{"#name":"last-page","_":"427"}]}]}]},{"#name":"source-text","$":{"id":"afs29"},"_":"K. Relia, Z. Li, S. H. Cook, R. Chunara, Race, ethnicity and national origin-based discrimination in social media and hate crimes across 100 us cities, in: Proceedings of the International AAAI Conference on Web and Social Media, Vol. 13, 2019, pp. 417–427."}]},{"#name":"bib-reference","$":{"id":"b39"},"$$":[{"#name":"label","_":"Ren et al., 2015"},{"#name":"reference","$":{"id":"sb39","refId":"39"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Ren"},{"#name":"given-name","_":"Shaoqing"}]},{"#name":"author","$$":[{"#name":"surname","_":"He"},{"#name":"given-name","_":"Kaiming"}]},{"#name":"author","$$":[{"#name":"surname","_":"Girshick"},{"#name":"given-name","_":"Ross"}]},{"#name":"author","$$":[{"#name":"surname","_":"Sun"},{"#name":"given-name","_":"Jian"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Faster r-cnn: Towards real-time object detection with region proposal networks"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Advances in neural information processing systems"}]},{"#name":"date","_":"2015"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"91"},{"#name":"last-page","_":"99"}]}]}]},{"#name":"source-text","$":{"id":"afs61"},"_":"S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object detection with region proposal networks, in: Advances in neural information processing systems, 2015, pp. 91–99."}]},{"#name":"bib-reference","$":{"id":"b40"},"$$":[{"#name":"label","_":"Ribeiro et al., 2017"},{"#name":"reference","$":{"id":"sb40","refId":"40"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Ribeiro"},{"#name":"given-name","_":"Manoel Horta"}]},{"#name":"author","$$":[{"#name":"surname","_":"Calais"},{"#name":"given-name","_":"Pedro H"}]},{"#name":"author","$$":[{"#name":"surname","_":"Santos"},{"#name":"given-name","_":"Yuri A"}]},{"#name":"author","$$":[{"#name":"surname","_":"Almeida"},{"#name":"given-name","_":"Virgílio AF"}]},{"#name":"author","$$":[{"#name":"surname","_":"Meira"},{"#name":"given-name","_":"Wagner"},{"#name":"suffix","_":"Jr."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"“ Like sheep among wolves”: Characterizing hateful users on Twitter"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2017"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref16","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1801.00317","type":"simple"},"_":"arXiv:1801.00317"}]}]},{"#name":"source-text","$":{"id":"afs22"},"_":"M. H. Ribeiro, P. H. Calais, Y. A. Santos, V. A. Almeida, W. Meira Jr, ” like sheep among wolves”: Characterizing hateful users on twitter, arXiv preprint arXiv:1801.00317."}]},{"#name":"bib-reference","$":{"id":"b41"},"$$":[{"#name":"label","_":"Sabat et al., 2019"},{"#name":"reference","$":{"id":"sb41","refId":"41"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Sabat"},{"#name":"given-name","_":"Benet Oriol"}]},{"#name":"author","$$":[{"#name":"surname","_":"Ferrer"},{"#name":"given-name","_":"Cristian Canton"}]},{"#name":"author","$$":[{"#name":"surname","_":"Giro-i Nieto"},{"#name":"given-name","_":"Xavier"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Hate speech in pixels: Detection of offensive memes towards automatic moderation"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2019"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref17","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1910.02334","type":"simple"},"_":"arXiv:1910.02334"}]}]},{"#name":"source-text","$":{"id":"afs7"},"_":"B. O. Sabat, C. C. Ferrer, X. Giro-i Nieto, Hate speech in pixels: Detection of offensive memes towards automatic moderation, arXiv preprint arXiv:1910.02334."}]},{"#name":"bib-reference","$":{"id":"b42"},"$$":[{"#name":"label","_":"Schmidt and Wiegand, 2017"},{"#name":"other-ref","$":{"id":"sb42","refId":"42"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Schmidt, Anna, & Wiegand, Michael (2017). A survey on hate speech detection using natural language processing. In "},{"#name":"italic","_":"Proceedings of the fifth international workshop on natural language processing for social media"},{"#name":"__text__","_":" (pp. 1–10)."}]}]}]},{"#name":"bib-reference","$":{"id":"b43"},"$$":[{"#name":"label","_":"Shang et al., 2019"},{"#name":"reference","$":{"id":"sb43","refId":"43"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Shang"},{"#name":"given-name","_":"Lanyu"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Daniel Yue"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Michael"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Dong"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"VulnerCheck: A content-agnostic detector for online hatred-vulnerable videos"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"2019 IEEE international conference on big data"}]},{"#name":"date","_":"2019"},{"#name":"publisher","$$":[{"#name":"name","_":"IEEE"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"573"},{"#name":"last-page","_":"582"}]}]}]},{"#name":"source-text","$":{"id":"afs36"},"_":"L. Shang, D. Y. Zhang, M. Wang, D. Wang, Vulnercheck: a content-agnostic detector for online hatred-vulnerable videos, in: 2019 IEEE International Conference on Big Data (Big Data), IEEE, 2019, pp. 573–582."}]},{"#name":"bib-reference","$":{"id":"b44"},"$$":[{"#name":"label","_":"Sharma et al., 2020"},{"#name":"reference","$":{"id":"sb44","refId":"44"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Sharma"},{"#name":"given-name","_":"Chhavi"}]},{"#name":"author","$$":[{"#name":"surname","_":"Bhageria"},{"#name":"given-name","_":"Deepesh"}]},{"#name":"author","$$":[{"#name":"surname","_":"Scott"},{"#name":"given-name","_":"William"}]},{"#name":"author","$$":[{"#name":"surname","_":"PYKL"},{"#name":"given-name","_":"Srinivas"}]},{"#name":"author","$$":[{"#name":"surname","_":"Das"},{"#name":"given-name","_":"Amitava"}]},{"#name":"author","$$":[{"#name":"surname","_":"Chakraborty"},{"#name":"given-name","_":"Tanmoy"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Semeval-2020 task 8: Memotion analysis–the visuo-lingual metaphor!"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2020"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref18","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:2008.03781","type":"simple"},"_":"arXiv:2008.03781"}]}]},{"#name":"source-text","$":{"id":"afs8"},"_":"C. Sharma, D. Bhageria, W. Scott, S. PYKL, A. Das, T. Chakraborty, V. Pulabaigari, B. Gamback, Semeval-2020 task 8: Memotion analysis–the visuo-lingual metaphor!, arXiv preprint arXiv:2008.03781."}]},{"#name":"bib-reference","$":{"id":"b45"},"$$":[{"#name":"label","_":"Shen and Robertson, 2020"},{"#name":"reference","$":{"id":"sb45","refId":"45"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Shen"},{"#name":"given-name","_":"Jialie"}]},{"#name":"author","$$":[{"#name":"surname","_":"Robertson"},{"#name":"given-name","_":"Neil"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"BBAS: Towards large scale effective ensemble adversarial attacks against deep neural network learning"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Information Sciences"}]}]},{"#name":"date","_":"2020"}]}]}]},{"#name":"source-text","$":{"id":"afs44"},"_":"J. Shen, N. Robertson, Bbas: Towards large scale effective ensemble adversarial attacks against deep neural network learning, Information Sciences."}]},{"#name":"bib-reference","$":{"id":"b46"},"$$":[{"#name":"label","_":"Shu et al., 2017"},{"#name":"reference","$":{"id":"sb46","refId":"46"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Shu"},{"#name":"given-name","_":"Kai"}]},{"#name":"author","$$":[{"#name":"surname","_":"Sliva"},{"#name":"given-name","_":"Amy"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Suhang"}]},{"#name":"author","$$":[{"#name":"surname","_":"Tang"},{"#name":"given-name","_":"Jiliang"}]},{"#name":"author","$$":[{"#name":"surname","_":"Liu"},{"#name":"given-name","_":"Huan"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Fake news detection on social media: A data mining perspective"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"ACM SIGKDD Explorations Newsletter"}]},{"#name":"volume-nr","_":"19"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2017"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"22"},{"#name":"last-page","_":"36"}]}]}]},{"#name":"source-text","$":{"id":"afs25"},"_":"K. Shu, A. Sliva, S. Wang, J. Tang, H. Liu, Fake news detection on social media: A data mining perspective, ACM SIGKDD explorations newsletter 19 (1) (2017) 22–36."}]},{"#name":"bib-reference","$":{"id":"b47"},"$$":[{"#name":"label","_":"Simonyan and Zisserman, 2014"},{"#name":"reference","$":{"id":"sb47","refId":"47"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Simonyan"},{"#name":"given-name","_":"Karen"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zisserman"},{"#name":"given-name","_":"Andrew"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Very deep convolutional networks for large-scale image recognition"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2014"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref19","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1409.1556","type":"simple"},"_":"arXiv:1409.1556"}]}]},{"#name":"source-text","$":{"id":"afs11"},"_":"K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556."}]},{"#name":"bib-reference","$":{"id":"b48"},"$$":[{"#name":"label","_":"Specia et al., 2016"},{"#name":"reference","$":{"id":"sb48","refId":"48"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Specia"},{"#name":"given-name","_":"Lucia"}]},{"#name":"author","$$":[{"#name":"surname","_":"Frank"},{"#name":"given-name","_":"Stella"}]},{"#name":"author","$$":[{"#name":"surname","_":"Sima’an"},{"#name":"given-name","_":"Khalil"}]},{"#name":"author","$$":[{"#name":"surname","_":"Elliott"},{"#name":"given-name","_":"Desmond"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A shared task on multimodal machine translation and crosslingual image description"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Proceedings of the first conference on machine translation"}]},{"#name":"book-series","$$":[{"#name":"series","$$":[{"#name":"volume-nr","_":"Vol. 2, Shared Task Papers"}]}]},{"#name":"date","_":"2016"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"543"},{"#name":"last-page","_":"553"}]}]}]},{"#name":"source-text","$":{"id":"afs50"},"_":"L. Specia, S. Frank, K. Simaan, D. Elliott, A shared task on multimodal machine translation and crosslingual image description, in: Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016, pp. 543–553."}]},{"#name":"bib-reference","$":{"id":"b49"},"$$":[{"#name":"label","_":"Srivastava et al., 2012"},{"#name":"reference","$":{"id":"sb49","refId":"49"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Srivastava"},{"#name":"given-name","_":"Nitish"}]},{"#name":"author","$$":[{"#name":"surname","_":"Salakhutdinov"},{"#name":"given-name","_":"Ruslan"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multimodal learning with deep Boltzmann machines"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"book-series","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"NIPS"}]},{"#name":"volume-nr","_":"Vol. 1"}]}]},{"#name":"date","_":"2012"},{"#name":"publisher","$$":[{"#name":"name","_":"Citeseer"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"2"}]}]}]},{"#name":"source-text","$":{"id":"afs57"},"_":"N. Srivastava, R. Salakhutdinov, et al., Multimodal learning with deep boltzmann machines., in: NIPS, Vol. 1, Citeseer, 2012, p. 2."}]},{"#name":"bib-reference","$":{"id":"b50"},"$$":[{"#name":"label","_":"Suryawanshi et al., 2020"},{"#name":"other-ref","$":{"id":"sb50","refId":"50"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Suryawanshi, Shardul, Chakravarthi, Bharathi Raja, Arcan, Mihael, & Buitelaar, Paul (2020). Multimodal meme dataset (MultiOFF) for identifying offensive content in image and text. In "},{"#name":"italic","_":"Proceedings of the second workshop on trolling, aggression and cyberbullying"},{"#name":"__text__","_":" (pp. 32–41)."}]}]}]},{"#name":"bib-reference","$":{"id":"b51"},"$$":[{"#name":"label","_":"Van Hee et al., 2018"},{"#name":"reference","$":{"id":"sb51","refId":"51"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Van Hee"},{"#name":"given-name","_":"Cynthia"}]},{"#name":"author","$$":[{"#name":"surname","_":"Jacobs"},{"#name":"given-name","_":"Gilles"}]},{"#name":"author","$$":[{"#name":"surname","_":"Emmery"},{"#name":"given-name","_":"Chris"}]},{"#name":"author","$$":[{"#name":"surname","_":"Desmet"},{"#name":"given-name","_":"Bart"}]},{"#name":"author","$$":[{"#name":"surname","_":"Lefever"},{"#name":"given-name","_":"Els"}]},{"#name":"author","$$":[{"#name":"surname","_":"Verhoeven"},{"#name":"given-name","_":"Ben"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Automatic detection of cyberbullying in social media text"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"PLoS One"}]},{"#name":"volume-nr","_":"13"}]},{"#name":"issue-nr","_":"10"},{"#name":"date","_":"2018"}]},{"#name":"article-number","_":"e0203794"}]}]},{"#name":"source-text","$":{"id":"afs1"},"_":"C. Van Hee, G. Jacobs, C. Emmery, B. Desmet, E. Lefever, B. Verhoeven, G. De Pauw, W. Daelemans, V. Hoste, Automatic detection of cyberbullying in social media text, PloS one 13 (10) (2018) e0203794."}]},{"#name":"bib-reference","$":{"id":"b52"},"$$":[{"#name":"label","_":"Vaswani et al., 2017"},{"#name":"reference","$":{"id":"sb52","refId":"52"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Vaswani"},{"#name":"given-name","_":"Ashish"}]},{"#name":"author","$$":[{"#name":"surname","_":"Shazeer"},{"#name":"given-name","_":"Noam"}]},{"#name":"author","$$":[{"#name":"surname","_":"Parmar"},{"#name":"given-name","_":"Niki"}]},{"#name":"author","$$":[{"#name":"surname","_":"Uszkoreit"},{"#name":"given-name","_":"Jakob"}]},{"#name":"author","$$":[{"#name":"surname","_":"Jones"},{"#name":"given-name","_":"Llion"}]},{"#name":"author","$$":[{"#name":"surname","_":"Gomez"},{"#name":"given-name","_":"Aidan N"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Attention is all you need"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2017"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref20","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1706.03762","type":"simple"},"_":"arXiv:1706.03762"}]}]},{"#name":"source-text","$":{"id":"afs13"},"_":"A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, arXiv preprint arXiv:1706.03762."}]},{"#name":"bib-reference","$":{"id":"b53"},"$$":[{"#name":"label","_":"Velioglu and Rose, 2020"},{"#name":"reference","$":{"id":"sb53","refId":"53"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Velioglu"},{"#name":"given-name","_":"Riza"}]},{"#name":"author","$$":[{"#name":"surname","_":"Rose"},{"#name":"given-name","_":"Jewgeni"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Detecting hate speech in memes using multimodal deep learning approaches: Prize-winning solution to hateful memes challenge"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2020"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref21","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:2012.12975","type":"simple"},"_":"arXiv:2012.12975"}]}]},{"#name":"source-text","$":{"id":"afs10"},"_":"R. Velioglu, J. Rose, Detecting hate speech in memes using multimodal deep learning approaches: Prize-winning solution to hateful memes challenge, arXiv preprint arXiv:2012.12975."}]},{"#name":"bib-reference","$":{"id":"b54"},"$$":[{"#name":"label","_":"Venugopalan et al., 2021"},{"#name":"reference","$":{"id":"sb54","refId":"54"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Venugopalan"},{"#name":"given-name","_":"Janani"}]},{"#name":"author","$$":[{"#name":"surname","_":"Tong"},{"#name":"given-name","_":"Li"}]},{"#name":"author","$$":[{"#name":"surname","_":"Hassanzadeh"},{"#name":"given-name","_":"Hamid Reza"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"May D"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Multimodal deep learning models for early detection of Alzheimer’s disease stage"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Scientific Reports"}]},{"#name":"volume-nr","_":"11"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2021"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"13"}]}]}]},{"#name":"source-text","$":{"id":"afs56"},"_":"J. Venugopalan, L. Tong, H. R. Hassanzadeh, M. D. Wang, Multimodal deep learning models for early detection of alzheimers disease stage, Scientific reports 11 (1) (2021) 1–13."}]},{"#name":"bib-reference","$":{"id":"b55"},"$$":[{"#name":"label","_":"Wang et al., 2015"},{"#name":"reference","$":{"id":"sb55","refId":"55"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Dong"}]},{"#name":"author","$$":[{"#name":"surname","_":"Abdelzaher"},{"#name":"given-name","_":"Tarek"}]},{"#name":"author","$$":[{"#name":"surname","_":"Kaplan"},{"#name":"given-name","_":"Lance"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Social sensing: Building reliable systems on unreliable data"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2015"},{"#name":"publisher","$$":[{"#name":"name","_":"Morgan Kaufmann"}]}]}]}]},{"#name":"source-text","$":{"id":"afs18"},"_":"D. Wang, T. Abdelzaher, L. Kaplan, Social sensing: building reliable systems on unreliable data, Morgan Kaufmann, 2015."}]},{"#name":"bib-reference","$":{"id":"b56"},"$$":[{"#name":"label","_":"Wang et al., 2012"},{"#name":"other-ref","$":{"id":"sb56","refId":"56"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Wang, Dong, Kaplan, Lance, Le, Hieu, & Abdelzaher, Tarek (2012). On truth discovery in social sensing: A maximum likelihood estimation approach. In "},{"#name":"italic","_":"Proceedings of the 11th international conference on information processing in sensor networks"},{"#name":"__text__","_":" (pp. 233–244)."}]}]}]},{"#name":"bib-reference","$":{"id":"b57"},"$$":[{"#name":"label","_":"Wang et al., 2019"},{"#name":"reference","$":{"id":"sb57","refId":"57"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Dong"}]},{"#name":"author","$$":[{"#name":"surname","_":"Szymanski"},{"#name":"given-name","_":"Boleslaw K"}]},{"#name":"author","$$":[{"#name":"surname","_":"Abdelzaher"},{"#name":"given-name","_":"Tarek"}]},{"#name":"author","$$":[{"#name":"surname","_":"Ji"},{"#name":"given-name","_":"Heng"}]},{"#name":"author","$$":[{"#name":"surname","_":"Kaplan"},{"#name":"given-name","_":"Lance"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"The age of social sensing"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Computer"}]},{"#name":"volume-nr","_":"52"}]},{"#name":"issue-nr","_":"1"},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"36"},{"#name":"last-page","_":"45"}]}]}]},{"#name":"source-text","$":{"id":"afs19"},"_":"D. Wang, B. K. Szymanski, T. Abdelzaher, H. Ji, L. Kaplan, The age of social sensing, Computer 52 (1) (2019) 36–45."}]},{"#name":"bib-reference","$":{"id":"b58"},"$$":[{"#name":"label","_":"Warrens, 2010"},{"#name":"reference","$":{"id":"sb58","refId":"58"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Warrens"},{"#name":"given-name","_":"Matthijs J."}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Inequalities between multi-rater kappas"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Advances in Data Analysis and Classification"}]},{"#name":"volume-nr","_":"4"}]},{"#name":"issue-nr","_":"4"},{"#name":"date","_":"2010"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"271"},{"#name":"last-page","_":"286"}]}]}]},{"#name":"source-text","$":{"id":"afs69"},"_":"M. J. Warrens, Inequalities between multi-rater kappas, Advances in data analysis and classification 4 (4) (2010) 271–286."}]},{"#name":"bib-reference","$":{"id":"b59"},"$$":[{"#name":"label","_":"Waseem and Hovy, 2016"},{"#name":"other-ref","$":{"id":"sb59","refId":"59"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Waseem, Zeerak, & Hovy, Dirk (2016). Hateful symbols or hateful people? Predictive features for hate speech detection on twitter. In "},{"#name":"italic","_":"Proceedings of the NAACL student research workshop"},{"#name":"__text__","_":" (pp. 88–93)."}]}]}]},{"#name":"bib-reference","$":{"id":"b60"},"$$":[{"#name":"label","_":"Wu and Liu, 2018"},{"#name":"other-ref","$":{"id":"sb60","refId":"60"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Wu, Liang, & Liu, Huan (2018). Tracing fake-news footprints: Characterizing social media messages by how they propagate. In "},{"#name":"italic","_":"Proceedings of the eleventh ACM international conference on web search and data mining"},{"#name":"__text__","_":" (pp. 637–645)."}]}]}]},{"#name":"bib-reference","$":{"id":"b61"},"$$":[{"#name":"label","_":"Wu et al., 2017"},{"#name":"reference","$":{"id":"sb61","refId":"61"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Wu"},{"#name":"given-name","_":"Qi"}]},{"#name":"author","$$":[{"#name":"surname","_":"Teney"},{"#name":"given-name","_":"Damien"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Peng"}]},{"#name":"author","$$":[{"#name":"surname","_":"Shen"},{"#name":"given-name","_":"Chunhua"}]},{"#name":"author","$$":[{"#name":"surname","_":"Dick"},{"#name":"given-name","_":"Anthony"}]},{"#name":"author","$$":[{"#name":"surname","_":"van den Hengel"},{"#name":"given-name","_":"Anton"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Visual question answering: A survey of methods and datasets"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Computer Vision and Image Understanding"}]},{"#name":"volume-nr","_":"163"}]},{"#name":"date","_":"2017"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"21"},{"#name":"last-page","_":"40"}]}]}]},{"#name":"source-text","$":{"id":"afs52"},"_":"Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, A. van den Hengel, Visual question answering: A survey of methods and datasets, Computer Vision and Image Understanding 163 (2017) 21–40."}]},{"#name":"bib-reference","$":{"id":"b62"},"$$":[{"#name":"label","_":"Yang et al., 2018"},{"#name":"other-ref","$":{"id":"sb62","refId":"62"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Yang, Jufeng, She, Dongyu, Lai, Yu -Kun, Rosin, Paul L, & Yang, Ming -Hsuan (2018). Weakly supervised coupled networks for visual sentiment analysis. In "},{"#name":"italic","_":"Proceedings of the IEEE conference on computer vision and pattern recognition"},{"#name":"__text__","_":" (pp. 7584–7592)."}]}]}]},{"#name":"bib-reference","$":{"id":"b63"},"$$":[{"#name":"label","_":"Yao et al., 2019"},{"#name":"other-ref","$":{"id":"sb63","refId":"63"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Yao, Mengfan, Chelmis, Charalampos, & Zois, Daphney Stavroula (2019). Cyberbullying ends here: Towards robust detection of cyberbullying in social media. In "},{"#name":"italic","_":"The world wide web conference"},{"#name":"__text__","_":" (pp. 3427–3433)."}]}]}]},{"#name":"bib-reference","$":{"id":"b64"},"$$":[{"#name":"label","_":"Yi et al., 2018"},{"#name":"reference","$":{"id":"sb64","refId":"64"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Yi"},{"#name":"given-name","_":"Kexin"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wu"},{"#name":"given-name","_":"Jiajun"}]},{"#name":"author","$$":[{"#name":"surname","_":"Gan"},{"#name":"given-name","_":"Chuang"}]},{"#name":"author","$$":[{"#name":"surname","_":"Torralba"},{"#name":"given-name","_":"Antonio"}]},{"#name":"author","$$":[{"#name":"surname","_":"Kohli"},{"#name":"given-name","_":"Pushmeet"}]},{"#name":"author","$$":[{"#name":"surname","_":"Tenenbaum"},{"#name":"given-name","_":"Josh"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Neural-symbolic vqa: Disentangling reasoning from vision and language understanding"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Advances in neural information processing systems"}]},{"#name":"date","_":"2018"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1031"},{"#name":"last-page","_":"1042"}]}]}]},{"#name":"source-text","$":{"id":"afs53"},"_":"K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, J. Tenenbaum, Neural-symbolic vqa: Disentangling reasoning from vision and language understanding, in: Advances in neural information processing systems, 2018, pp. 1031–1042."}]},{"#name":"bib-reference","$":{"id":"b65"},"$$":[{"#name":"label","_":"Yin et al., 2018"},{"#name":"other-ref","$":{"id":"sb65","refId":"65"},"$$":[{"#name":"textref","$$":[{"#name":"__text__","_":"Yin, Yifang, Shah, Rajiv Ratn, & Zimmermann, Roger (2018). Learning and fusing multimodal deep features for acoustic scene categorization. In "},{"#name":"italic","_":"Proceedings of the 26th ACM international conference on multimedia"},{"#name":"__text__","_":" (pp. 1892–1900)."}]}]}]},{"#name":"bib-reference","$":{"id":"b66"},"$$":[{"#name":"label","_":"Zhang et al., 2020"},{"#name":"reference","$":{"id":"sb66","refId":"66"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Yang"}]},{"#name":"author","$$":[{"#name":"surname","_":"Dong"},{"#name":"given-name","_":"Xiangyu"}]},{"#name":"author","$$":[{"#name":"surname","_":"Shang"},{"#name":"given-name","_":"Lanyu"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Daniel"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Dong"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A multi-modal graph neural network approach to traffic risk forecasting in smart urban sensing"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"2020 17th annual IEEE international conference on sensing, communication, and networking"}]},{"#name":"date","_":"2020"},{"#name":"publisher","$$":[{"#name":"name","_":"IEEE"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1"},{"#name":"last-page","_":"9"}]}]}]},{"#name":"source-text","$":{"id":"afs48"},"_":"Y. Zhang, X. Dong, L. Shang, D. Zhang, D. Wang, A multi-modal graph neural network approach to traffic risk forecasting in smart urban sensing, in: 2020 17th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON), IEEE, 2020, pp. 1–9."}]},{"#name":"bib-reference","$":{"id":"b67"},"$$":[{"#name":"label","_":"Zhang, Lu, Zhang, Shang, and Wang, 2018"},{"#name":"reference","$":{"id":"sb67","refId":"67"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Yang"}]},{"#name":"author","$$":[{"#name":"surname","_":"Lu"},{"#name":"given-name","_":"Yiwen"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Daniel"}]},{"#name":"author","$$":[{"#name":"surname","_":"Shang"},{"#name":"given-name","_":"Lanyu"}]},{"#name":"author","$$":[{"#name":"surname","_":"Wang"},{"#name":"given-name","_":"Dong"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Risksens: A multi-view learning approach to identifying risky traffic locations in intelligent transportation systems using social and remote sensing"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"2018 IEEE international conference on big data"}]},{"#name":"date","_":"2018"},{"#name":"publisher","$$":[{"#name":"name","_":"IEEE"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"1544"},{"#name":"last-page","_":"1553"}]}]}]},{"#name":"source-text","$":{"id":"afs45"},"_":"Y. Zhang, Y. Lu, D. Zhang, L. Shang, D. Wang, Risksens: A multi-view learning approach to identifying risky traffic locations in intelligent transportation systems using social and remote sensing, in: 2018 IEEE International Conference on Big Data (Big Data), IEEE, 2018, pp. 1544–1553."}]},{"#name":"bib-reference","$":{"id":"b68"},"$$":[{"#name":"label","_":"Zhang and Luo, 2019"},{"#name":"reference","$":{"id":"sb68","refId":"68"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Ziqi"}]},{"#name":"author","$$":[{"#name":"surname","_":"Luo"},{"#name":"given-name","_":"Lei"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Hate speech detection: A solved problem? The challenging case of long tail on twitter"}]}]},{"#name":"host","$$":[{"#name":"issue","$$":[{"#name":"series","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"Semantic Web"}]},{"#name":"volume-nr","_":"10"}]},{"#name":"issue-nr","_":"5"},{"#name":"date","_":"2019"}]},{"#name":"pages","$$":[{"#name":"first-page","_":"925"},{"#name":"last-page","_":"945"}]}]}]},{"#name":"source-text","$":{"id":"afs68"},"_":"Z. Zhang, L. Luo, Hate speech detection: A solved problem? the challenging case of long tail on twitter, Semantic Web 10 (5) (2019) 925–945."}]},{"#name":"bib-reference","$":{"id":"b69"},"$$":[{"#name":"label","_":"Zhang et al., 2018"},{"#name":"reference","$":{"id":"sb69","refId":"69"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhang"},{"#name":"given-name","_":"Daniel Yue"}]},{"#name":"author","$$":[{"#name":"surname","_":"Shang"},{"#name":"given-name","_":"Lanyu"}]},{"#name":"author","$$":[{"#name":"surname","_":"Geng"},{"#name":"given-name","_":"Biao"}]},{"#name":"author","$$":[{"#name":"surname","_":"Lai"},{"#name":"given-name","_":"Shuyue"}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Ke"}]},{"#name":"author","$$":[{"#name":"surname","_":"Zhu"},{"#name":"given-name","_":"Hongmin"}]},{"#name":"et-al"}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Fauxbuster: A content-free fauxtography detector using social media comments"}]}]},{"#name":"host","$$":[{"#name":"edited-book","$$":[{"#name":"title","$$":[{"#name":"maintitle","_":"2018 IEEE international conference on big data"}]},{"#name":"date","_":"2018"},{"#name":"publisher","$$":[{"#name":"name","_":"IEEE"}]}]},{"#name":"pages","$$":[{"#name":"first-page","_":"891"},{"#name":"last-page","_":"900"}]}]}]},{"#name":"source-text","$":{"id":"afs26"},"_":"D. Y. Zhang, L. Shang, B. Geng, S. Lai, K. Li, H. Zhu, M. T. Amin, D. Wang, Fauxbuster: A content-free fauxtography detector using social media comments, in: 2018 IEEE International Conference on Big Data (Big Data), IEEE, 2018, pp. 891–900."}]},{"#name":"bib-reference","$":{"id":"b70"},"$$":[{"#name":"label","_":"Zhou et al., 2018"},{"#name":"reference","$":{"id":"sb70","refId":"70"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhou"},{"#name":"given-name","_":"Mingyang"}]},{"#name":"author","$$":[{"#name":"surname","_":"Cheng"},{"#name":"given-name","_":"Runxiang"}]},{"#name":"author","$$":[{"#name":"surname","_":"Lee"},{"#name":"given-name","_":"Yong Jae"}]},{"#name":"author","$$":[{"#name":"surname","_":"Yu"},{"#name":"given-name","_":"Zhou"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"A visual attention grounding neural model for multimodal machine translation"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2018"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref22","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1808.08266","type":"simple"},"_":"arXiv:1808.08266"}]}]},{"#name":"source-text","$":{"id":"afs51"},"_":"M. Zhou, R. Cheng, Y. J. Lee, Z. Yu, A visual attention grounding neural model for multimodal machine translation, arXiv preprint arXiv:1808.08266."}]},{"#name":"bib-reference","$":{"id":"b71"},"$$":[{"#name":"label","_":"Zhou et al., 2017"},{"#name":"reference","$":{"id":"sb71","refId":"71"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhou"},{"#name":"given-name","_":"Wengang"}]},{"#name":"author","$$":[{"#name":"surname","_":"Li"},{"#name":"given-name","_":"Houqiang"}]},{"#name":"author","$$":[{"#name":"surname","_":"Tian"},{"#name":"given-name","_":"Qi"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Recent advance in content-based image retrieval: A literature survey"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2017"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref23","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:1706.06064","type":"simple"},"_":"arXiv:1706.06064"}]}]},{"#name":"source-text","$":{"id":"afs6"},"_":"W. Zhou, H. Li, Q. Tian, Recent advance in content-based image retrieval: A literature survey, arXiv preprint arXiv:1706.06064."}]},{"#name":"bib-reference","$":{"id":"b72"},"$$":[{"#name":"label","_":"Zhu, 2020"},{"#name":"reference","$":{"id":"sb72","refId":"72"},"$$":[{"#name":"contribution","$":{"langtype":"en"},"$$":[{"#name":"authors","$$":[{"#name":"author","$$":[{"#name":"surname","_":"Zhu"},{"#name":"given-name","_":"Ron"}]}]},{"#name":"title","$$":[{"#name":"maintitle","_":"Enhance multimodal transformer with external label and in-domain pretrain: Hateful meme challenge winning solution"}]}]},{"#name":"host","$$":[{"#name":"book","$$":[{"#name":"date","_":"2020"}]}]},{"#name":"comment","$$":[{"#name":"__text__","_":"arXiv preprint "},{"#name":"inter-ref","$":{"id":"interref24","role":"http://www.elsevier.com/xml/linking-roles/preprint","href":"arxiv:2012.08290","type":"simple"},"_":"arXiv:2012.08290"}]}]},{"#name":"source-text","$":{"id":"afs40"},"_":"R. Zhu, Enhance multimodal transformer with external label and in-domain pretrain: Hateful meme challenge winning solution, arXiv preprint arXiv:2012.08290."}]}]}]}],"floats":[],"footnotes":[],"attachments":[],"sourceTextMap":{"1":"T. Baltrušaitis, C. Ahuja, L.-P. Morency, Multimodal machine learning: A survey and taxonomy, IEEE transactions on pattern analysis and machine intelligence 41 (2) (2018) 423–443.","2":"L. K. Börzsei, Makes a meme instead, The Selected Works of Linda Börzsei (2013) 1–28.","3":"C. M. Castaño Díaz, Defining and characterizing the concept of internet meme, Ces Psicología 6 (2) (2013) 82–104.","7":"D. Choi, S. Chun, H. Oh, J. Han, et al., Rumor propagation is amplified by echo chambers in social media, Scientific Reports 10 (1) (2020) 1–10.","9":"J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805.","10":"E. Englander, E. Donnerstein, R. Kowalski, C. A. Lin, K. Parti, Defining cyberbullying, Pediatrics 140 (Supplement 2) (2017) S148–S151.","11":"P. Fortuna, S. Nunes, A survey on automatic detection of hate speech in text, ACM Computing Surveys (CSUR) 51 (4) (2018) 1–30.","12":"A. Graves, J. Schmidhuber, Framewise phoneme classification with bidirectional lstm and other neural network architectures, Neural networks 18 (5-6) (2005) 602–610.","14":"D. R. Hofstadter, Analogy as the core of cognition, The analogical mind: Perspectives from cognitive science (2001) 499–538.","15":"M. Z. Hossain, F. Sohel, M. F. Shiratuddin, H. Laga, A comprehensive survey of deep learning for image captioning, ACM Computing Surveys (CSUR) 51 (6) (2019) 1–36.","16":"N. Islam, Z. Islam, N. Noor, A survey on optical character recognition system, arXiv preprint arXiv:1710.05703.","17":"A. Jakubowicz, et al., Alt_right white lite: trolling, hate speech and cyber racism on social media, Cosmopolitan Civil Societies: An Interdisciplinary Journal 9 (3) (2017) 41.","18":"S. Jhaver, S. Ghoshal, A. Bruckman, E. Gilbert, Online harassment and content moderation: The case of blocklists, ACM Transactions on Computer-Human Interaction (TOCHI) 25 (2) (2018) 1–33.","19":"D. Kiela, H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshia, D. Testuggine, The hateful memes challenge: Detecting hate speech in multimodal memes, arXiv preprint arXiv:2005.04790.","20":"Z. Kou, D. Y. Zhang, L. Shang, D. Wang, Exfaux: A weakly supervised approach to explainable fauxtography detection, in: 2020 IEEE International Conference on Big Data (Big Data), IEEE, 2020, pp. 631–636.","22":"L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, K.-W. Chang, Visualbert: A simple and performant baseline for vision and language, arXiv preprint arXiv:1908.03557.","24":"T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, C. L. Zitnick, Microsoft coco: Common objects in context, in: European conference on computer vision, Springer, 2014, pp. 740–755.","25":"Z. C. Lipton, J. Berkowitz, C. Elkan, A critical review of recurrent neural networks for sequence learning, arXiv preprint arXiv:1506.00019.","26":"I. Loshchilov, F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.05101.","27":"J. Lu, J. Yang, D. Batra, D. Parikh, Hierarchical question-image co-attention for visual question answering, in: Advances in neural information processing systems, 2016, pp. 289–297.","28":"S. MacAvaney, H.-R. Yao, E. Yang, K. Russell, N. Goharian, O. Frieder, Hate speech detection: Challenges and solutions, PloS one 14 (8) (2019) e0221152.","29":"R. Magu, K. Joshi, J. Luo, Detecting the hate code on social media, arXiv preprint arXiv:1703.05443.","32":"L. Noriega, Multilayer perceptron tutorial, School of Computing. Staffordshire University.","33":"J. Paavola, T. Helo, H. Jalonen, M. Sartonen, A. Huhtinen, Understanding the trolling phenomenon: The automated detection of bots and cyborgs in the social media, Journal of Information Warfare 15 (4) (2016) 100–111.","35":"A. Pronobis, P. Jensfelt, Large-scale semantic mapping and reasoning with heterogeneous modalities, in: 2012 IEEE International Conference on Robotics and Automation, IEEE, 2012, pp. 3515–3522.","36":"Z. Qin, F. Yu, C. Liu, X. Chen, How convolutional neural network see the world-a survey of convolutional neural network visualization methods, arXiv preprint arXiv:1804.11191.","37":"D. Ramachandram, G. W. Taylor, Deep multimodal learning: A survey on recent advances and trends, IEEE Signal Processing Magazine 34 (6) (2017) 96–108.","38":"K. Relia, Z. Li, S. H. Cook, R. Chunara, Race, ethnicity and national origin-based discrimination in social media and hate crimes across 100 us cities, in: Proceedings of the International AAAI Conference on Web and Social Media, Vol. 13, 2019, pp. 417–427.","39":"S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object detection with region proposal networks, in: Advances in neural information processing systems, 2015, pp. 91–99.","40":"M. H. Ribeiro, P. H. Calais, Y. A. Santos, V. A. Almeida, W. Meira Jr, ” like sheep among wolves”: Characterizing hateful users on twitter, arXiv preprint arXiv:1801.00317.","41":"B. O. Sabat, C. C. Ferrer, X. Giro-i Nieto, Hate speech in pixels: Detection of offensive memes towards automatic moderation, arXiv preprint arXiv:1910.02334.","43":"L. Shang, D. Y. Zhang, M. Wang, D. Wang, Vulnercheck: a content-agnostic detector for online hatred-vulnerable videos, in: 2019 IEEE International Conference on Big Data (Big Data), IEEE, 2019, pp. 573–582.","44":"C. Sharma, D. Bhageria, W. Scott, S. PYKL, A. Das, T. Chakraborty, V. Pulabaigari, B. Gamback, Semeval-2020 task 8: Memotion analysis–the visuo-lingual metaphor!, arXiv preprint arXiv:2008.03781.","45":"J. Shen, N. Robertson, Bbas: Towards large scale effective ensemble adversarial attacks against deep neural network learning, Information Sciences.","46":"K. Shu, A. Sliva, S. Wang, J. Tang, H. Liu, Fake news detection on social media: A data mining perspective, ACM SIGKDD explorations newsletter 19 (1) (2017) 22–36.","47":"K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556.","48":"L. Specia, S. Frank, K. Simaan, D. Elliott, A shared task on multimodal machine translation and crosslingual image description, in: Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, 2016, pp. 543–553.","49":"N. Srivastava, R. Salakhutdinov, et al., Multimodal learning with deep boltzmann machines., in: NIPS, Vol. 1, Citeseer, 2012, p. 2.","51":"C. Van Hee, G. Jacobs, C. Emmery, B. Desmet, E. Lefever, B. Verhoeven, G. De Pauw, W. Daelemans, V. Hoste, Automatic detection of cyberbullying in social media text, PloS one 13 (10) (2018) e0203794.","52":"A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, arXiv preprint arXiv:1706.03762.","53":"R. Velioglu, J. Rose, Detecting hate speech in memes using multimodal deep learning approaches: Prize-winning solution to hateful memes challenge, arXiv preprint arXiv:2012.12975.","54":"J. Venugopalan, L. Tong, H. R. Hassanzadeh, M. D. Wang, Multimodal deep learning models for early detection of alzheimers disease stage, Scientific reports 11 (1) (2021) 1–13.","55":"D. Wang, T. Abdelzaher, L. Kaplan, Social sensing: building reliable systems on unreliable data, Morgan Kaufmann, 2015.","57":"D. Wang, B. K. Szymanski, T. Abdelzaher, H. Ji, L. Kaplan, The age of social sensing, Computer 52 (1) (2019) 36–45.","58":"M. J. Warrens, Inequalities between multi-rater kappas, Advances in data analysis and classification 4 (4) (2010) 271–286.","61":"Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, A. van den Hengel, Visual question answering: A survey of methods and datasets, Computer Vision and Image Understanding 163 (2017) 21–40.","64":"K. Yi, J. Wu, C. Gan, A. Torralba, P. Kohli, J. Tenenbaum, Neural-symbolic vqa: Disentangling reasoning from vision and language understanding, in: Advances in neural information processing systems, 2018, pp. 1031–1042.","66":"Y. Zhang, X. Dong, L. Shang, D. Zhang, D. Wang, A multi-modal graph neural network approach to traffic risk forecasting in smart urban sensing, in: 2020 17th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON), IEEE, 2020, pp. 1–9.","67":"Y. Zhang, Y. Lu, D. Zhang, L. Shang, D. Wang, Risksens: A multi-view learning approach to identifying risky traffic locations in intelligent transportation systems using social and remote sensing, in: 2018 IEEE International Conference on Big Data (Big Data), IEEE, 2018, pp. 1544–1553.","68":"Z. Zhang, L. Luo, Hate speech detection: A solved problem? the challenging case of long tail on twitter, Semantic Web 10 (5) (2019) 925–945.","69":"D. Y. Zhang, L. Shang, B. Geng, S. Lai, K. Li, H. Zhu, M. T. Amin, D. Wang, Fauxbuster: A content-free fauxtography detector using social media comments, in: 2018 IEEE International Conference on Big Data (Big Data), IEEE, 2018, pp. 891–900.","70":"M. Zhou, R. Cheng, Y. J. Lee, Z. Yu, A visual attention grounding neural model for multimodal machine translation, arXiv preprint arXiv:1808.08266.","71":"W. Zhou, H. Li, Q. Tian, Recent advance in content-based image retrieval: A literature survey, arXiv preprint arXiv:1706.06064.","72":"R. Zhu, Enhance multimodal transformer with external label and in-domain pretrain: Hateful meme challenge winning solution, arXiv preprint arXiv:2012.08290."}},"referenceLinks":{"external":[{"refId":10,"scopusHubEid":"2-s2.0-85033589604"},{"refId":12,"scopusHubEid":"2-s2.0-27744588611"},{"refId":14,"crossRefDoi":"10.7551/mitpress/1251.003.0020"},{"refId":17,"scopusHubEid":"2-s2.0-85037622282","crossRefDoi":"10.5130/ccs.v9i3.5655"},{"refId":18,"crossRefDoi":"10.1145/3185593"},{"refId":20,"scopusHubEid":"2-s2.0-85103829812","crossRefDoi":"10.1109/bigdata50022.2020.9378019"},{"refId":24,"scopusHubEid":"2-s2.0-84906493406","crossRefDoi":"10.1007/978-3-319-10602-1_48"},{"refId":27,"scopusHubEid":"2-s2.0-85018917850","crossRefDoi":"10.1142/9789814651011_0042"},{"refId":28,"scopusHubEid":"2-s2.0-85070928037","crossRefDoi":"10.1371/journal.pone.0221152"},{"refId":35,"scopusHubEid":"2-s2.0-84864465291"},{"refId":37,"scopusHubEid":"2-s2.0-85040306665"},{"refId":38,"scopusHubEid":"2-s2.0-85070382250","crossRefDoi":"10.1609/icwsm.v13i01.3354"},{"refId":39,"scopusHubEid":"2-s2.0-84960980241"},{"refId":43,"scopusHubEid":"2-s2.0-85081396295","crossRefDoi":"10.1109/bigdata47090.2019.9006329"},{"refId":46,"crossRefDoi":"10.1145/3137597.3137600"},{"refId":48,"scopusHubEid":"2-s2.0-85123206144","crossRefDoi":"10.18653/v1/W16-2346"},{"refId":49,"scopusHubEid":"2-s2.0-84861197551"},{"refId":51,"scopusHubEid":"2-s2.0-85054774650","crossRefDoi":"10.1371/journal.pone.0203794"},{"refId":54,"crossRefDoi":"10.1504/ijbir.2021.10046667"},{"refId":57,"scopusHubEid":"2-s2.0-85063054342","crossRefDoi":"10.1109/mc.2018.2890173"},{"refId":58,"scopusHubEid":"2-s2.0-78650064898","crossRefDoi":"10.1007/s11634-010-0073-4"},{"refId":61,"scopusHubEid":"2-s2.0-85019154252"},{"refId":64,"scopusHubEid":"2-s2.0-85064824191"},{"refId":67,"scopusHubEid":"2-s2.0-85062622724","crossRefDoi":"10.1109/bigdata.2018.8621996"},{"refId":68,"scopusHubEid":"2-s2.0-85072407649","crossRefDoi":"10.3233/sw-180338"},{"refId":69,"scopusHubEid":"2-s2.0-85062616099","crossRefDoi":"10.1109/bigdata.2018.8622344"}],"internal":[{"refId":12,"pii":"S0893608005001206","filesize":"328KB","pdf":{"urlType":"download","url":"/science/article/pii/S0893608005001206/pdfft?md5=c309497aaf1390dcf5f8da00c9fc2b7c&pid=1-s2.0-S0893608005001206-main.pdf"}},{"refId":61,"pii":"S1077314217300772","filesize":"4MB","pdf":{"urlType":"download","url":"/science/article/pii/S1077314217300772/pdfft?md5=7c641aaee8fe1b28b8fbcbe0b749c352&pid=1-s2.0-S1077314217300772-main.pdf"}}]},"refersTo":{},"referredToBy":{},"relatedContent":{"isModal":false,"isOpenSpecialIssueArticles":false,"isOpenVirtualSpecialIssueLink":false,"isOpenRecommendations":true,"isOpenSubstances":true,"citingArticles":[false,false,false,false,false,false],"recommendations":[false,false,false,false,false,false]},"seamlessAccess":{},"specialIssueArticles":{},"substances":{},"supplementaryFilesData":[],"tableOfContents":{"outlineTitle":"Outline","outline":[{"#name":"entry","$":{"id":"d1e515","type":"abstract","class":"author","depth":"1"},"$$":[{"#name":"title","$":{"id":"d1e516"},"_":"Abstract"}]},{"#name":"entry","$":{"id":"d1e527","type":"abstract","class":"author-highlights","depth":"1"},"$$":[{"#name":"title","$":{"id":"d1e528"},"_":"Highlights"}]},{"#name":"entry","$":{"id":"d1e559","type":"keywords","class":"keyword","depth":"1"},"$$":[{"#name":"title","$":{"id":"d1e560"},"_":"Keywords"}]},{"#name":"entry","$":{"id":"sec1","type":"sections","depth":"1"},"$$":[{"#name":"label","_":"1"},{"#name":"title","$":{"id":"d1e576"},"_":"Introduction"}]},{"#name":"entry","$":{"id":"sec2","type":"sections","depth":"1"},"$$":[{"#name":"label","_":"2"},{"#name":"title","$":{"id":"d1e780"},"_":"Related work"},{"#name":"entry","$":{"id":"sec2.1","depth":"2"},"$$":[{"#name":"label","_":"2.1"},{"#name":"title","$":{"id":"d1e785"},"_":"Social media misbehavior"}]},{"#name":"entry","$":{"id":"sec2.2","depth":"2"},"$$":[{"#name":"label","_":"2.2"},{"#name":"title","$":{"id":"d1e850"},"_":"Hate speech detection"}]},{"#name":"entry","$":{"id":"sec2.3","depth":"2"},"$$":[{"#name":"label","_":"2.3"},{"#name":"title","$":{"id":"d1e883"},"_":"Social media memes"}]},{"#name":"entry","$":{"id":"sec2.4","depth":"2"},"$$":[{"#name":"label","_":"2.4"},{"#name":"title","$":{"id":"d1e927"},"_":"Multi-modal learning"}]}]},{"#name":"entry","$":{"id":"sec3","type":"sections","depth":"1"},"$$":[{"#name":"label","_":"3"},{"#name":"title","$":{"id":"d1e1014"},"_":"Problem definition"}]},{"#name":"entry","$":{"id":"sec4","type":"sections","depth":"1"},"$$":[{"#name":"label","_":"4"},{"#name":"title","$":{"id":"d1e1570"},"_":"Solution"},{"#name":"entry","$":{"id":"sec4.1","depth":"2"},"$$":[{"#name":"label","_":"4.1"},{"#name":"title","$":{"id":"d1e1593"},"_":"Analogy-aware multi-modal representation learning module"},{"#name":"entry","$":{"id":"sec4.1.1","depth":"3"},"$$":[{"#name":"label","_":"4.1.1"},{"#name":"title","$":{"id":"d1e1600"},"_":"Visual content extraction"}]},{"#name":"entry","$":{"id":"sec4.1.2","depth":"3"},"$$":[{"#name":"label","_":"4.1.2"},{"#name":"title","$":{"id":"d1e1831"},"_":"Embedded caption extraction"}]},{"#name":"entry","$":{"id":"sec4.1.3","depth":"3"},"$$":[{"#name":"label","_":"4.1.3"},{"#name":"title","$":{"id":"d1e2420"},"_":"Contextual information extraction"}]}]},{"#name":"entry","$":{"id":"sec4.2","depth":"2"},"$$":[{"#name":"label","_":"4.2"},{"#name":"title","$":{"id":"d1e2494"},"_":"Attentive multi-modal analogy alignment module"}]},{"#name":"entry","$":{"id":"sec4.3","depth":"2"},"$$":[{"#name":"label","_":"4.3"},{"#name":"title","$":{"id":"d1e3452"},"_":"Supervised learning module"}]}]},{"#name":"entry","$":{"id":"sec5","type":"sections","depth":"1"},"$$":[{"#name":"label","_":"5"},{"#name":"title","$":{"id":"d1e3774"},"_":"Data"}]},{"#name":"entry","$":{"id":"sec6","type":"sections","depth":"1"},"$$":[{"#name":"label","_":"6"},{"#name":"title","$":{"id":"d1e3837"},"_":"Evaluation"},{"#name":"entry","$":{"id":"sec6.1","depth":"2"},"$$":[{"#name":"label","_":"6.1"},{"#name":"title","$":{"id":"d1e3849"},"_":"Baselines and experiment setup"},{"#name":"entry","$":{"id":"sec6.1.1","depth":"3"},"$$":[{"#name":"label","_":"6.1.1"},{"#name":"title","$":{"id":"d1e3854"},"_":"Baselines"}]},{"#name":"entry","$":{"id":"sec6.1.2","depth":"3"},"$$":[{"#name":"label","_":"6.1.2"},{"#name":"title","$":{"id":"d1e3962"},"_":"Experiment setup"}]}]},{"#name":"entry","$":{"id":"sec6.2","depth":"2"},"$$":[{"#name":"label","_":"6.2"},{"#name":"title","$":{"id":"d1e4029"},"_":"Detection effectiveness"}]},{"#name":"entry","$":{"id":"sec6.3","depth":"2"},"$$":[{"#name":"label","_":"6.3"},{"#name":"title","$":{"id":"d1e4069"},"_":"Analogy awareness"}]}]},{"#name":"entry","$":{"id":"sec7","type":"sections","depth":"1"},"$$":[{"#name":"label","_":"7"},{"#name":"title","$":{"id":"d1e4083"},"_":"Conclusion"}]},{"#name":"entry","$":{"id":"d1e4087","type":"sections","depth":"1"},"$$":[{"#name":"title","$":{"id":"d1e4088"},"_":"CRediT authorship contribution statement"}]},{"#name":"entry","$":{"id":"d1e4109","type":"acknowledgment","depth":"1"},"$$":[{"#name":"title","$":{"id":"d1e4110"},"_":"Acknowledgments"}]},{"#name":"entry","$":{"id":"bib1","type":"bibliography","depth":"1"},"$$":[{"#name":"title","$":{"id":"d1e4137"},"_":"References"}]}],"figures":[],"tables":[{"#name":"entry","$":{"id":"tbl1","type":"sections","local-type":"table"},"$$":[{"#name":"label","_":"Table 1"},{"#name":"caption","$":{"truncated":"false"},"_":"Dataset statistics."}]},{"#name":"entry","$":{"id":"tbl2","type":"sections","local-type":"table"},"$$":[{"#name":"label","_":"Table 2"},{"#name":"caption","$":{"truncated":"false"},"_":"Classification performance for all methods — Gab."}]},{"#name":"entry","$":{"id":"tbl3","type":"sections","local-type":"table"},"$$":[{"#name":"label","_":"Table 3"},{"#name":"caption","$":{"truncated":"false"},"_":"Classification performance for all methods — Reddit."}]}],"extras":[],"attachments":[],"showEntitledTocLinks":false},"tail":{},"transientError":{"isOpen":false},"userAgent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36 Edg/124.0.0.0","sidePanel":{"openState":1},"viewConfig":{"articleFeature":{"rightsAndContentLink":true},"pathPrefix":""},"virtualSpecialIssue":{"showVirtualSpecialIssueLink":false},"userCookiePreferences":{"STRICTLY_NECESSARY":true,"PERFORMANCE":true,"FUNCTIONAL":true,"TARGETING":true}}</script>
<script src="https://assets.adobedtm.com/4a848ae9611a/032db4f73473/launch-a6263b31083f.min.js" type="f099078ee6a757eb17f3100f-text/javascript" async></script>
<script type="f099078ee6a757eb17f3100f-text/javascript">
    window.pageData = {"content":[{"contentType":"JL","format":"MIME-XHTML","id":"sd:article:pii:S0306457321001527","type":"sd:article:JL:scope-abstract","detail":"sd:article:subtype:fla","publicationType":"journal","issn":"0306-4573","volumeNumber":"58","issueNumber":"5","provider":"elsevier","entitlementType":"unsubscribed"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productAppVersion":"abstract-redirected","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1713677274208,"loadTime":""},"visitor":{"accessType":"ae:ANON_GUEST","accountId":"ae:228598","accountName":"ae:ScienceDirect Guests","loginStatus":"anonymous","userId":"ae:12975512","ipAddress":"116.239.5.38","appSessionId":"5f2e486b-6030-4006-a2b4-ab2302d26b51"}};
    window.pageData.page.loadTime = performance ? Math.round(performance.now()).toString() : '';

    try {
      appData.push({
      event: 'pageLoad',
      page: pageData.page,
      visitor: pageData.visitor,
      content: pageData.content
      })
    } catch(e) {
        console.warn("There was an error loading or running Adobe DTM: ", e);
    }
</script>
<script nomodule src="https://sciencedirect.elseviercdn.cn/shared-assets/73/js/core-js/3.20.2/core-js.es.minified.js" type="f099078ee6a757eb17f3100f-text/javascript"></script>
<script src="https://sciencedirect.elseviercdn.cn/shared-assets/100/js/react/16.14.0/react.production.min.js" type="f099078ee6a757eb17f3100f-text/javascript"></script>
<script src="https://sciencedirect.elseviercdn.cn/shared-assets/100/js/react-dom/16.14.0/react-dom.production.min.js" type="f099078ee6a757eb17f3100f-text/javascript"></script>
<script src="https://sciencedirect.elseviercdn.cn/prod/f624014e8238d0b70668b2e1dcc65fef7457d17f/arp.js" async type="f099078ee6a757eb17f3100f-text/javascript"></script>
<script type="f099078ee6a757eb17f3100f-text/javascript">
    const pendoData = {"visitor":{"pageName":"SD:product:journal:article","pageType":"CP-CA","pageProduct":"SD","pageLanguage":"en","pageEnvironment":"prod","accessType":"ae:ANON_GUEST","countryCode":"CN"},"account":{"id":"ae:228598","name":"ae:ScienceDirect Guests"},"events":{}};;
    pendoData.events = {
      ready: function () {
        pendo.addAltText();
      },
    };
    function runPendo(data, options) {
  const {
    firstDelay,
    maxRetries,
    urlPrefix,
    urlSuffix,
    apiKey
  } = options;
  (function (apiKey) {
    (function (p, e, n, d, o) {
      var v, w, x, y, z;
      o = p[d] = p[d] || {};
      o._q = [];
      v = ['initialize', 'identify', 'updateOptions', 'pageLoad'];
      for (w = 0, x = v.length; w < x; ++w) (function (m) {
        o[m] = o[m] || function () {
          o._q[m === v[0] ? 'unshift' : 'push']([m].concat([].slice.call(arguments, 0)));
        };
      })(v[w]);
      y = e.createElement(n);
      y.async = !0;
      y.src = urlPrefix + apiKey + urlSuffix;
      z = e.getElementsByTagName(n)[0];
      z.parentNode.insertBefore(y, z);
    })(window, document, 'script', 'pendo');
    pendo.addAltText = function () {
      var target = document.querySelector('body');
      var observer = new MutationObserver(function (mutations) {
        mutations.forEach(function (mutation) {
          if (mutation?.addedNodes?.length) {
            if (mutation.addedNodes[0]?.className?.includes("_pendo-badge")) {
              const badge = mutation.addedNodes[0];
              const altText = badge?.attributes['aria-label'].value ? badge?.attributes['aria-label'].value : 'Feedback';
              const pendoBadgeImage = pendo.dom(`#${badge?.attributes?.id.value} img`);
              if (pendoBadgeImage.length) {
                pendoBadgeImage[0]?.setAttribute('alt', altText);
              }
            }
          }
        });
      });
      var config = {
        attributeFilter: ['data-layout'],
        attributes: true,
        childList: true,
        characterData: true,
        subtree: false
      };
      observer.observe(target, config);
    };
  })(apiKey);
  (function watchAndSetPendo(nextDelay, retryAttempt) {
    if (typeof pageDataTracker === 'object' && typeof pageDataTracker.getVisitorId === 'function' && pageDataTracker.getVisitorId()) {
      data.visitor.id = pageDataTracker.getVisitorId();
      console.debug(`initializing pendo`);
      pendo.initialize(data);
    } else {
      if (retryAttempt > 0) {
        return setTimeout(function () {
          watchAndSetPendo(nextDelay * 2, retryAttempt - 1);
        }, nextDelay);
      }
      pendo.initialize(data);
      console.debug(`gave up ... pendo initialized`);
    }
  })(firstDelay, maxRetries);
}
    runPendo(pendoData, {
      firstDelay: 100,
      maxRetries: 5,
      urlPrefix: 'https://cdn.pendo.io/agent/static/',
      urlSuffix: '/pendo.js',
      apiKey: 'd6c1d995-bc7e-4e53-77f1-2ea4ecbb9565',
    });
  </script>
<span id="pendo-answer-rating"></span>
<script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=MML_SVG" type="f099078ee6a757eb17f3100f-text/javascript"></script>
<script async src="https://www.googletagservices.com/tag/js/gpt.js" type="f099078ee6a757eb17f3100f-text/javascript"></script>
<script async src="https://scholar.google.com/scholar_js/casa.js" type="f099078ee6a757eb17f3100f-text/javascript"></script>
<script type="f099078ee6a757eb17f3100f-module" src="https://static.mendeley.com/view-pdf-component/0.8.9/dist/view-pdf-element.js" crossorigin="anonymous" async></script>
<script src="/cdn-cgi/scripts/7d0fa10a/cloudflare-static/rocket-loader.min.js" data-cf-settings="f099078ee6a757eb17f3100f-|49" defer></script><script>(function(){if (!document.body) return;var js = "window['__CF$cv$params']={r:'877af7b29c1c04cb',t:'MTcxMzY3NzI3NC43NzcwMDA='};_cpo=document.createElement('script');_cpo.nonce='',_cpo.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js',document.getElementsByTagName('head')[0].appendChild(_cpo);";var _0xh = document.createElement('iframe');_0xh.height = 1;_0xh.width = 1;_0xh.style.position = 'absolute';_0xh.style.top = 0;_0xh.style.left = 0;_0xh.style.border = 'none';_0xh.style.visibility = 'hidden';document.body.appendChild(_0xh);function handler() {var _0xi = _0xh.contentDocument || _0xh.contentWindow.document;if (_0xi) {var _0xj = _0xi.createElement('script');_0xj.innerHTML = js;_0xi.getElementsByTagName('head')[0].appendChild(_0xj);}}if (document.readyState !== 'loading') {handler();} else if (window.addEventListener) {document.addEventListener('DOMContentLoaded', handler);} else {var prev = document.onreadystatechange || function () {};document.onreadystatechange = function (e) {prev(e);if (document.readyState !== 'loading') {document.onreadystatechange = prev;handler();}};}})();</script></body>
</html>